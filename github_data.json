{
    "Advanced-RAG-Q-A-Chatbot-With-Chain-And-Retrievers-Using-Langchain": {
        "github_url": "https://github.com/Parthiban-3997/Advanced-RAG-Q-A-Chatbot-With-Chain-And-Retrievers-Using-Langchain",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Advanced-RAG-Q-A-Chatbot-With-Chain-And-Retrievers-Using-Langchain",
        "description": """
        ================================================
File: README.md
================================================
# Advanced RAG Q&A Chatbot with LangChain

This project implements a powerful Question-Answering chatbot using LangChain's Retrieval-Augmented Generation (RAG) concepts. The chatbot can process uploaded PDF documents, answer complex questions based on their content, and leverage advanced techniques like chains and retrievers.

**Key Features**

*   **PDF Document Processing:** Ability to upload and extract knowledge from PDF files.
*   **Retrieval-Augmented Generation (RAG):** Employs retrievers to access relevant information and chains to structure the question answering process.
*   **Streamlit Integration:** User-friendly web interface powered by Streamlit.
*   **LangChain:** Built on the flexible and powerful LangChain framework.
*   **OpenAI Integration (Optional):** Support for integrating OpenAI's language models like GPT-3.5.

**Prerequisites**

*   Python 3.x
*   A LangChain API Key (if specified in the code)
*   An OpenAI API Key (if you're using OpenAI models)

**Installation**

1.  Clone this repository:
    ```bash
    git clone [https://github.com/](https://github.com/)<your-username>/<your-repo-name>
    ```
2.  Navigate to the project directory:
    ```bash
    cd <your-repo-name>
    ```
3.  Create a virtual environment (recommended):
    ```bash
    python -m venv env
    env\Scripts\activate  # For Windows
    source env/bin/activate  # For Linux/macOS
    ```
4.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

**Usage**

1.  **Environment Variables:**
    *   Create a `.env` file in the project root.
    *   Add the following, replacing placeholders with your actual keys:
        ```
        OPENAI_API_KEY=<your_openai_api_key>
        LANGCHAIN_API_KEY=<your_langchain_api_key>
        ```
    *   Load the environment variables using the `dotenv` library (already included in the code).

2.  **Start the Application:**
    ```bash
    streamlit run app.py 
    ```

3.  **Using the Web Interface**
    *   Open `http://localhost:8501` (or the provided Streamlit URL) in your web browser.
    *   Upload a PDF file.
    *   Type your question in the text box.
    *   Click 'Submit' to get the answer.

**Customization**

*   **LLM Choice:** Modify the code to switch between OpenAI and Ollama language models.
*   **Embedding Techniques:** Experiment with other embedding providers.
*   **Streamlit Enhancements:** Add more interactive elements or styling to the web interface.

**Contributions**

This project welcomes contributions! Feel free to add features, fix bugs, or suggest improvements.

**License**

[Specify the license of your project, e.g., MIT, Apache 2.0, etc.]


================================================
File: LICENSE
================================================
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  'This License' refers to version 3 of the GNU General Public License.

  'Copyright' also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  'The Program' refers to any copyrightable work licensed under this
License.  Each licensee is addressed as 'you'.  'Licensees' and
'recipients' may be individuals or organizations.

  To 'modify' a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a 'modified version' of the
earlier work or a work 'based on' the earlier work.

  A 'covered work' means either the unmodified Program or a work based
on the Program.

  To 'propagate' a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To 'convey' a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays 'Appropriate Legal Notices'
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The 'source code' for a work means the preferred form of the work
for making modifications to it.  'Object code' means any non-source
form of a work.

  A 'Standard Interface' means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The 'System Libraries' of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
'Major Component', in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The 'Corresponding Source' for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    'keep intact all notices'.

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
'aggregate' if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A 'User Product' is either (1) a 'consumer product', which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, 'normally used' refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  'Installation Information' for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  'Additional permissions' are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered 'further
restrictions' within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An 'entity transaction' is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A 'contributor' is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's 'contributor version'.

  A contributor's 'essential patent claims' are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, 'control' includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a 'patent license' is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To 'grant' such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  'Knowingly relying' means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is 'discriminatory' if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License 'or any later version' applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM 'AS IS' WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the 'copyright' line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an 'about box'.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a 'copyright disclaimer' for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.


================================================
File: requirements.txt
================================================
langchain_openai 
langchain_core
python-dotenv
streamlit
langchain_community
langserve
fastapi
uvicorn
sse_starlette
bs4
pypdf
chromadb
faiss-cpu
PyPDF2

================================================
File: api/app.py
================================================
from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langserve import add_routes
import uvicorn
import os
from langchain_community.llms import Ollama
from dotenv import load_dotenv

load_dotenv()

os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')

app=FastAPI(
    title='Langchain Server',
    version='1.0',
    decsription='A simple API Server'

)

add_routes(
    app,
    ChatOpenAI(),
    path='/openai'
)
model=ChatOpenAI()
##ollama llama2
llm=Ollama(model='llama2')

prompt1=ChatPromptTemplate.from_template('Write me an essay about {topic} with 100 words')
prompt2=ChatPromptTemplate.from_template('Write me an poem about {topic} for a 5 years child with 100 words')

add_routes(
    app,
    prompt1|model,
    path='/essay'


)

add_routes(
    app,
    prompt2|llm,
    path='/poem'


)


if __name__=='__main__':
    uvicorn.run(app,host='localhost',port=8000)



================================================
File: api/client.py
================================================
import requests
import streamlit as st

def get_openai_response(input_text):
    response=requests.post('http://localhost:8000/essay/invoke',
    json={'input':{'topic':input_text}})

    return response.json()['output']['content']

def get_ollama_response(input_text):
    response=requests.post(
    'http://localhost:8000/poem/invoke',
    json={'input':{'topic':input_text}})

    return response.json()['output']

    ## streamlit framework

st.title('Langchain Demo With LLAMA2 API')
input_text=st.text_input('Write an essay on')
input_text1=st.text_input('Write a poem on')

if input_text:
    st.write(get_openai_response(input_text))

if input_text1:
    st.write(get_ollama_response(input_text1))


================================================
File: chatbot/app.py
================================================
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

import streamlit as st
import os
from dotenv import load_dotenv

os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')
## Langmith tracking
os.environ['LANGCHAIN_TRACING_V2']='true'
os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')

## Prompt Template

prompt=ChatPromptTemplate.from_messages(
    [
        ('system','You are a helpful assistant. Please response to the user queries'),
        ('user','Question:{question}')
    ]
)

## streamlit framework

st.title('Langchain Demo With OPENAI API')
input_text=st.text_input('Search the topic u want')

# openAI LLm 
llm=ChatOpenAI(model='gpt-3.5-turbo')
output_parser=StrOutputParser()
chain=prompt|llm|output_parser

if input_text:
    st.write(chain.invoke({'question':input_text}))

================================================
File: chatbot/localama.py
================================================
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
import streamlit as st
import os
from dotenv import load_dotenv

load_dotenv()

os.environ['LANGCHAIN_TRACING_V2']='true'
os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')

## Prompt Template

prompt=ChatPromptTemplate.from_messages(
    [
        ('system','You are a helpful assistant. Please response to the user queries'),
        ('user','Question:{question}')
    ]
)
## streamlit framework

st.title('Langchain Demo With LLAMA2 API')
input_text=st.text_input('Search the topic u want')

# ollama LLAma2 LLm 
llm=Ollama(model='llama2')
output_parser=StrOutputParser()
chain=prompt|llm|output_parser

if input_text:
    st.write(chain.invoke({'question':input_text}))

================================================
File: rag/speech.txt
================================================
The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.

Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.

…

It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.

We have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.

It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.

To such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.


        """,
        "repo_created_at": "2025-02-12T13:08:25Z",
        "last_updated": "2025-02-13T00:55:23Z",
        "stars": 0
    },
    "Chat_Groq_Document_Q_A": {
        "github_url": "https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Chat_Groq_Document_Q_A",
        "description": """
        ================================================
File: README.md
================================================
## Deployed Link
Document Q&A app is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/Chat_Groq_Document_Q_A)

## Screenshots
![chat_groq_1](https://github.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/6125958f-93ac-4399-aa86-55a9f2613801)
![chat_groq_5](https://github.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/c0fc5328-cad2-4e13-9b61-5323fb653305)
![chat_groq_6](https://github.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/574bd55a-0726-48b3-aff7-e6d9ddfd874c)

# Chat Groq Document Q&A

This Streamlit web application allows you to upload PDF documents, embed them using Google's Generative AI Embeddings, and perform question-answering based on the uploaded documents. The app utilizes the powerful Groq language model and its efficient inference engine to provide accurate and relevant answers to your queries. The application follows the RAG (Retrieval-Augmented Generation) approach, which combines the strengths of retrieval systems and generative language models.

## Features

- **PDF Document Upload**: Upload multiple PDF files to create a knowledge base.
- **Document Embedding**: Leverage Google's Generative AI Embeddings to embed the uploaded PDF documents.
- **Question Answering**: Ask questions related to the uploaded documents, and receive precise answers powered by the Groq's LPU inference engine.
- **Custom Prompt Templates**: Customize the prompt template for question-answering to suit your specific needs.
- **Model Selection**: Choose from a variety of available Groq open source models, including `llama3-8b-8192`, `llama3-70b-8192`, `mixtral-8x7b-32768`, and `gemma-7b-it`.
- **Document Similarity Search**: Explore relevant document chunks that match the provided question.
- **Response Time Tracking**: Monitor the response time for each query.

## RAG (Retrieval-Augmented Generation) Approach

The application follows the RAG approach, which combines the strengths of retrieval systems and generative language models. Here's how it works:

- **Retrieval**: The application embeds the uploaded PDF documents using Google's Generative AI Embeddings and stores them in a vector store (FAISS). When a user asks a question, the relevant document chunks are retrieved from the vector store based on their similarity to the question.

- **Augmentation**: The retrieved document chunks are combined and provided as context to the Groq language model. This augments the language model's knowledge with relevant information from the uploaded documents.

- **Generation**: The Groq language model uses the provided context and the question to generate a precise and relevant answer. The language model's generative capabilities allow it to synthesize information from the retrieved document chunks and produce a coherent response.

The RAG approach offers several advantages:

- **Scalability**: By leveraging a retrieval system, the application can handle large knowledge bases efficiently, allowing users to query information from extensive document collections.
- **Contextual Understanding**: The language model can better understand the context of the question and provide more accurate and nuanced answers by using the relevant document chunks as context.
- **Knowledge Grounding**: The generated answers are grounded in the factual information present in the uploaded documents, ensuring the reliability and trustworthiness of the responses.

## Advantages of Groq LPU Inference Engine

- **High Performance**: Groq's LP (Learning Processor) Inference Engine is designed to deliver exceptional performance for large language models, enabling fast and efficient question answering.
- **Energy Efficiency**: The LP Inference Engine is optimized for energy efficiency, making it suitable for deployment on various devices and environments.
- **Scalability**: Groq's architecture allows for seamless scaling of language models, ensuring that the application can handle increasing demands and larger knowledge bases.
- **Low Latency**: The LP Inference Engine provides low-latency inference, ensuring quick response times for user queries.

## Langsmith Integration

This application integrates with Langsmith, a platform for monitoring and managing large language model deployments. Langsmith provides the following benefits:

- **Log Monitoring**: Monitor and analyze logs generated by the application, making it easier to debug and troubleshoot issues.
- **Cost Optimization**: Langsmith helps optimize the cost of running language models by providing insights into usage patterns and suggesting ways to reduce costs.
- **Performance Monitoring**: Track the performance of the application and identify potential bottlenecks or areas for improvement.



================================================
File: app.py
================================================
import streamlit as st
import os
import tempfile
import time
from langchain_groq import ChatGroq
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv


load_dotenv()




st.set_page_config(page_title='Chat with PDFs', page_icon=':books:')

st.title('Chat Groq Document Q&A')

# Custom prompt template
custom_context_input = '''
<context>
{context}
<context>
Questions:{input}
'''

# Default prompt template
default_prompt_template = '''
Answer the questions based on the provided context only.
Please provide the most accurate response based on the question
<context>
{context}
<context>
Questions:{input}
'''

def vector_embedding(pdf_files):
    if 'vectors' not in st.session_state:
        st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')

    documents = []
    for pdf_file in pdf_files:
        # Save the uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_file_path = tmp_file.name

        # Load the PDF from the temporary file path
        loader = PyPDFLoader(tmp_file_path)
        documents.extend(loader.load()) ## append the files

        # Remove the temporary file
        os.remove(tmp_file_path)

        st.session_state.text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
        st.session_state.final_documents = st.session_state.text_splitter.split_documents(documents)
        st.session_state.vectors = FAISS.from_documents(st.session_state.final_documents, st.session_state.embeddings)

    st.success('Document embedding is completed!')


# Define model options
model_options = [
    'llama-3.1-70b-versatile',
    'llama-3.1-8b-instant',
    'llama3-8b-8192',
    'llama3-70b-8192',
    'mixtral-8x7b-32768',
    'gemma2-9b-it'
]

# Sidebar elements
with st.sidebar:
    st.header('Configuration')
    st.markdown('Enter your API keys below:')
    groq_api_key = st.text_input('Enter your GROQ API Key', type='password', help='Get your API key from [GROQ Console](https://console.groq.com/keys)')
    google_api_key = st.text_input('Enter your Google API Key', type='password', help='Get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey)')
    langsmith_api_key = st.text_input('Enter your Langsmith API Key', type='password',placeholder='For Tracing the flows (Optional!)', help='Get your API key from [Langsmith Console](https://smith.langchain.com/o/2a79134f-7562-5c92-a437-96b080547a1e/settings)')
    selected_model = st.selectbox('Select any Groq Model', model_options)
    os.environ['GOOGLE_API_KEY']=str(google_api_key)
    os.environ['LANGCHAIN_API_KEY']=str(langsmith_api_key)
    # Langmith tracking
    os.environ['LANGCHAIN_TRACING_V2'] = 'true'
    st.markdown('Upload your PDF files:')
    uploaded_files = st.file_uploader('Choose PDF files', accept_multiple_files=True, type='pdf')


    # Custom prompt text areas
    custom_prompt_template = st.text_area('Custom Prompt Template', placeholder='Enter your custom prompt here to set the tone of the message...(Optional)')

    if st.button('Start Document Embedding'):
        if uploaded_files:
            vector_embedding(uploaded_files)
            st.success('Vector Store DB is Ready')
        else:
            st.warning('Please upload at least one PDF file.')

# Main section for question input and results
prompt1 = st.text_area('Enter Your Question From Documents')

if prompt1 and 'vectors' in st.session_state:
    if custom_prompt_template:
        custom_prompt = custom_prompt_template + custom_context_input
        prompt = ChatPromptTemplate.from_template(custom_prompt)
    else:
        prompt = ChatPromptTemplate.from_template(default_prompt_template)
    
    llm = ChatGroq(groq_api_key=groq_api_key, model_name=selected_model)
    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = st.session_state.vectors.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    start = time.process_time()
    response = retrieval_chain.invoke({'input': prompt1})
    st.write('Response time:', time.process_time() - start)
    st.write(response['answer'])

    # With a Streamlit expander
    with st.expander('Document Similarity Search'):
        # Find the relevant chunks
        for i, doc in enumerate(response['context']):
            st.write(doc.page_content)
            st.write('--------------------------------') 


================================================
File: requirements.txt
================================================
faiss-cpu
groq
PyPDF2
langchain_google_genai
langchain_groq
langchain
streamlit
langchain_community
python-dotenv
pypdf






        """,
        "repo_created_at": "2025-02-12T13:08:35Z",
        "last_updated": "2025-02-13T00:54:51Z",
        "stars": 0
    },
    "Chat_With_IPYNB_Files": {
        "github_url": "https://github.com/Parthiban-3997/Chat_With_IPYNB_Files",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Chat_With_IPYNB_Files",
        "description": """
        ================================================
File: README.md
================================================
# Chat Jupyter Notebooks files using Google Gemini

This Streamlit web application allows you to upload Jupyter Notebook files (.ipynb), embed them using Google's Generative AI Embeddings, and perform question-answering based on the uploaded notebooks. The app utilizes the powerful Gemini language model and its efficient inference engine to provide accurate and relevant answers to your queries. The application follows the RAG (Retrieval-Augmented Generation) approach, which combines the strengths of retrieval systems and generative language models.

## Deployed Link

Chat with .IPYNB files app is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/Chat_With_IPYNB_Files)

## Screenshots

![Capture](https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A/assets/26496805/e7ccdc0a-82e5-4ef2-92b5-b03dc24d519c)


## Features

- **Notebook File Upload**: Upload multiple Jupyter Notebook files (.ipynb) to create a knowledge base.
- **Document Embedding**: Leverage Google's Generative AI Embeddings to embed the uploaded notebook documents.
- **Question Answering**: Ask questions related to the uploaded notebooks, and receive precise answers powered by the Gemini model.
- **Custom Prompt Templates**: Customize the prompt template for question-answering to suit your specific needs.
- **Model Selection**: Choose from a variety of available Gemini models, including `models/gemini-1.0-pro`, `models/gemini-1.0-pro-001`, and more.
- **Document Similarity Search**: Explore relevant document chunks that match the provided question.
- **Response Time Tracking**: Monitor the response time for each query.

## RAG (Retrieval-Augmented Generation) Approach

The application follows the RAG approach, which combines the strengths of retrieval systems and generative language models. Here's how it works:

- **Retrieval**: The application embeds the uploaded notebook documents using Google's Generative AI Embeddings and stores them in a vector store (FAISS). When a user asks a question, the relevant document chunks are retrieved from the vector store based on their similarity to the question.
- **Augmentation**: The retrieved document chunks are combined and provided as context to the Gemini language model. This augments the language model's knowledge with relevant information from the uploaded documents.
- **Generation**: The Gemini language model uses the provided context and the question to generate a precise and relevant answer. The language model's generative capabilities allow it to synthesize information from the retrieved document chunks and produce a coherent response.

The RAG approach offers several advantages:

- **Scalability**: By leveraging a retrieval system, the application can handle large knowledge bases efficiently, allowing users to query information from extensive document collections.
- **Contextual Understanding**: The language model can better understand the context of the question and provide more accurate and nuanced answers by using the relevant document chunks as context.
- **Knowledge Grounding**: The generated answers are grounded in the factual information present in the uploaded documents, ensuring the reliability and trustworthiness of the responses.

## Advantages of Gemini Model

- **High Performance**: The Gemini model is designed to deliver exceptional performance for large language models, enabling fast and efficient question answering.
- **Energy Efficiency**: The model is optimized for energy efficiency, making it suitable for deployment on various devices and environments.
- **Scalability**: Gemini's architecture allows for seamless scaling of language models, ensuring that the application can handle increasing demands and larger knowledge bases.
- **Low Latency**: The model provides low-latency inference, ensuring quick response times for user queries.






================================================
File: app.py
================================================
import streamlit as st
import os
import tempfile
import time
import nbformat
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv
from langchain_core.documents import Document

load_dotenv()

st.set_page_config(page_title='Chat with Notebooks', page_icon=':books:')

st.title('Chat Gemini Document Q&A with Jupyter Notebooks')

# Custom prompt template
custom_context_input = '''
<context>
{context}
</context>
Questions:{input}
'''

# Default prompt template
default_prompt_template = '''
Answer the questions based on the provided context only.
Please provide the most accurate response based on the question
<context>
{context}
</context>
Questions:{input}
'''

def load_notebook(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        notebook = nbformat.read(f, as_version=4)
    return notebook

def extract_text_from_notebook(notebook):
    text = []
    for cell in notebook.cells:
        if cell.cell_type == 'markdown':
            text.append(cell.source)
        elif cell.cell_type == 'code':
            text.append(cell.source)
            if 'outputs' in cell:
                for output in cell.outputs:
                    if output.output_type == 'stream':
                        text.append(output.text)
                    elif output.output_type == 'execute_result' and 'data' in output:
                        text.append(output.data.get('text/plain', ''))
    return '\n'.join(text)

def vector_embedding(ipynb_files):
    if 'vectors' not in st.session_state:
        st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')

    documents = []
    for ipynb_file in ipynb_files:
        # Save the uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix='.ipynb') as tmp_file:
            tmp_file.write(ipynb_file.getvalue())
            tmp_file_path = tmp_file.name

        # Load the .ipynb file from the temporary file path
        notebook = load_notebook(tmp_file_path)
        text = extract_text_from_notebook(notebook)
        # Create a Document object instead of using plain text
        documents.append(Document(page_content=text))

        # Remove the temporary file
        os.remove(tmp_file_path)

    # Ensure documents are properly segmented or chunked
    st.session_state.text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
    try:
        segmented_documents = st.session_state.text_splitter.split_documents(documents)
        st.session_state.final_documents = segmented_documents

        if st.session_state.final_documents:
            # Embedding using FAISS
            st.session_state.vectors = FAISS.from_documents(st.session_state.final_documents, st.session_state.embeddings)
            st.success('Document embedding is completed!')
        else:
            st.warning('No documents found to embed.')
    
    except Exception as e:
        st.error(f'Error splitting or embedding documents: {str(e)}')
        st.session_state.final_documents = []  # Handle empty documents or retry

# Define model options for Gemini
model_options = [
  'gemini-1.5-flash',
  'gemini-1.5-pro',
  'gemini-1.0-pro'
]

# Sidebar elements
with st.sidebar:
    st.header('Configuration')
    st.markdown('Enter your API key below:')
    google_api_key = st.text_input('Enter your Google API Key', type='password', help='Get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey)')
    selected_model = st.selectbox('Select Gemini Model', model_options)
    os.environ['GOOGLE_API_KEY'] = str(google_api_key)
    
    st.markdown('Upload your .ipynb files:')
    uploaded_files = st.file_uploader('Choose .ipynb files', accept_multiple_files=True, type='ipynb')

    # Custom prompt text areas
    custom_prompt_template = st.text_area('Custom Prompt Template', placeholder='Enter your custom prompt here...(optional)')

    if st.button('Start Document Embedding'):
        if uploaded_files:
            vector_embedding(uploaded_files)
            st.success('Vector Store DB is Ready')
        else:
            st.warning('Please upload at least one .ipynb file.')

# Main section for question input and results
prompt1 = st.text_area('Enter Your Question From Documents')

if prompt1 and 'vectors' in st.session_state:
    if custom_prompt_template:
        custom_prompt = custom_prompt_template + custom_context_input
        prompt = ChatPromptTemplate.from_template(custom_prompt)
    else:
        prompt = ChatPromptTemplate.from_template(default_prompt_template)
    
    llm = ChatGoogleGenerativeAI(model=selected_model, temperature=0.3)
    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = st.session_state.vectors.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    start = time.process_time()
    response = retrieval_chain.invoke({'input': prompt1})
    st.write('Response time:', time.process_time() - start)
    st.write(response['answer'])

    # With a Streamlit expander
    with st.expander('Document Similarity Search'):
        # Find the relevant chunks
        for i, doc in enumerate(response['context']):
            st.write(doc.page_content)
            st.write('--------------------------------')

================================================
File: requirements.txt
================================================
streamlit 
langchain 
openai 
nbformat 
faiss-cpu 
langchain-google-genai
langchain-groq
langchain_community
python-dotenv


        """,
        "repo_created_at": "2025-02-12T13:08:40Z",
        "last_updated": "2025-02-13T00:54:14Z",
        "stars": 0
    },
    "Chat_With_Multiple_Data_Sources": {
        "github_url": "https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Chat_With_Multiple_Data_Sources",
        "description": """
        ================================================
File: README.md
================================================
# Chat_With_Multiple_Data_Sources

This Streamlit web application leverages LangChain to provide an advanced query assistant that integrates multiple tools and data sources. Users can query information from Wikipedia, arXiv, and custom URLs. The application also supports custom prompts and maintains conversation history.


## Deployed Link
Document Q&A app is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/Chat_With_Multiple_Data_Sources)


## Screenshots
![data3_2](https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A/assets/26496805/4e7b3d1b-c660-4340-88e0-3cf91c1c8d57)
![data3_3](https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A/assets/26496805/d5ee0b91-8121-48ea-9fa4-6aff0bc68ae9)
![data3_1](https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A/assets/26496805/0aba916f-d173-4f9a-900d-a2328992d44c)


## Features
- **Integration with Wikipedia and arXiv**: Retrieve information directly from Wikipedia and arXiv.
- **Custom URL Support**: Load documents from custom URLs seperated by commas (,) and search within them.
- **Conversation History**: Maintains a conversation history using LangChain's ConversationBufferMemory.
- **Custom Prompts**: Users can define their custom prompts to tailor the assistant's responses.

### LangChain Toolkits
LangChain toolkits provide a set of utilities and pre-built components that simplify the interaction with language models:

- **Document Loaders**: Tools to load documents from various sources. For example, `WebBaseLoader` allows loading web pages and splitting them into manageable chunks.
- **Text Splitters**: Components like `RecursiveCharacterTextSplitter` help divide documents into chunks of a specified size, optimizing them for processing.
- **Embeddings**: `OpenAIEmbeddings` generate vector representations of text, useful for similarity searches and information retrieval.
- **Vector Stores**: `FAISS` is used to store and query vector embeddings efficiently.

### LangChain Agents
LangChain agents are orchestrators that manage the interaction between the language model and various tools:

- **create_openai_tools_agent**: This function creates an agent that can interact with multiple tools using an OpenAI language model. The agent is configured with a prompt template and can utilize tools like WikipediaQueryRun, ArxivQueryRun, and custom URL retrievers.
- **AgentExecutor**: Manages the execution of the agent, handling user inputs and generating responses.

## Components

### Main Components
1. **ChatOpenAI**: The main language model used for generating responses.
2. **ConversationBufferMemory**: Stores the conversation history, allowing the agent to maintain context across multiple interactions.
3. **WikipediaQueryRun**: A tool for retrieving information from Wikipedia.
4. **ArxivQueryRun**: A tool for retrieving information from arXiv.
5. **WebBaseLoader**: Loads documents from specified URLs.
6. **FAISS**: Vector store for managing document embeddings.
7. **ChatPromptTemplate**: Defines the structure of prompts used by the language model.

### Configuration
The application allows users to configure the following settings via the sidebar:

- **OpenAI API Key**: For accessing OpenAI's language models.
- **Custom URLs**: Users can provide URLs to load and search documents.
- **Custom Prompts**: Define custom prompt templates for tailored responses.

### Usage
Users can enter their queries in the main input area. The agent processes the query using the configured tools and returns the response. The conversation history is displayed, showing interactions between the user and the assistant.

### Memory Management
The application uses `ConversationBufferMemory` to retain the conversation history, ensuring context is preserved across multiple user queries. This memory is updated with each interaction, providing a coherent and continuous conversation experience.

## Example Usage
1. Enter your OpenAI API Key in the sidebar.
2. Optionally, provide URLs to load custom documents.
3. Define custom prompts if needed.
4. Click 'Load Tools' to initialize the agent.
5. Enter your query in the main input area and receive a response based on the integrated tools and data sources.


================================================
File: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      'License' shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      'Licensor' shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      'Legal Entity' shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      'control' means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      'You' (or 'Your') shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      'Source' form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      'Object' form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      'Work' shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      'Derivative Works' shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      'Contribution' shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, 'submitted'
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as 'Not a Contribution.'

      'Contributor' shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a 'NOTICE' text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an 'AS IS' BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets '[]'
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same 'printed page' as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the 'License');
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an 'AS IS' BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


================================================
File: app.py
================================================
import streamlit as st
import os
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.vectorstores import FAISS
from langchain import hub
from langchain.tools.retriever import create_retriever_tool
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from langchain_community.utilities import ArxivAPIWrapper
from langchain_community.tools import ArxivQueryRun
from langchain.agents import create_openai_tools_agent
from langchain.agents import AgentExecutor
from htmlTemplates import css, bot_template, user_template

st.set_page_config(page_title='Query Assistant', page_icon=':robot_face:')
st.write(css, unsafe_allow_html=True)


# Initialize session state
if 'agent_executor' not in st.session_state:
    st.session_state.agent_executor = None
if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# Sidebar
with st.sidebar:
    st.title('Query Assistant')
    st.subheader('Configuration')
    openai_api_key = st.text_input('Enter your OpenAI API Key', type='password', help='Get your API key from [OpenAI Website](https://platform.openai.com/api-keys)')
    os.environ['OPENAI_API_KEY'] = str(openai_api_key)

    custom_urls = st.text_area('Enter URLs (optional)', placeholder='Enter URLs separated by (,)')

    # Custom prompt text area
    custom_prompt_template = st.text_area('User Prompts', placeholder='Enter your custom prompt here...(Optional)')

    if st.button('Load Tools'):
        with st.spinner('Loading tools and creating agent...'):
            # Load Wikipedia tool
            api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=600)
            wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)

            # Load arXiv tool
            arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=600)
            arxiv_tool = ArxivQueryRun(api_wrapper=arxiv_wrapper)

            if custom_urls:
                urls = [url.strip() for url in custom_urls.split(',')]
                all_documents = []
                for url in urls:
                    loader = WebBaseLoader(url)
                    docs = loader.load()
                    documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)
                    all_documents.extend(documents)

                vectordb = FAISS.from_documents(all_documents, OpenAIEmbeddings())
                retriever = vectordb.as_retriever()
                retriever_tool = create_retriever_tool(retriever, 'custom_search', 'Search for information if you find any matching keywords from the provided URLs then use this tool and provide the best fit answer from that')
                tools = [wiki_tool, arxiv_tool, retriever_tool]
            else:
                tools = [wiki_tool, arxiv_tool]

            # Load language model
            llm = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0.4)

            # Set the prompt template
            if custom_prompt_template:
                prompt = ChatPromptTemplate.from_messages([
                    ('system', custom_prompt_template),
                    MessagesPlaceholder('chat_history', optional=True),
                    ('human', '{input}'),
                    MessagesPlaceholder('agent_scratchpad'),
                ])
            else:
                prompt = hub.pull('hwchase17/openai-functions-agent')

            # Create the agent with memory
            agent = create_openai_tools_agent(llm, tools, prompt=prompt.partial(chat_history=st.session_state.memory.buffer))
            st.session_state.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)
            st.success('Tools loaded successfully!')

# Main app
user_query = st.chat_input('Enter your query:')

if user_query and st.session_state.agent_executor:
    with st.spinner('Processing your query...'):
        response = st.session_state.agent_executor.invoke({'input': user_query})
        st.session_state.memory.save_context({'input': user_query}, {'chat_history': response['output']})
        st.session_state.chat_history.append({'role': 'user', 'content': user_query})
        st.session_state.chat_history.append({'role': 'assistant', 'content': response['output']})
        print(st.session_state.chat_history)


    for message in st.session_state.chat_history:
      if message['role'] == 'user':
            st.write(user_template.replace(
                '{{MSG}}', message['content']), unsafe_allow_html=True)
      else:
            st.write(bot_template.replace(
                '{{MSG}}', message['content']), unsafe_allow_html=True)

================================================
File: htmlTemplates.py
================================================
css = '''
<style>
.chat-message {
    padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex
}
.chat-message.user {
    background-color: #2b313e
}
.chat-message.bot {
    background-color: #475063
}
.chat-message .avatar {
  width: 20%;
}
.chat-message .avatar img {
  max-width: 78px;
  max-height: 78px;
  border-radius: 50%;
  object-fit: cover;
}
.chat-message .message {
  width: 80%;
  padding: 0 1.5rem;
  color: #fff;
}
'''

bot_template = '''
<div class='chat-message bot'>
    <div class='avatar'>
        <img src='https://i.ibb.co/cN0nmSj/Screenshot-2023-05-28-at-02-37-21.png'>
    </div>
    <div class='message'>{{MSG}}</div>
</div>
'''

user_template = '''
<div class='chat-message user'>
    <div class='avatar'>
        <img src='https://i.ibb.co/tMd5k0j/human-face.png'>
    </div>    
    <div class='message'>{{MSG}}</div>
</div>
'''


================================================
File: requirements.txt
================================================
langchain_openai 
langchain_core
python-dotenv
streamlit
langchain_community
uvicorn
bs4
faiss-cpu
beautifulsoup4
wikipedia
arxiv
langchainhub


        """,
        "repo_created_at": "2025-02-12T13:08:44Z",
        "last_updated": "2025-02-12T13:08:44Z",
        "stars": 0
    },
    "Chat_With_Multiple_SQL_Databases": {
        "github_url": "https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases",
        "description": """
        ================================================
File: README.md
================================================
# SQL Chat Assistant with Multi-Database Support

This project is a Streamlit application that allows users to interact with multiple SQL databases through a natural language interface. The application leverages the power of language models, such as GPT-4 and GROQ models, to understand user queries and generate appropriate SQL queries to retrieve the desired information from the connected databases.


## Deployed Link

Document Q&A app is Deployed And Available [Here](https://chatwithmultiplesqldatabase.streamlit.app/)


## Screenshots

![db_1](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/a2f233a3-8100-42a9-9a26-f42047a4d36e)
![db_2](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/72417ea9-baa6-41fe-8d3c-07a8b9bca5a3)
![db_3](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/d1e18b55-ffa7-4cf8-a60b-c000ef816f0a)
![db_4](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/d8fdecef-0d0b-424f-936f-8e7ad3a80743)
![db_5](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/108f2f5b-b2ed-48b9-b892-74bbc8cef64d)
![db_6](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/741b27f6-5ef6-49c0-866b-97831e1b3824)

## Key Features

- **Multi-Database Support**: The application can connect to multiple databases simultaneously, allowing users to query data across different data sources.
- **Natural Language Interface**: Users can ask questions about the databases in natural language, and the application will translate the queries into SQL queries and execute them against the relevant databases.
- **Chain of Thought Prompting**: This project utilizes the Chain of Thought prompting technique from LangChain, which encourages the language model to break down complex problems into smaller steps, leading to more accurate and interpretable results.
- **Few-Shot Learning**: The application provides a set of examples to the language model, allowing it to learn the desired behavior and generate more accurate SQL queries based on the provided context.

## Project Uniqueness

This project stands out due to its innovative approach to handling multi-database queries using natural language. By combining the power of language models with LangChain's Chain of Thought prompting and Few-Shot Learning techniques, the application can effectively understand and process complex queries spanning multiple databases.

The use of Chain of Thought prompting encourages the language model to break down the problem into smaller steps, making it easier to understand the user's intent and generate appropriate SQL queries. Additionally, Few-Shot Learning allows the model to learn from a set of examples, enhancing its ability to generate accurate SQL queries for a wide range of scenarios.

Furthermore, the application's ability to connect to multiple databases simultaneously makes it a powerful tool for data analysts and business professionals who need to work with data from different sources. This feature eliminates the need for manual data consolidation and enables seamless querying across multiple data sources.

Consider the following diagram to understand how the different chains and components are built:

![Chatbot Architecture](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/d833ca1e-a51d-47b6-a074-6703dd957525)


## Getting Started

To run the SQL Chat Assistant with Multi-Database Support locally, follow these steps:

1. Clone the repository: `git clone https://github.com/your-repo/sql-chat-assistant.git`
2. Install the required dependencies: `pip install -r requirements.txt`
3. Set up your database connections.
4. If you're connecting to a local database, you'll need to expose it using ngrok. Install the ngrok .exe file from [here](https://ngrok.com/download) and generate the authtoken from [here](https://dashboard.ngrok.com/get-started/your-authtoken).
5. Finally run the following command to start ngrok and expose your local database on port 3306: `ngrok tcp 3306`
6. Copy the ngrok URL (e.g., `tcp://x.tcp.ngrok.io:12345`) and use it as the `Host` and 'Port' value.
7. Run the Streamlit application: `streamlit run app.py`

For detailed instructions and additional configuration options, please refer to the project's documentation.

## Contributing

Contributions to this project are welcome! If you have any ideas, bug fixes, or improvements, feel free to submit a pull request. Please ensure that your code adheres to the project's coding standards and is well-documented.

## License

This project is licensed under the [MIT License](LICENSE).


================================================
File: app.py
================================================
import streamlit as st
import os
import toml
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_community.utilities import SQLDatabase
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
#from dotenv import load_dotenv

#load_dotenv() 

# Function to update config.toml file
def update_secrets_file(user, data):
    secrets_file_path = '.streamlit/config.toml'
    secrets_data = {}

    # Load existing data from config.toml
    if os.path.exists(secrets_file_path):
        try:
            with open(secrets_file_path, 'r') as file:
                secrets_data = toml.load(file)
        except toml.TomlDecodeError:
            secrets_data = {}

    # Update user-specific secrets data
    secrets_data[user] = data

    # Write updated data back to config.toml
    with open(secrets_file_path, 'w') as file:
        toml.dump(secrets_data, file)


# Initialize database connections
def init_databases(user):
    secrets_file_path = '.streamlit/config.toml'
    secrets_data = {}
    if os.path.exists(secrets_file_path):
        with open(secrets_file_path, 'r') as file:
            secrets_data = toml.load(file)
    
    user_data = secrets_data.get(user, {})
    
    db_connections = {}
    for database in user_data.get('Databases', '').split(','):
        database = database.strip()
        if database:
            db_uri = f'mysql+mysqlconnector://{user_data['User']}:{user_data['Password']}@{user_data['Host']}:{user_data['Port']}/{database}'
            db_connections[database] = SQLDatabase.from_uri(db_uri)
    return db_connections


# Function to get SQL chain
def get_sql_chain(dbs, llm):
    template = '''
    You are a Senior and vastly experienced Data analyst at a company with around 20 years of experience. 
    You are interacting with a user who is asking you questions about the company's databases.
    Based on the table schemas below, write SQL queries that would answer the user's question. Take the conversation history into account.
    
    <SCHEMAS>{schemas}</SCHEMAS>
    
    Conversation History: {chat_history}
    
    Write the SQL queries for each relevant database, prefixed by the database name (e.g., DB1: SELECT * FROM ...; DB2: SELECT * FROM ...).
    Do not wrap the SQL queries in any other text, not even backticks.
    

    For example:

    Question: Which 3 artists have the most tracks?
    SQL Query: SELECT ArtistId, COUNT(*) as track_count FROM Track GROUP BY ArtistId ORDER BY track_count DESC LIMIT 3;

    Question: Name 10 artists
    SQL Query: SELECT Name FROM Artist LIMIT 10;

    Question:can you show SRK( a.k.a Shah Rukh Khan) movies where the imdb rating is greater than average rating of all movies and also show which movie had highest revenue in millions (INR)
    SQL Query:  SELECT m.title, m.imdb_rating, f.revenue 
                FROM movies m 
                JOIN financials f ON m.movie_id = f.movie_id 
                JOIN movie_actor ma ON m.movie_id = ma.movie_id 
                JOIN actors a ON ma.actor_id = a.actor_id 
                WHERE a.name = 'Shah Rukh Khan' AND m.imdb_rating > (SELECT AVG(imdb_rating) FROM movies) 
                ORDER BY f.revenue DESC;

    Question: How many Van huesen black medium t shirts are available in stock?
    SQL Query: SELECT SUM(stock_quantity) FROM t_shirts WHERE brand = 'Van Huesen' AND color = 'Black' AND size = 'M';

    Question: How much is the price of the inventory for all small size t-shirts?
    SQL Query: SELECT SUM(price * stock_quantity) FROM t_shirts WHERE size = 'S';

    Question: If we have to sell all the Levi's T-shirts today with discounts applied, how much revenue our store will generate (post discounts)?
    SQL Query: SELECT SUM(a.total_amount * ((100 - COALESCE(discounts.pct_discount, 0)) / 100)) AS total_revenue FROM (SELECT SUM(price * stock_quantity) AS total_amount, t_shirt_id FROM t_shirts WHERE brand = 'Levi' GROUP BY t_shirt_id) a LEFT JOIN discounts ON a.t_shirt_id = discounts.t_shirt_id;

    Question: For each brand, find the total revenue generated from t-shirts with a discount applied, grouped by the discount percentage.
    SQL Query: SELECT brand, COALESCE(discounts.pct_discount, 0) AS discount_pct, SUM(t.price * t.stock_quantity * (1 - COALESCE(discounts.pct_discount, 0) / 100)) AS total_revenue FROM t_shirts t LEFT JOIN discounts ON t.t_shirt_id = discounts.t_shirt_id GROUP BY brand, COALESCE(discounts.pct_discount, 0);

    Question: Find the top 3 most popular colors for each brand, based on the total stock quantity.
    SQL Query: SELECT brand, color, SUM(stock_quantity) AS total_stock FROM t_shirts GROUP BY brand, color ORDER BY brand, total_stock DESC;

    Question: Calculate the average price per size for each brand, excluding sizes with less than 10 t-shirts in stock.
    SQL Query: SELECT brand, size, AVG(price) AS avg_price FROM t_shirts WHERE stock_quantity >= 10 GROUP BY brand, size HAVING COUNT(*) >= 10;

    Question: Find the brand and color combination with the highest total revenue, considering discounts.
    SQL Query: SELECT brand, color, SUM(t.price * t.stock_quantity * (1 - COALESCE(d.pct_discount, 0) / 100)) AS total_revenue FROM t_shirts t LEFT JOIN discounts d ON t.t_shirt_id = d.t_shirt_id GROUP BY brand, color ORDER BY total_revenue DESC LIMIT 1;

    Question: Create a view that shows the total stock quantity and revenue for each brand, size, and color combination.
    SQL Query: CREATE VIEW brand_size_color_stats AS SELECT brand, size, color, SUM(stock_quantity) AS total_stock, SUM(price * stock_quantity) AS total_revenue FROM t_shirts GROUP BY brand, size, color;

    Question: How much is the price of the inventory for all variants of t-shirts and group them by brands?
    SQL Query: SELECT brand, SUM(price * stock_quantity) FROM t_shirts GROUP BY brand;

    Question: List the total revenue of t-shirts of L size for all brands.
    SQL Query: SELECT brand, SUM(price * stock_quantity) AS total_revenue FROM t_shirts WHERE size = 'L' GROUP BY brand;

    Question: How many shirts are available in stock grouped by colours from each size and finally show me all brands?
    SQL Query: SELECT brand, color, size, SUM(stock_quantity) AS total_stock FROM t_shirts GROUP BY brand, color, size;

    Question: select all the movies with minimum and maximum release_year. Note that there can be more than one movies in min and max year hence output rows can be more than 2?
    SQL Query: select * from movies where release_year in (select min(release_year) from movies, select max(release_year) from movies);

    Question: Generate a yearly report for Croma India where there are two columns 1. Fiscal Year and 2. Total Gross Sales amount In that year from Croma
    SQL Query: select get_fiscal_year(date) as fiscal_year, sum(round(sold_quantity*g.gross_price,2)) as yearly_sales from fact_sales_monthly s join fact_gross_price g on g.fiscal_year=get_fiscal_year(s.date) and g.product_code=s.product_code where customer_code=90002002 group by get_fiscal_year(date) order by fiscal_year;

    Question: What is the total freight cost incurred by each customer in the month of May 2024?
    SQL Query: SELECT s.customer_name, SUM(f.freight_cost) AS total_freight_cost FROM gdb0041.sales_monthly s JOIN gdb056.freight_cost f ON s.customer_id = f.customer_id WHERE s.month = 'May 2024' GROUP BY s.customer_id;

    Question: Which market has the highest gross price sales in the last quarter?
    SQL Query: SELECT s.market, SUM(g.gross_price) AS total_gross_price FROM gdb041.sales_monthly s JOIN gdb056.gross_price g ON s.market_id = g.market_id WHERE s.quarter = 'Q2 2024' GROUP BY s.market_id ORDER BY total_gross_price DESC LIMIT 1;

    Question: What is the manufacturing cost of products sold in each region last year?
    SQL Query: SELECT s.region, SUM(m.manufacturing_cost) AS total_manufacturing_cost FROM gdb0041.sales_monthly s JOIN gdb056.manufacturing_cost m ON s.product_id = m.product_id WHERE s.year = 2023 GROUP BY s.region;

    Question: How many pre-invoice deductions were applied to each customer's sales in the last six months?
    SQL Query: SELECT s.customer_name, COUNT(p.pre_invoice_deduction_id) AS total_pre_invoice_deductions FROM gdb041.sales_monthly s JOIN gdb056.pre_invoice_deductions p ON s.sales_id = p.sales_id WHERE s.date BETWEEN DATE_SUB(NOW(), INTERVAL 6 MONTH) AND NOW() GROUP BY s.customer_id;

    Question: What are the post-invoice deductions for each product in the current year?
    SQL Query: SELECT f.product_name, SUM(p.amount) AS total_post_invoice_deductions FROM gdb0041.forecast_monthly f JOIN gdb056.post_invoice_deductions p ON f.product_id = p.product_id WHERE YEAR(f.date) = YEAR(NOW()) GROUP BY f.product_id;

    Question: How many movies did Sanjay Dutt act in, and which film gained the most revenue, ordered from highest to lowest revenue?
    SQL Query: SELECT m.title, SUM(b.revenue) AS total_revenue FROM movies m JOIN box_office b ON m.movie_id = b.movie_id JOIN movie_actor ma ON m.movie_id = ma.movie_id JOIN actors a ON ma.actor_id = a.actor_id WHERE a.name = 'Sanjay Dutt' GROUP BY m.title ORDER BY total_revenue DESC;    
        

    Your turn:
    
    Question: {question}
    SQL Queries:
    '''
    
    prompt = ChatPromptTemplate.from_template(template)
    llm = llm

    def get_schema(_):
        schemas = {db_name: db.get_table_info() for db_name, db in dbs.items()}
        return schemas

    def parse_multi_line_queries(result):
        queries = {}
        lines = result.strip().split('\n')
        current_db = None
        current_query = []

        for line in lines:
            if ':' in line and not current_db:  # Only split on colon for the database name
                current_db, query_start = line.split(':', 1)
                current_db = current_db.strip()
                current_query.append(query_start.strip())
            else:
                current_query.append(line.strip())

        if current_db:
            queries[current_db] = ' '.join(current_query).strip()

        return queries
    
    return (
        RunnablePassthrough.assign(schemas=get_schema)
        | prompt
        | llm
        | StrOutputParser()
        | parse_multi_line_queries
    )

# Function to get response
def get_response(user_query, dbs, chat_history, llm):
    sql_chain = get_sql_chain(dbs, llm)
    
    template = '''
    You are a Senior and vastly experienced Data analyst at a company with around 20 years of experience.
    You are interacting with a user who is asking you questions about the company's databases.
    Based on the table schemas below, question, sql queries, and sql responses, write an 
    accurate natural language response so that the end user can understand things
    and make sure do not include words like 'Based on the SQL queries I ran'. 
    Just provide only the answer with some text that the user expects.

    <SCHEMAS>{schemas}</SCHEMAS>
    Conversation History: {chat_history}
    SQL Queries: <SQL>{queries}</SQL>
    User question: {question}
    SQL Responses: {responses}
    '''
    
    prompt = ChatPromptTemplate.from_template(template)
    llm = llm
   
    def run_queries(var):
        responses = {}
        for db_name, query in var['queries'].items():
            responses[db_name] = dbs[db_name].run(query)
            print(dbs.keys())
            print(var['queries'].keys())
        return responses
    
    chain = (
        RunnablePassthrough.assign(queries=sql_chain).assign(
            schemas=lambda _: {db_name: db.get_table_info() for db_name, db in dbs.items()},
            responses=run_queries,)  # The comma at the end of the assign() method call is used to indicate that there may be more keyword arguments or method calls following it
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return chain.invoke({
        'question': user_query,
        'chat_history': chat_history,
    })

# Streamlit app configuration
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = [
        AIMessage(content='Hello! I'm a SQL assistant. Ask me anything about your database.'),
    ]

st.set_page_config(page_title='Chat with MySQL', page_icon='🛢️')
st.title('Chat with MySQL')

# Define model options
model_options_groq = [
    'llama-3.1-70b-versatile',
    'llama-3.1-8b-instant',
    'llama3-70b-8192',
    'llama3-8b-8192',
    'mixtral-8x7b-32768',
    'gemma2-9b-it'
]

model_options_openai = [
    'gpt-4o',
    'gpt-4o-mini',
    'gpt-4-turbo',
    'gpt-3.5-turbo-0125'
]

with st.sidebar:
    st.subheader('Settings')
    st.write('This is a simple chat application using MySQL. Connect to the database and start chatting.')
    
    if 'db' not in st.session_state:
        st.session_state.user_id = st.text_input('User ID',placeholder='Enter any random numbers')
        st.session_state.Host = st.text_input('Host')
        st.session_state.Port = st.text_input('Port')
        st.session_state.User = st.text_input('User')
        st.session_state.Password = st.text_input('Password', type='password')
        st.session_state.Databases = st.text_input('Databases', placeholder='Enter DB's separated by (,)')
        st.session_state.openai_api_key = st.text_input('OpenAI API Key', type='password', help='Get your API key from [OpenAI Website](https://platform.openai.com/api-keys)')
        selected_model_openai = st.selectbox('Select any OpenAI Model', model_options_openai)
        st.session_state.groq_api_key = st.text_input('Groq API Key', type='password', help='Get your API key from [GROQ Console](https://console.groq.com/keys)')
        selected_model_groq = st.selectbox('Select any Groq Model', model_options_groq)
        st.info('Note: For interacting multiple databases and dealing with complex queries, GPT-4 Model is recommended for accurate results else proceed with Groq Model')
        
       
        os.environ['OPENAI_API_KEY'] = str(st.session_state.openai_api_key)
       

        if st.button('Connect'):
            with st.spinner('Connecting to databases...'):

                # Update config.toml with user-specific connection details
                update_secrets_file(st.session_state.user_id, {
                    'Host': st.session_state.Host,
                    'Port': st.session_state.Port,
                    'User': st.session_state.User,
                    'Password': st.session_state.Password,
                    'Databases': st.session_state.Databases
                })

                dbs = init_databases(st.session_state.user_id)
                st.session_state.dbs = dbs

                if len(dbs) > 1:
                    st.success(f'Connected to {len(dbs)} databases')
                else:
                    st.success('Connected to database')
                
                

        if st.session_state.openai_api_key == '' and st.session_state.groq_api_key == '':
            st.error('Enter one API Key At least')
        elif st.session_state.openai_api_key:    
            st.session_state.llm = ChatOpenAI(model=selected_model_openai, api_key=st.session_state.openai_api_key)
        elif st.session_state.groq_api_key:
            st.session_state.llm = ChatGroq(model_name=selected_model_groq, temperature=0.5, api_key=st.session_state.groq_api_key)
        else:
            pass

# Display chat messages
for message in st.session_state.chat_history:
    if isinstance(message, AIMessage):
        with st.chat_message('AI'):
            st.markdown(message.content)
    elif isinstance(message, HumanMessage):
        with st.chat_message('Human'):
            st.markdown(message.content)

# Handle user input
user_query = st.chat_input('Type a message...')
if user_query is not None and user_query.strip() != '':
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    
    with st.chat_message('Human'):
        st.markdown(user_query)
        
    with st.chat_message('AI'):
        response = get_response(user_query, st.session_state.dbs, st.session_state.chat_history, st.session_state.llm)
        st.markdown(response)
        
    st.session_state.chat_history.append(AIMessage(content=response))


================================================
File: requirements.txt
================================================
streamlit
langchain
langchain-community
langchain-core
langchain-openai
mysql-connector-python
groq
langchain-groq
toml
sqlalchemy



================================================
File: .devcontainer/devcontainer.json
================================================
{
  'name': 'Python 3',
  // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile
  'image': 'mcr.microsoft.com/devcontainers/python:1-3.11-bullseye',
  'customizations': {
    'codespaces': {
      'openFiles': [
        'README.md',
        'app.py'
      ]
    },
    'vscode': {
      'settings': {},
      'extensions': [
        'ms-python.python',
        'ms-python.vscode-pylance'
      ]
    }
  },
  'updateContentCommand': '[ -f packages.txt ] && sudo apt update && sudo apt upgrade -y && sudo xargs apt install -y <packages.txt; [ -f requirements.txt ] && pip3 install --user -r requirements.txt; pip3 install --user streamlit; echo '✅ Packages installed and Requirements met'',
  'postAttachCommand': {
    'server': 'streamlit run app.py --server.enableCORS false --server.enableXsrfProtection false'
  },
  'portsAttributes': {
    '8501': {
      'label': 'Application',
      'onAutoForward': 'openPreview'
    }
  },
  'forwardPorts': [
    8501
  ]
}


        """,
        "repo_created_at": "2025-02-12T13:08:47Z",
        "last_updated": "2025-02-13T01:15:54Z",
        "stars": 0
    },
    "Comparison-of-Supervised-and-Unsupervised-Image-Recognition": {
        "github_url": "https://github.com/Parthiban-3997/Comparison-of-Supervised-and-Unsupervised-Image-Recognition",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Comparison-of-Supervised-and-Unsupervised-Image-Recognition",
        "description": """
        ================================================
File: README.md
================================================
# Comparison of Supervised and Unsupervised Image Recognition

## Image Classification and Denoising with CIFAR-10

This document summarizes the processes, results, and analysis of applying different machine learning models on the CIFAR-10 dataset for image classification and image denoising tasks.

## 1. Image Classification

Two different approaches were implemented for image classification:

### 1.1 Support Vector Machine (SVM)

#### Training:
- The CIFAR-10 dataset was loaded using TensorFlow.
- Pixel values were preprocessed by normalizing to a range of 0-1 and flattening the 32x32x3 image arrays into 1D vectors.
- An SVM classifier with an RBF kernel was trained on the training set using all available CPU cores (n_jobs=-1) for faster training. GPU acceleration was attempted but not found to be beneficial for this specific task and model.
- The trained model was saved to svm_model.pkl using pickle.

#### Testing:
- The saved SVM model was loaded.
- Predictions were made on the preprocessed test set.
- Accuracy: 54%
- Classification Report: The classification report revealed that the model performs decently for some classes like 'airplane', 'automobile', and 'ship' but struggles with others like 'cat', 'deer', and 'frog'. This suggests that the features learned by the SVM with an RBF kernel might not be discriminative enough for all classes in the CIFAR-10 dataset.

#### Prediction Examples:
- Predictions were made on a separate set of images stored in the Images/ directory.
- The predicted class labels were displayed alongside the images.
- Predictions were also visualized on a subset of the test set images to further illustrate the model's performance.

### 1.2 Convolutional Neural Network (CNN)

#### Training:
- The CIFAR-10 dataset was loaded and preprocessed similarly to the SVM approach.
- The training set was further split into training and validation sets (80%-20% split).
- A CNN model was defined using the Keras Sequential API. The architecture included:
  - Convolutional layers with ReLU activation
  - Max-pooling layers
  - Dropout for regularization
  - Dense layers with ReLU activation
  - A final dense layer with softmax activation for multi-class classification
- The model was compiled using the Adam optimizer with a learning rate of 0.001 and sparse categorical cross-entropy loss.
- Training was performed for 20 epochs with a batch size of 32.
- Training and validation accuracy and loss were plotted.
- Test Accuracy: 76.28%

#### Model Summary:
- The model.summary() function provided a detailed overview of the CNN architecture, including the output shape and number of parameters for each layer.

#### Saving the Model:
- The trained CNN model was saved to CNNFinal_Model.h5.

#### Evaluation:
- The saved CNN model was loaded.
- Predictions were made on the test set, and a classification report was generated.
- Classification Report: The CNN model significantly outperformed the SVM, achieving an accuracy of 76.28%. This highlights the superiority of CNNs for image classification tasks due to their ability to learn spatial hierarchies of features.

#### Prediction Examples:
- Similar to the SVM, predictions were visualized for images from the Images/ directory and a subset of the test set.

## 2. Image Denoising

An Autoencoder was implemented for image denoising:

### 2.1 Autoencoder

#### Training:
- The CIFAR-10 dataset was loaded, and Gaussian noise was added to the images to create a noisy dataset.
- An autoencoder model was defined using the Keras Sequential API. The architecture consisted of:
  - Encoder: Convolutional and MaxPooling layers to learn a compressed representation of the input.
  - Decoder: Convolutional and UpSampling layers to reconstruct the denoised image from the compressed representation.
- The model was compiled using the Adam optimizer and mean squared error loss.
- Training was performed for a specified number of epochs.
- The trained autoencoder model was saved to Denoising_Model_Final.h5.

#### Testing:
- The saved autoencoder model was loaded.
- The model was evaluated on a noisy test set.
- Test Loss: 22982.15
- Test Accuracy: 33.79%

#### Prediction Examples:
- A visualize_data function was defined to display grids of images.
- The function was used to visualize:
  - Noisy images from the test set.
  - Denoised images predicted by the autoencoder.
  - Original clean images for comparison.
- The autoencoder's denoising capability was further demonstrated on:
  - Images with smaller resolutions.
  - Images with larger resolutions.

## 3. Image Classification using K-means with PCA

#### Training:
- The CIFAR-10 dataset was loaded and preprocessed by normalizing pixel values and flattening the images.
- PCA (Principal Component Analysis) was applied to reduce the dimensionality of the data while retaining 99% of the variance.
- A K-means clustering model was trained on the PCA-transformed training data with 10 clusters, representing the 10 classes in CIFAR-10.
- The trained K-means model was saved to kmeans_model.pkl.

#### Evaluation:
- The saved K-means model was loaded.
- Silhouette Score: 0.0538
- Davies-Bouldin Score: 2.7032
- These scores suggest that the clustering is not very distinct, which is expected given the complexity of image data and the limitations of K-means in high-dimensional spaces.

#### Test Accuracy: 22.11%

#### Prediction Examples:
- Predictions were visualized on a subset of the test set images.
- Predictions were also made and displayed for images from the Images/ directory.

## Conclusion

This project explored different machine learning techniques for image classification and denoising on the CIFAR-10 dataset. The CNN model achieved the highest accuracy (76.28%) for image classification, demonstrating the power of deep learning for this task. The autoencoder showed promising results in denoising images, while the K-means approach, though less accurate, provided insights into clustering image data. Future work could involve exploring more complex CNN architectures, fine-tuning hyperparameters, and experimenting with different autoencoder designs for improved performance.


        """,
        "repo_created_at": "2025-02-12T13:08:51Z",
        "last_updated": "2025-02-13T00:52:45Z",
        "stars": 0
    },
    "docker-llm-sql-gpu-stack": {
        "github_url": "https://github.com/Parthiban-3997/docker-llm-sql-gpu-stack",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/docker-llm-sql-gpu-stack",
        "description": """
        ================================================
File: README.md
================================================
# ChatDB GPU - Docker Setup Guide

This repository contains a Docker-based setup for running a Streamlit application with Ollama, optimized for GPU usage, supporting both .GGUF and Ollama models.

## Prerequisites

### System Requirements
- NVIDIA GPU
- Docker Desktop
- WSL2 (for Windows users)
- NVIDIA GPU drivers
- NVIDIA Container Toolkit

### Windows-Specific Setup
1. Install and Configure WSL2
   ```bash
   wsl --install
   wsl --set-default-version 2
   ```

2. Docker Desktop Settings
   - Enable 'Use WSL 2 based engine' in Settings > General
   - Enable WSL Integration in Settings > Resources > WSL Integration

3. Verify GPU Support
   ```bash
   docker run --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
   ```

## Installation

1. Pull Docker Images
   ```bash
   docker pull parthi97/llm-sql-gpu:latest
   docker pull parthi97/ollama:latest
   ```

2. Create `docker-compose.yml`
   ```yaml
   version: '3.8'
   services:
     streamlit:
       image: parthi97/llm-sql-gpu:latest
       ports:
         - '8501:8501'
       deploy:
         resources:
           reservations:
             devices:
               - driver: nvidia
                 count: 1
                 capabilities: [gpu]
       networks:
         - llm_network
       environment:
         - OLLAMA_HOST=http://host.docker.internal:11434
         - NVIDIA_VISIBLE_DEVICES=all
         - NVIDIA_DRIVER_CAPABILITIES=compute,utility
       volumes:
         - model_data:/app/models
       depends_on:
         - ollama

     ollama:
       image: parthi97/ollama:latest
       ports:
         - '11434:11434'
       environment:
         - OLLAMA_HOST=0.0.0.0
         - OLLAMA_MODELS_MEMORY_USAGE=60
         - CUDA_VISIBLE_DEVICES=0
         - NVIDIA_VISIBLE_DEVICES=all
         - NVIDIA_DRIVER_CAPABILITIES=compute,utility
         - CUDA_LAUNCH_BLOCKING=1
         - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
       deploy:
         resources:
           reservations:
             devices:
               - driver: nvidia
                 count: 1
                 capabilities: [gpu]
               options:
                 memory: 3G
       volumes:
         - model_data:/root/.ollama
       networks:
         - llm_network

   networks:
     llm_network:
       driver: bridge

   volumes:
     model_data:
   ```

## Running the Application

### Method : Using Command Line (From the current working directory)
```bash
docker-compose up
```

## Container Management

### Stopping Running Containers

Before setting up .GGUF models, you should stop any running streamlit containers to prevent conflicts:

1. List running containers:
   ```bash
   docker ps
   ```

2. Stop Streamlit container:
   ```bash
   docker stop <STREAMLIT_CONTAINER_ID>
   ```



### Example Workflow

Here's a step-by-step example of stopping containers and setting up a .GGUF model:

1. List running containers:
   ```bash
   # This will show container IDs and names
   docker ps
   ```
   Example output:
   ```
   CONTAINER ID   IMAGE                           NAMES
   a1b2c3d4e5f6   parthi97/llm-sql-gpu:latest     chatdb_streamlit
   ```

2. Stop the containers:
   ```bash
   # Replace with your actual container IDs
   docker stop a1b2c3d4e5f6  # Streamlit container
   ```

## .GGUF Model Setup

### Mounting .GGUF Models

To use a local .GGUF model:

1. Prepare your .GGUF file:
   - Ensure the model is in an accessible location
   - Use the full, absolute path to the model file

2. Run the container with model mounting:
   ```bash
   docker run --gpus all \
     -v <FULL_PATH_TO_GGUF_MODEL>:/app/models/model.gguf \
     -p 8501:8501 \
     parthi97/llm-sql-gpu:latest
   ```

### Real-World Example

For Windows users with a model in `G:\chatdb_llm\models\`:

```bash
# Example with a specific model
docker run --gpus all \
  -v G:\chatdb_llm\models\natural-sql-7b.Q4_K_M.gguf:/app/models/model.gguf \
  -p 8501:8501 \
  parthi97/llm-sql-gpu:latest
```



2. In the Streamlit UI:
   - Under 'Model Path' input field, enter: `/app/models/model.gguf`
   - This path maps to your locally mounted .GGUF file

### Using Ollama Models
1. First, identify the Ollama container ID:
   ```bash
   docker ps
   ```
   Look for the container running the `parthi97/ollama:latest` image

2. Pull the desired model using the container ID:
   ```bash
   docker exec -it <container_id> ollama pull <model_name>
   ```

   Example:
   ```bash
   docker exec -it 8ff644f8a2c7 ollama pull llama3.1:8b
   ```

3. In the Streamlit UI:
   - Select 'Ollama' as your model type
   - Choose the pulled model from the available options

### Model Usage Best Practices
1. For .GGUF Models:
   - Ensure the model file is in a location accessible to Docker
   - Use absolute paths when mounting volumes
   - Verify file permissions are correct

2. For Ollama Models:
   - Pull models after the container is running
   - Monitor GPU memory usage while pulling models
   - Consider available GPU memory when selecting models

3. General Recommendations:
   - Start with smaller models if GPU memory is limited
   - Monitor system resources during model loading
   - Ensure proper model path configuration in Streamlit UI

## Accessing the Application
- Streamlit Interface: http://localhost:8501
- Ollama API: http://localhost:11434

## Verification and Troubleshooting

### Check Container Status
```bash
docker ps
docker-compose logs streamlit
docker-compose logs ollama
```

### Verify GPU Access
```bash
nvidia-smi
docker run --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
```

### Common Issues and Solutions
1. GPU not detected:
   - Verify NVIDIA drivers are installed
   - Check Docker Desktop GPU settings
   - Ensure WSL2 GPU support is enabled

2. Container fails to start:
   - Check docker-compose logs
   - Verify all ports are available
   - Ensure sufficient GPU memory

3. Model-Related Issues:
   - Model Not Found:
     - Verify the correct path is mounted for .GGUF models
     - Check if Ollama models were pulled successfully
     - Ensure proper permissions on model files
   
   - GPU Memory Issues:
     - Monitor GPU memory usage with `nvidia-smi`
     - Consider using quantized models for better memory efficiency
     - Close other GPU-intensive applications
   
   - Model Loading Errors:
     - Check container logs for specific error messages
     - Verify model compatibility with the system
     - Ensure sufficient disk space for model files

## Stopping the Application
```bash
docker-compose down        # Stop containers
docker-compose down -v     # Stop containers and remove volumes
```

## Resource Requirements
- Minimum 4GB GPU Memory
- Approximately 10GB Disk Space for images
- NVIDIA GPU with CUDA support


================================================
File: docker-compose.yml
================================================
version: '3.8'
services:
  streamlit:
    image: parthi97/llm-sql-gpu:latest
    ports:
      - '8501:8501'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - llm_network
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - model_data:/app/models
    depends_on:
      - ollama

  ollama:
    image: parthi97/ollama:latest  # Updated image name
    ports:
      - '11434:11434'
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS_MEMORY_USAGE=60
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_LAUNCH_BLOCKING=1
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
              options:
                memory: 3G
    volumes:
      - model_data:/root/.ollama
    networks:
      - llm_network

networks:
  llm_network:
    driver: bridge

volumes:
  model_data:

================================================
File: sample-movies-db.sql
================================================
CREATE DATABASE  IF NOT EXISTS `moviesdb` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci */ /*!80016 DEFAULT ENCRYPTION='N' */;
USE `moviesdb`;
-- MySQL dump 10.13  Distrib 8.0.31, for Win64 (x86_64)
--
-- Host: localhost    Database: moviesdb
-- ------------------------------------------------------
-- Server version	8.0.31

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `actors`
--

DROP TABLE IF EXISTS `actors`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `actors` (
  `actor_id` int unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(45) NOT NULL,
  `birth_year` year DEFAULT NULL,
  PRIMARY KEY (`actor_id`)
) ENGINE=InnoDB AUTO_INCREMENT=171 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `actors`
--

LOCK TABLES `actors` WRITE;
/*!40000 ALTER TABLE `actors` DISABLE KEYS */;
INSERT INTO `actors` VALUES (50,'Yash',1986),(51,'Sanjay Dutt',1959),(52,'Benedict Cumberbatch',1976),(53,'Elizabeth Olsen',1989),(54,'Chris Hemsworth',1983),(55,'Natalie Portman',1981),(56,'Tom Hiddleston',1981),(57,'Amitabh Bachchan',1942),(58,'Jaya Bachchan',1948),(59,'Shah Rukh Khan',1965),(60,'Kajol',1974),(61,'Aamir Khan',1965),(62,'R. Madhavan',1970),(63,'Sharman Joshi',1979),(64,'Hrithik Roshan',1974),(65,'Ranveer Singh',1985),(66,'Deepika Padukone',1986),(67,'Tim Robbins',1958),(68,'Morgan Freeman',1937),(69,'Leonardo DiCaprio',1974),(70,'Ken Watanabe',1959),(71,'Matthew McConaughey',1969),(72,'Anne Hathaway',1982),(73,'John David Washington',1984),(74,'Robert Pattinson',1986),(75,'Will Smith',1968),(76,'Thandiwe Newton',1972),(77,'Russell Crowe',1964),(78,'Joaquin Phoenix',1974),(79,'Kate Winslet',1975),(80,'James Stewart',1908),(81,'Donna Reed',1921),(82,'Sam Worthington',1976),(83,'Zoe Saldana',1978),(84,'Marlon Brando',1924),(85,'Al Pacino',1940),(86,'Christian Bale',1974),(87,'Heath Ledger',1979),(88,'Liam Neeson',1952),(89,'Ben Kingsley',1943),(90,'Sam Neill',1947),(91,'Laura Dern',1967),(92,'Song Kang-ho',1967),(93,'Lee Sun-kyun',1975),(94,'Robert Downey Jr.',1965),(95,'Chris Evans',1981),(150,'Kanu Banerjee',1905),(151,'Karuna Banerjee',1919),(152,'Darsheel Safary',1997),(153,'Sunil Dutt',1929),(154,'Anushka Sharma',1988),(155,'Ranbir Kapoor',1982),(156,'Allu Arjun',1982),(157,'Fahadh Faasil',1982),(158,'N. T. Rama Rao Jr.',1983),(159,'Ram Charan',1985),(160,'Prabhas',1979),(161,'Rana Daggubati',1984),(162,'Mithun Chakraborty',1950),(163,'Anupam Kher',1955),(164,'Salman Khan',1965),(165,'Nawazuddin Siddiqui',1967),(166,'Tommy Lee Jones',1946),(167,'Sebastian Stan',1982),(168,'Anil Kapoor',1956),(169,'Sidharth Malhotra',1985),(170,'Kiara Advani',1991);
/*!40000 ALTER TABLE `actors` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `financials`
--

DROP TABLE IF EXISTS `financials`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `financials` (
  `movie_id` int unsigned NOT NULL,
  `budget` decimal(10,2) DEFAULT NULL,
  `revenue` decimal(10,2) DEFAULT NULL,
  `unit` enum('Units','Thousands','Millions','Billions') DEFAULT NULL,
  `currency` char(3) DEFAULT NULL,
  PRIMARY KEY (`movie_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `financials`
--

LOCK TABLES `financials` WRITE;
/*!40000 ALTER TABLE `financials` DISABLE KEYS */;
INSERT INTO `financials` VALUES (101,1.00,12.50,'Billions','INR'),(102,200.00,954.80,'Millions','USD'),(103,165.00,644.80,'Millions','USD'),(104,180.00,854.00,'Millions','USD'),(105,250.00,670.00,'Millions','USD'),(107,400.00,2000.00,'Millions','INR'),(108,550.00,4000.00,'Millions','INR'),(109,390.00,1360.00,'Millions','INR'),(110,1.40,3.50,'Billions','INR'),(111,25.00,73.30,'Millions','USD'),(113,165.00,701.80,'Millions','USD'),(114,205.00,365.30,'Millions','USD'),(115,55.00,307.10,'Millions','USD'),(116,103.00,460.50,'Millions','USD'),(117,200.00,2202.00,'Millions','USD'),(118,3.18,3.30,'Millions','USD'),(119,237.00,2847.00,'Millions','USD'),(120,7.20,291.00,'Millions','USD'),(121,185.00,1006.00,'Millions','USD'),(122,22.00,322.20,'Millions','USD'),(123,63.00,1046.00,'Millions','USD'),(124,15.50,263.10,'Millions','USD'),(125,400.00,2798.00,'Millions','USD'),(126,400.00,2048.00,'Millions','USD'),(127,70000.00,100000.00,'Thousands','INR'),(128,120.00,1350.00,'Millions','INR'),(129,100.00,410.00,'Millions','INR'),(130,850.00,8540.00,'Millions','INR'),(131,1.00,5.90,'Billions','INR'),(132,2.00,3.60,'Billions','INR'),(133,5.50,12.00,'Billions','INR'),(134,1.80,6.50,'Billions','INR'),(135,250.00,3409.00,'Millions','INR'),(136,900.00,11690.00,'Millions','INR'),(137,216.70,370.60,'Millions','USD'),(138,177.00,714.40,'Millions','USD'),(139,1.80,3.10,'Billions','INR'),(140,500.00,950.00,'Millions','INR'),(406,30.00,350.00,'Millions','INR'),(412,160.00,836.80,'Millions','USD');
/*!40000 ALTER TABLE `financials` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `languages`
--

DROP TABLE IF EXISTS `languages`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `languages` (
  `language_id` tinyint unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(45) NOT NULL,
  PRIMARY KEY (`language_id`),
  UNIQUE KEY `name_UNIQUE` (`name`)
) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `languages`
--

LOCK TABLES `languages` WRITE;
/*!40000 ALTER TABLE `languages` DISABLE KEYS */;
INSERT INTO `languages` VALUES (7,'Bengali'),(5,'English'),(6,'French'),(8,'Gujarati'),(1,'Hindi'),(3,'Kannada'),(4,'Tamil'),(2,'Telugu');
/*!40000 ALTER TABLE `languages` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `movie_actor`
--

DROP TABLE IF EXISTS `movie_actor`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `movie_actor` (
  `movie_id` int unsigned NOT NULL,
  `actor_id` int unsigned NOT NULL,
  PRIMARY KEY (`movie_id`,`actor_id`),
  KEY `fk_movie_actor_movies_idx` (`movie_id`),
  KEY `fk_movie_actor_actors_idx` (`actor_id`),
  CONSTRAINT `fk_movie_actor_actors` FOREIGN KEY (`actor_id`) REFERENCES `actors` (`actor_id`) ON DELETE RESTRICT ON UPDATE CASCADE,
  CONSTRAINT `fk_movie_actor_movies` FOREIGN KEY (`movie_id`) REFERENCES `movies` (`movie_id`) ON DELETE RESTRICT ON UPDATE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `movie_actor`
--

LOCK TABLES `movie_actor` WRITE;
/*!40000 ALTER TABLE `movie_actor` DISABLE KEYS */;
INSERT INTO `movie_actor` VALUES (101,50),(101,51),(102,52),(102,53),(103,54),(103,55),(103,56),(104,54),(104,56),(105,54),(105,55),(106,57),(106,58),(107,59),(107,60),(108,61),(108,62),(108,63),(109,57),(109,59),(109,64),(110,65),(110,66),(111,67),(111,68),(112,69),(112,70),(113,71),(113,72),(115,75),(115,76),(116,77),(116,78),(117,69),(117,79),(118,80),(118,81),(119,82),(119,83),(120,84),(120,85),(121,86),(121,87),(122,88),(122,89),(123,90),(123,91),(124,92),(124,93),(125,54),(125,94),(125,95),(126,54),(126,94),(126,95),(127,150),(127,151),(128,61),(128,152),(129,51),(129,153),(130,61),(130,154),(131,154),(131,155),(132,156),(132,157),(133,158),(133,159),(134,160),(134,161),(135,162),(135,163),(136,164),(136,165),(137,95),(137,166),(138,95),(138,167),(139,164),(139,168),(140,169),(140,170);
/*!40000 ALTER TABLE `movie_actor` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `movies`
--

DROP TABLE IF EXISTS `movies`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `movies` (
  `movie_id` int unsigned NOT NULL AUTO_INCREMENT,
  `title` varchar(150) NOT NULL,
  `industry` varchar(45) DEFAULT NULL,
  `release_year` year DEFAULT NULL,
  `imdb_rating` decimal(3,1) DEFAULT NULL,
  `studio` varchar(45) DEFAULT NULL,
  `language_id` tinyint unsigned DEFAULT NULL,
  PRIMARY KEY (`movie_id`),
  KEY `fk_movies_languages_idx` (`language_id`),
  CONSTRAINT `fk_movies_languages` FOREIGN KEY (`language_id`) REFERENCES `languages` (`language_id`) ON DELETE RESTRICT ON UPDATE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=141 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `movies`
--

LOCK TABLES `movies` WRITE;
/*!40000 ALTER TABLE `movies` DISABLE KEYS */;
INSERT INTO `movies` VALUES (101,'K.G.F: Chapter 2','Bollywood',2022,8.4,'Hombale Films',3),(102,'Doctor Strange in the Multiverse of Madness','Hollywood',2022,7.0,'Marvel Studios',5),(103,'Thor: The Dark World ','Hollywood',2013,6.8,'Marvel Studios',5),(104,'Thor: Ragnarok ','Hollywood',2017,7.9,'Marvel Studios',5),(105,'Thor: Love and Thunder ','Hollywood',2022,6.8,'Marvel Studios',5),(106,'Sholay','Bollywood',1975,8.1,'United Producers',1),(107,'Dilwale Dulhania Le Jayenge','Bollywood',1995,8.0,'Yash Raj Films',1),(108,'3 Idiots','Bollywood',2009,8.4,'Vinod Chopra Films',1),(109,'Kabhi Khushi Kabhie Gham','Bollywood',2001,7.4,'Dharma Productions',1),(110,'Bajirao Mastani ','Bollywood',2015,7.2,'',1),(111,'The Shawshank Redemption','Hollywood',1994,9.3,'Castle Rock Entertainment',5),(112,'Inception','Hollywood',2010,8.8,'Warner Bros. Pictures',5),(113,'Interstellar','Hollywood',2014,8.6,'Warner Bros. Pictures',5),(115,'The Pursuit of Happyness','Hollywood',2006,8.0,'Columbia Pictures',5),(116,'Gladiator','Hollywood',2000,8.5,'Universal Pictures  ',5),(117,'Titanic','Hollywood',1997,7.9,'Paramount Pictures',5),(118,'It\'s a Wonderful Life','Hollywood',1946,8.6,'Liberty Films',5),(119,'Avatar','Hollywood',2009,7.8,'20th Century Fox',5),(120,'The Godfather','Hollywood',1972,9.2,'Paramount Pictures',5),(121,'The Dark Knight','Hollywood',2008,9.0,'Syncopy',5),(122,'Schindler\'s List','Hollywood',1993,9.0,'Universal Pictures',5),(123,'Jurassic Park','Hollywood',1993,8.2,'Universal Pictures',5),(124,'Parasite','Hollywood',2019,8.5,'',5),(125,'Avengers: Endgame','Hollywood',2019,8.4,'Marvel Studios',5),(126,'Avengers: Infinity War','Hollywood',2018,8.4,'Marvel Studios',5),(127,'Pather Panchali','Bollywood',1955,8.3,'Government of West Bengal',7),(128,'Taare Zameen Par','Bollywood',2007,8.3,'',1),(129,'Munna Bhai M.B.B.S.','Bollywood',2003,8.1,'Vinod Chopra Productions',1),(130,'PK','Bollywood',2014,8.1,'Vinod Chopra Films',1),(131,'Sanju','Bollywood',2018,NULL,'Vinod Chopra Films',1),(132,'Pushpa: The Rise - Part 1','Bollywood',2021,7.6,'Mythri Movie Makers',2),(133,'RRR','Bollywood',2022,8.0,'DVV Entertainment',2),(134,'Baahubali: The Beginning','Bollywood',2015,8.0,'Arka Media Works',2),(135,'The Kashmir Files','Bollywood',2022,8.3,'Zee Studios',1),(136,'Bajrangi Bhaijaan','Bollywood',2015,8.1,'Salman Khan Films',1),(137,'Captain America: The First Avenger','Hollywood',2011,6.9,'Marvel Studios',5),(138,'Captain America: The Winter Soldier','Hollywood',2014,7.8,'Marvel Studios',5),(139,'Race 3','Bollywood',2018,1.9,'Salman Khan Films',1),(140,'Shershaah','Bollywood',2021,8.4,'Dharma Productions',1);
/*!40000 ALTER TABLE `movies` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2022-11-12 12:32:35



        """,
        "repo_created_at": "2025-02-12T13:08:56Z",
        "last_updated": "2025-02-13T00:52:13Z",
        "stars": 0
    },
    "DocMulti-Chat-Assistant-Using-LlamaIndex": {
        "github_url": "https://github.com/Parthiban-3997/DocMulti-Chat-Assistant-Using-LlamaIndex",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/DocMulti-Chat-Assistant-Using-LlamaIndex",
        "description": """
        ================================================
File: README.md
================================================
# DocMulti Chat Assistant Using LlamaIndex 🦙

## Overview
DocMulti Chat Assistant is a powerful Streamlit-based application that leverages LlamaIndex to provide an interactive chat interface for querying multiple documents. This tool excels at processing and understanding complex data structures, ensuring that no information is lost during the parsing and indexing stages.


## Deployed Link
DocMulti-Chat-Assistant app is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/DocMulti_Chat_Assistant_Using_LlamaIndex)


## Screenshots
![llamaindex_1](https://github.com/user-attachments/assets/15b00af3-e8a6-4a09-a0ab-b5bf9fe53f54)
![llamaindex_2](https://github.com/user-attachments/assets/7d8925f2-6be2-4588-a0ad-d88ec14c8dec)
![llamaindex_3](https://github.com/user-attachments/assets/595e7cd3-d99a-41c4-8a2a-66efddc9b4aa)
![llamaindex_4](https://github.com/user-attachments/assets/7224ff3e-e47a-471f-bceb-6a699f1d228c)

### Examples from uploaded document file structures
![llamaindex_5](https://github.com/user-attachments/assets/c1b1b728-eb23-4ebf-a92b-9712ca8bff9c)

## Features
- **Multi-Document Support**: Upload and process multiple document types.
- **Advanced Parsing**: Option to use LlamaParse for complex documents with graphs and tables.
- **Customizable Models**: Choose from various Groq models for text generation.
- **Flexible Embedding**: Utilizes Google's Gemini for document embedding.
- **Interactive Chat Interface**: Engage in a conversation about your documents.
- **Memory Retention**: Maintains context across multiple queries using a chat memory buffer.
- **Custom Parsing Instructions**: Tailor the document parsing process with custom instructions.
- **Custom Prompt Templates**: Define your own prompts for more specific interactions.

## Components
- **Streamlit**: For the web interface.
- **LlamaIndex**: Core indexing and querying engine.
- **Groq**: Large Language Model for text generation.
- **Google Gemini**: For document embedding.
- **LlamaParse**: Optional advanced document parser.

## Usage
1. Launch the Streamlit app.
2. In the sidebar:
   - Enter your API keys for Groq, Google, and Llama Cloud.
   - Select a Groq model.
   - Upload your documents.
   - Choose whether to use LlamaParse.
   - Set any advanced options like custom parsing instructions or prompt templates.
3. Click 'Start Document Indexing' to process your documents.
4. Once indexed, use the chat interface to ask questions about your documents.

## Supported Document Types
DocMulti Chat Assistant supports an extensive range of document formats, making it incredibly versatile for various use cases. Here's the full list of supported extensions:

- **Text Documents**: .txt, .rtf, .md
- **Microsoft Office**: .doc, .docx, .docm, .dot, .dotm, .xls, .xlsx, .xlsm, .xlsb, .ppt, .pptx, .pptm, .pot, .potm, .potx
- **OpenDocument**: .odt, .ods, .odp
- **PDF**: .pdf
- **eBooks**: .epub
- **Images**: .jpg, .jpeg, .png, .gif, .bmp, .svg, .tiff, .webp
- **Web**: .htm, .html, .xml
- **Spreadsheets**: .csv, .tsv, .dif, .sylk, .slk, .prn
- **Database**: .dbf
- **Other Office Suites**: 
  - Apple iWork: .pages, .numbers, .key
  - WordPerfect: .wpd
  - Lotus: .wks, .123
  - Quattro Pro: .qpw
- **Specialized Formats**: .602, .abw, .cgm, .cwk, .hwp, .lwp, .mw, .mcw, .pbd, .sda, .sdd, .sdp, .sdw, .sgl, .sti, .sxi, .sxw, .uof, .uop, .uot, .vor, .wps, .zabw, .et, .eth, .wk1, .wk2, .wk3, .wk4, .wq1, .wq2, .wb1, .wb2, .wb3, .xlr

## Advanced Features
- **Custom Parsing Instructions**: Tailor how LlamaParse extracts information from your documents.
- **Custom Prompt Templates**: Create specific prompts to guide the AI's responses.
- **Adjustable Model Parameters**: Select different Groq models to balance between speed and capability.

## Notes
- Ensure you have valid API keys for Groq, Google, and Llama Cloud before using the application.
- Processing time may vary depending on the number and complexity of uploaded documents.

## Troubleshooting
- If you encounter errors during document processing, check your API keys and internet connection.
- For issues with specific document types, try toggling the LlamaParse option.

## Contributing
Contributions to improve DocMulti Chat Assistant are welcome. Please submit pull requests or open issues on the project repository.



================================================
File: app.py
================================================
import streamlit as st
import os
import tempfile
from dotenv import load_dotenv
from llama_parse import LlamaParse
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.llms.groq import Groq
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.query_engine import RetrieverQueryEngine
from langchain_core.messages import HumanMessage, AIMessage
from llama_index.core.memory import ChatMemoryBuffer
import time

load_dotenv()

st.set_page_config(page_title='Chat with Documents', page_icon=':books:')
st.title('DocMulti Chat Assistant Using LlamaIndex 🦙')

# Initialize chat history in session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# Initialize memory buffer
if 'memory' not in st.session_state:
    st.session_state.memory = ChatMemoryBuffer.from_defaults(token_limit=4090)

SUPPORTED_EXTENSIONS = [
    '.pdf', '.602', '.abw', '.cgm', '.cwk', '.doc', '.docx', '.docm', '.dot', '.dotm',
    '.hwp', '.key', '.lwp', '.mw', '.mcw', '.pages', '.pbd', '.ppt', '.pptm', '.pptx',
    '.pot', '.potm', '.potx', '.rtf', '.sda', '.sdd', '.sdp', '.sdw', '.sgl', '.sti',
    '.sxi', '.sxw', '.stw', '.sxg', '.txt', '.uof', '.uop', '.uot', '.vor', '.wpd',
    '.wps', '.xml', '.zabw', '.epub', '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg',
    '.tiff', '.webp', '.htm', '.html', '.xlsx', '.xls', '.xlsm', '.xlsb', '.xlw', '.csv',
    '.dif', '.sylk', '.slk', '.prn', '.numbers', '.et', '.ods', '.fods', '.uos1', '.uos2',
    '.dbf', '.wk1', '.wk2', '.wk3', '.wk4', '.wks', '.123', '.wq1', '.wq2', '.wb1', '.wb2',
    '.wb3', '.qpw', '.xlr', '.eth', '.tsv'
]

# Sidebar configuration
if 'config' not in st.session_state:
    with st.sidebar:
        st.header('Configuration')
        st.markdown('Enter your API keys below:')

        # GROQ API Key input
        st.session_state.groq_api_key = st.text_input(
            'Enter your GROQ API Key', 
            type='password', 
            help='Get your API key from [GROQ Console](https://console.groq.com/keys)'
        )
        
        # Google API Key input
        st.session_state.google_api_key = st.text_input(
            'Enter your Google API Key', 
            type='password', 
            help='Get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey)'
        )

        # Llama Cloud API Key input
        st.session_state.llama_cloud_api_key = st.text_input(
            'Enter your Llama Cloud API Key', 
            type='password', 
            help='Get your API key from [Llama Cloud](https://cloud.llamaindex.ai/api-key)'
        )

        # Set environment variables
        os.environ['GROQ_API_KEY'] = st.session_state.groq_api_key
        os.environ['GOOGLE_API_KEY'] = st.session_state.google_api_key
        os.environ['LLAMA_CLOUD_API_KEY'] = st.session_state.llama_cloud_api_key

        # Model selection
        model_options = [
            'llama-3.1-70b-versatile',
            'llama-3.1-8b-instant',
            'llama3-8b-8192',
            'llama3-70b-8192',
            'mixtral-8x7b-32768',
            'gemma2-9b-it'
        ]
        st.session_state.selected_model = st.selectbox(
            'Select any Groq Model', 
            model_options
        )

        # Document upload
        st.session_state.uploaded_files = st.file_uploader(
            'Choose files', 
            accept_multiple_files=True, 
            type=SUPPORTED_EXTENSIONS, 
            key='file_uploader'
        )
        
        # Checkbox for LlamaParse usage
        st.session_state.use_llama_parse = st.checkbox(
            'Use LlamaParse for complex documents (graphs, tables, etc.)'
        )

        with st.expander('Advanced Options'): 
            # Parsing instruction input
            st.session_state.parsing_instruction = st.text_area(
                'Custom Parsing Instruction',
                value=st.session_state.get('parsing_instruction', 'Extract all information'),
                help='Enter custom instructions for document parsing'
            )

            # Custom prompt template input
            st.session_state.custom_prompt_template = st.text_area(
                'Custom Prompt Template', 
                placeholder='Enter your custom prompt here...(Optional)', 
                value=st.session_state.get('custom_prompt_template', '')
            )

# Step 3: Load and parse documents
def parse_and_index_documents(uploaded_files, use_llama_parse, parsing_instruction):
    all_documents = []

    if st.session_state.use_llama_parse and os.environ.get('LLAMA_CLOUD_API_KEY'):
        with st.spinner('Using LlamaParse for document parsing'):
            parser = LlamaParse(result_type='markdown', parsing_instruction=parsing_instruction)
            for uploaded_file in st.session_state.uploaded_files:
                file_info_placeholder = st.empty()
                file_info_placeholder.info(f'Processing file: {uploaded_file.name}')
                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[-1]) as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name

                try:
                    parsed_documents = parser.load_data(tmp_file_path)
                    all_documents.extend(parsed_documents)
                except Exception as e:
                    st.error(f'Error parsing {uploaded_file.name}: {str(e)}')
                finally:
                    os.remove(tmp_file_path)
                    time.sleep(4)
                    file_info_placeholder.empty()
    else:
        with st.spinner('Using SimpleDirectoryReader for document parsing'):
            for uploaded_file in st.session_state.uploaded_files:
                file_info_placeholder = st.empty()
                file_info_placeholder.info(f'Processing file: {uploaded_file.name}')
                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[-1]) as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name

                try:
                    reader = SimpleDirectoryReader(input_files=[tmp_file_path])
                    docs = reader.load_data()
                    all_documents.extend(docs)
                except Exception as e:
                    st.error(f'Error loading {uploaded_file.name}: {str(e)}')
                finally:
                    os.remove(tmp_file_path)
                    time.sleep(4)
                    file_info_placeholder.empty()

    if not all_documents:
        st.error('No valid documents found.')
        return None

    with st.spinner('Creating Vector Store Index...'):
        try:
            groq_llm = Groq(model=st.session_state.selected_model)
            gemini_embed_model = GeminiEmbedding(model_name='models/embedding-001')

            Settings.llm = groq_llm
            Settings.embed_model = gemini_embed_model
            Settings.chunk_size = 2048

            index = VectorStoreIndex.from_documents(all_documents, embed_model=gemini_embed_model)
            
            # Create a retriever from the index
            retriever = VectorIndexRetriever(index=index, similarity_top_k=2)
            
            # Create a postprocessor
            postprocessor = SimilarityPostprocessor(similarity_cutoff=0.50)
            
            # Create the query engine
            query_engine = RetrieverQueryEngine(
                retriever=retriever,
                node_postprocessors=[postprocessor]
            )
            
            # Create a chat engine with memory, using the custom query engine
            chat_engine = index.as_chat_engine(
                chat_mode='condense_question',
                memory=st.session_state.memory,
                verbose=False
            )
            
            # Set the query engine for the chat engine
            chat_engine.query_engine = query_engine
            return chat_engine
        
        except Exception as e:
            st.error(f'Error building index: {str(e)}')
            return None
        
        
    st.success('Data Processed. Ready to answer your question!')


# Step 5: Start document indexing
if st.sidebar.button('Start Document Indexing'):
    if st.session_state.uploaded_files:
        try:
            chat_engine = parse_and_index_documents(st.session_state.uploaded_files, st.session_state.use_llama_parse, st.session_state.parsing_instruction)
            if chat_engine:
                    st.session_state.chat_engine = chat_engine
                    st.success('Data Processed.Ready to answer your question!!')
            else:
                    st.error('Failed to create data index store.')
        except Exception as e:
            st.error(f'An error occurred during indexing: {str(e)}')
    else:
        st.warning('Please upload at least one file.')

# Step 6: Querying logic
def get_response(query, chat_engine, custom_prompt):
    try:
        # Prepare the query
        if custom_prompt:
            query = f'{custom_prompt}\n\nQuestion: {query}'

        # Use the chat engine to get a response
        response = chat_engine.chat(query)

        # If response is empty or not valid
        if not response or not response.response:
            return 'I couldn't find a relevant answer. Could you rephrase?'

        return response.response
    except Exception as e:
        st.error(f'Error processing query: {str(e)}')
        return 'An error occurred.'


st.markdown('---')
user_query = st.chat_input('Enter Your Question')

if user_query and 'chat_engine' in st.session_state:
    # Add user's message to chat history
    st.session_state.chat_history.append({'role': 'user', 'content': user_query})

    # Get response from the chat engine
    response = get_response(user_query, st.session_state.chat_engine, st.session_state.custom_prompt_template)

    if response:
        # Add AI's response to chat history
        st.session_state.chat_history.append({'role': 'assistant', 'content': str(response)})

        # Display chat history
        for message in st.session_state.chat_history:
            if message['role'] == 'user':
                st.chat_message('user').write(message['content'])
            elif message['role'] == 'assistant':
                st.chat_message('assistant').write(message['content'])
    else:
        st.warning('Unable to process the query.')

================================================
File: requirements.txt
================================================
llama-index
openai
pypdf
python-dotenv
llama-index-llms-groq
llama-index-llms-gemini
llama-index-embeddings-gemini
langchain_core


================================================
File: .streamlit/config.toml
================================================
[theme]
primaryColor='#c77f7f'
backgroundColor='#af99f1'
secondaryBackgroundColor='#eedacf'
textColor='#0e1212'
font='monospace'














        """,
        "repo_created_at": "2025-02-12T13:08:59Z",
        "last_updated": "2025-02-13T00:51:38Z",
        "stars": 0
    },
    "Excel-Sales-Analytics": {
        "github_url": "https://github.com/Parthiban-3997/Excel-Sales-Analytics",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Excel-Sales-Analytics",
        "description": """
        ================================================
File: README.md
================================================
## Sales Report :


- **Project objective:** 

    **1.** Create a _[customer performance report](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/Customer%20Performance%20Report.pdf)_ 

    **2.** Conduct a comprehensive comparison between _[market performance and sales targets](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/Market%20Performance%20vs%20Target%20Report.pdf)_

- **Purpose of sales analytics:** Empower businesses to monitor and evaluate their sales activities and performance.

- **Importance of analyzing sales data:** Identify sales patterns and track key performance indicators (KPIs).

- **Role of reports:** Determine effective customer discounts, facilitate negotiations with consumers, and identify potential business expansion opportunities in promising countries.


## Finance Report :

- **Project objective:** 

    **1.** Create Profit and Loss (P&L) reports by _[Fiscal Year](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/P%26L%20Statement%20by%20Fiscal%20Year.pdf)_ & _[Months](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/P%26L%20Statement%20by%20Months.pdf)_ 

   **2.** Create Profit and Loss (P&L) reports by _[Markets](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/P%26L%20Statement%20by%20Markets.pdf)_

- **Purpose of sales analytics:** Evaluation of financial performance, support decision-making, and facilitate communication with stakeholders.

- **Importance of analyzing Finance data:** Aid in benchmarking against industry peers and previous periods Foundation for budgeting and forecasting.

- **Role of reports:** Align financial planning with strategic goals Instill confidence in the organization's financial outlook.


## Technical & Soft Skills:
- [x]	Proficiency in ETL methodology (Extract, Transform, Load).
- [x]	Skills to generate a date table using Power Query.
- [x]	Ability to derive fiscal months and quarters.
- [x]	Establishing data model relationships with Power Pivot.
- [x]	Proficiency in incorporating supplementary data into an existing data model.
- [x]	Utilizing DAX to create calculated columns.

## Soft Skills:
- [x]	Refined understanding of Sales & Finance Reports
- [x]	Designing user-centric reports with empathy in mind.
- [x]	Optimization of report generation through meticulous fine-tuning.
- [x]	Developing a systematic approach to devising a report building plan.



        """,
        "repo_created_at": "2025-02-12T13:09:03Z",
        "last_updated": "2025-02-12T13:09:11Z",
        "stars": 0
    },
    
    "IPL-Data-Analysis-2008-2022": {
        "github_url": "https://github.com/Parthiban-3997/IPL-Data-Analysis-2008-2022",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/IPL-Data-Analysis-2008-2022",
        "description": """
        ================================================
File: README.md
================================================
## IPL-Data-Analysis-2008-2022


This project analyzes IPL match data from 2008 to 2022 to gain insights into various aspects of the tournament.It demonstrates a strong understanding of data analysis techniques and the ability to extract valuable insights from real-world data. It showcases the potential of data analysis to enhance our understanding of sports and provide valuable information for decision-making.


# Data Sources
- **Match Data**: A CSV file containing information about each match, including date, teams, venue, toss, result, and margin of victory.
- **Ball-by-Ball Data**: A CSV file containing detailed information about each ball bowled in every match, including runs scored, wickets taken, and extra runs.


# Data Cleaning and Preparation
- **Handling Missing Data**: Identified and addressed missing values in both datasets.
- **Data Type Conversion**: Converted relevant columns to appropriate data types (e.g., dates, integers).
- **Feature Engineering**: Created new features like 'No. of Hundereds' and 'No. of Fifties' to analyze batsman performance.


# Features Analyzed
- **Team Performance**: Analyzed team win percentage, winning mode (wickets or runs), and toss preference.
- **Player Performance**: Identified top run-scorers, wicket-takers, and players with the most centuries and half-centuries.
- **Venue Impact**: Investigated the influence of different venues on match outcomes.
- **Toss Decision**: Examined the impact of winning the toss on match results.
- **Season Trends**: Identified season-wise trends in toss decisions, winning teams, and match outcomes.


## Impactful Findings
1.	Home Advantage: Teams playing at their home ground tend to have a higher chance of winning.
2.	Second Batting Advantage: Teams winning the toss generally choose to bowl first, indicating a preference for chasing targets.
3.	Mumbai Indians Dominance: Mumbai Indians have consistently been a dominant team, winning the most matches and having the highest win percentage.
4.	Toss Decision Trends: While teams generally prefer bowling first, there were specific seasons (2009, 2010, 2013) where teams opted to bat first more often.
5.	Venue Impact: Certain venues have been more conducive to high-scoring matches, while others have favored bowlers.
6.	Top Performers: The analysis identified top run-scorers and wicket-takers, highlighting the most impactful players in the tournament.


## Project Steps
1.	Data Acquisition: Obtained the IPL match data and ball-by-ball data from reliable sources.
2.	Data Cleaning: Cleaned and preprocessed the data to handle missing values and inconsistencies.
3.	Exploratory Data Analysis: Performed exploratory analysis to understand the data distribution and identify key trends.
4.	Feature Engineering: Created new features to enhance the analysis and gain deeper insights.
5.	Statistical Analysis: Performed statistical analysis to test hypotheses and draw meaningful conclusions.
6.	Data Visualization: Created insightful visualizations to present the findings in a clear and engaging manner.
7.	Conclusion and Recommendations: Summarized the key findings and provided recommendations based on the analysis.


## Conclusion
This project provides a comprehensive analysis of IPL match data, revealing valuable insights into team performance, player statistics, venue impact, and toss decisions. The findings can be used by cricket enthusiasts, analysts, and teams to understand the dynamics of the tournament and make informed decisions.


## Future Work
1. Prediction Models: Develop predictive models to forecast match outcomes based on various factors.
2. Advanced Feature Engineering: Explore more complex features to improve the accuracy of predictions.
3. Sentiment Analysis: Analyze social media data to understand fan sentiment and its impact on match outcomes.
4. Player Performance Prediction: Develop models to predict individual player performance in future matches.







        """,
        "repo_created_at": "2025-02-12T13:09:11Z",
        "last_updated": "2025-02-12T13:09:21Z",
        "stars": 0
    },
    "Langchain_Series_GenAI": {
        "github_url": "https://github.com/Parthiban-3997/Langchain_Series_GenAI",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Langchain_Series_GenAI",
        "description": """
        ================================================
File: README.md
================================================
# GenAI Chatbot Project

## Overview
This project demonstrates the integration of Langchain and OpenAI to create a conversational AI chatbot. The chatbot utilizes the capabilities of Langchain for language processing and OpenAI's GPT-3.5 model for generating responses.

## Requirements
- Python 3.10
- langchain_openai
- langchain_core
- streamlit
- python-dotenv

## Installation
1. Clone the repository:
git clone https://github.com/Parthiban-3997/Langchain_Series_GenAI.git
cd Langchain_Series_GenAI


2. Install dependencies:
pip install -r requirements.txt



## Usage
1. Set up environment variables:
- Create a `.env` file in the root directory of your project.
- Add the following variables to your `.env` file:
  ```
  LANGCHAIN_API_KEY='your_langchain_api_key_here'
  OPENAI_API_KEY='your_openai_api_key_here'
  ```
2. Run the Streamlit app:
streamlit run app.py


3. Enter the topic you want to search for in the text input field and the chatbot will provide responses based on the input.

## Example
![screenshot](https://github.com/Parthiban-3997/Langchain_Series_GenAI/assets/26496805/ef76bcf3-30cd-4745-b338-842966d75fa2)

## License
This project is licensed under the [MIT License](LICENSE).





================================================
File: LICENSE
================================================
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  'This License' refers to version 3 of the GNU General Public License.

  'Copyright' also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  'The Program' refers to any copyrightable work licensed under this
License.  Each licensee is addressed as 'you'.  'Licensees' and
'recipients' may be individuals or organizations.

  To 'modify' a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a 'modified version' of the
earlier work or a work 'based on' the earlier work.

  A 'covered work' means either the unmodified Program or a work based
on the Program.

  To 'propagate' a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To 'convey' a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays 'Appropriate Legal Notices'
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The 'source code' for a work means the preferred form of the work
for making modifications to it.  'Object code' means any non-source
form of a work.

  A 'Standard Interface' means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The 'System Libraries' of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
'Major Component', in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The 'Corresponding Source' for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    'keep intact all notices'.

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
'aggregate' if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A 'User Product' is either (1) a 'consumer product', which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, 'normally used' refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  'Installation Information' for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  'Additional permissions' are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered 'further
restrictions' within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An 'entity transaction' is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A 'contributor' is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's 'contributor version'.

  A contributor's 'essential patent claims' are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, 'control' includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a 'patent license' is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To 'grant' such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  'Knowingly relying' means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is 'discriminatory' if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License 'or any later version' applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM 'AS IS' WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the 'copyright' line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an 'about box'.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a 'copyright disclaimer' for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.


================================================
File: app.py
================================================
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser ##Default Ouput Parser for LLM

import streamlit as st
import os
from dotenv import load_dotenv

load_dotenv()  ##Loads the .env files


## Langmith tracking
os.environ['LANGCHAIN_TRACING_V2']='true'
os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')

## Prompt Template

prompt=ChatPromptTemplate.from_messages(
    [
        ('system','You are a helpful assistant. Please response to the user queries'),
        ('user','Question:{question}')
    ]
)

## streamlit framework

st.title('Langchain Demo With OPENAI API')
input_text=st.text_input('Search the topic u want')

# openAI LLM 
llm=ChatOpenAI(model='gpt-3.5-turbo')
output_parser=StrOutputParser()
chain=prompt|llm|output_parser

if input_text:
    st.write(chain.invoke({'question':input_text}))

================================================
File: localama.py
================================================
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
import streamlit as st
import os
from dotenv import load_dotenv

load_dotenv()

os.environ['LANGCHAIN_TRACING_V2']='true'
os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')

## Prompt Template

prompt=ChatPromptTemplate.from_messages(
    [
        ('system','You are a helpful assistant. Please response to the user queries'),
        ('user','Question:{question}')
    ]
)
## streamlit framework

st.title('Langchain Demo With LLAMA2 API')
input_text=st.text_input('Search the topic u want')

# ollama LLAma2 LLm 
llm=Ollama(model='llama2')
output_parser=StrOutputParser()
chain=prompt|llm|output_parser

if input_text:
    st.write(chain.invoke({'question':input_text}))

================================================
File: requirements.txt
================================================
langchain_openai 
langchain_core
python-dotenv
streamlit
langchain_community



        """,
        "repo_created_at": "2025-02-12T13:09:17Z",
        "last_updated": "2025-02-13T00:49:47Z",
        "stars": 0
    },
    "MLProject": {
        "github_url": "https://github.com/Parthiban-3997/MLProject",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/MLProject",
        "description": """
        ================================================
File: README.md
================================================
## End to End ML Projects

================================================
File: application.py
================================================
from flask import Flask,request,render_template
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from src.pipeline.predict_pipeline import CustomData,PredictPipeline

application=Flask(__name__) ## Creates an entry point

app=application

## Route for a home page

@app.route('/')
def index():
    return render_template('index.html') 

@app.route('/predictdata',methods=['GET','POST'])
def predict_datapoint():
    if request.method=='GET': ##Here we are getting the input values 
        return render_template('home.html')
    else:                     ## Here in the else block we are doing POST action after which we are mapping to data and calculating standard scaling and pediction
        data=CustomData(
            gender=request.form.get('gender'),
            race_ethnicity=request.form.get('ethnicity'),
            parental_level_of_education=request.form.get('parental_level_of_education'),
            lunch=request.form.get('lunch'),
            test_preparation_course=request.form.get('test_preparation_course'),
            reading_score=float(request.form.get('writing_score')),
            writing_score=float(request.form.get('reading_score'))

        )
        pred_df=data.get_data_as_data_frame()
        print(pred_df)

        predict_pipeline=PredictPipeline()
        results=predict_pipeline.predict(pred_df)
        return render_template('home.html',results=results[0])
    

if __name__=='__main__':
    app.run(host='0.0.0.0')

================================================
File: requirements.txt
================================================
pandas
numpy
seaborn
matplotlib
scikit-learn
catboost
xgboost
dill
Flask
#-e .

================================================
File: setup.py
================================================
from setuptools import find_packages,setup
from typing import List

HYPEN_E_DOT='-e .'
def get_requirements(file_path:str)->List[str]:
    '''
    this function will return the list of requirements
    '''
    requirements=[]
    with open(file_path) as file_obj:
        requirements=file_obj.readlines()
        requirements=[req.replace('\n','') for req in requirements]

        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
    
    return requirements

setup(
name='MLProject',
version='0.0.1',
author='Parthi',
author_email='rparthiban729@gmail.com',
packages=find_packages(),
install_requires=get_requirements('requirements.txt')

)

================================================
File: artifacts/test.csv
================================================
gender,race_ethnicity,parental_level_of_education,lunch,test_preparation_course,math_score,reading_score,writing_score
female,group C,associate's degree,standard,none,91,86,84
female,group B,some college,free/reduced,completed,53,66,73
male,group D,bachelor's degree,standard,none,80,73,72
male,group C,some college,free/reduced,none,74,77,73
male,group E,some college,standard,completed,84,83,78
male,group D,associate's degree,free/reduced,none,81,75,78
male,group B,associate's degree,free/reduced,completed,69,70,63
female,group B,some high school,standard,completed,54,61,62
male,group C,associate's degree,free/reduced,none,87,73,72
male,group B,some high school,standard,completed,51,54,41
male,group A,high school,free/reduced,none,45,47,49
male,group E,some high school,standard,none,30,26,22
female,group B,high school,free/reduced,completed,67,80,81
female,group D,some college,free/reduced,none,49,65,61
male,group D,some college,standard,completed,85,81,85
female,group D,some high school,standard,completed,65,78,82
male,group D,high school,standard,none,53,52,42
male,group D,bachelor's degree,free/reduced,none,55,46,44
female,group D,some high school,standard,none,48,58,54
female,group D,associate's degree,free/reduced,none,56,65,63
male,group C,master's degree,standard,none,79,72,69
female,group C,bachelor's degree,free/reduced,completed,43,51,54
female,group C,some college,free/reduced,completed,45,73,70
female,group C,high school,free/reduced,none,36,53,43
male,group D,some high school,free/reduced,completed,80,79,79
male,group D,associate's degree,standard,none,80,75,77
male,group D,bachelor's degree,standard,completed,68,74,74
female,group C,associate's degree,standard,none,40,59,51
female,group A,high school,free/reduced,completed,34,48,41
female,group D,some college,free/reduced,none,49,58,60
male,group B,some college,standard,none,62,61,57
male,group D,some college,standard,completed,71,61,69
male,group B,bachelor's degree,free/reduced,none,62,63,56
male,group E,some college,standard,none,76,71,72
male,group E,some college,standard,none,84,77,71
female,group B,some college,free/reduced,none,45,53,55
male,group D,associate's degree,free/reduced,none,77,78,73
female,group D,some college,standard,none,69,77,77
female,group C,master's degree,standard,none,73,78,74
female,group C,some high school,free/reduced,none,0,17,10
male,group C,associate's degree,standard,completed,82,75,77
male,group B,some high school,standard,completed,65,66,62
male,group D,bachelor's degree,standard,completed,67,61,68
female,group A,some college,standard,none,54,63,67
male,group D,associate's degree,free/reduced,none,90,87,75
female,group E,high school,standard,completed,59,63,75
male,group D,high school,free/reduced,none,74,70,69
female,group C,high school,standard,none,29,29,30
male,group D,some high school,standard,completed,89,88,82
female,group A,high school,standard,completed,75,82,79
male,group B,some college,standard,completed,71,75,70
female,group D,associate's degree,standard,none,64,76,74
male,group C,some college,standard,completed,79,79,78
female,group B,some college,free/reduced,completed,48,56,58
female,group C,some high school,standard,none,69,73,73
female,group D,some college,standard,none,69,74,74
male,group D,bachelor's degree,standard,none,88,78,83
male,group C,associate's degree,standard,none,58,54,52
male,group B,associate's degree,standard,none,87,85,73
female,group A,some high school,standard,completed,85,90,92
male,group A,some high school,standard,completed,46,41,43
female,group C,some high school,free/reduced,completed,71,84,87
female,group C,associate's degree,standard,none,81,77,79
female,group B,some college,free/reduced,none,58,61,66
male,group D,master's degree,free/reduced,completed,84,89,90
female,group C,bachelor's degree,free/reduced,completed,66,74,81
female,group D,some college,free/reduced,none,55,71,69
male,group C,high school,free/reduced,none,59,53,52
female,group D,some college,free/reduced,completed,58,63,73
female,group D,associate's degree,standard,none,82,95,89
male,group E,associate's degree,standard,completed,66,63,64
female,group C,bachelor's degree,standard,none,81,88,90
male,group C,some college,free/reduced,none,58,57,54
female,group A,associate's degree,free/reduced,none,37,57,56
male,group C,some high school,standard,completed,63,60,57
male,group E,some high school,standard,completed,77,76,77
female,group D,some college,standard,completed,85,86,98
male,group B,associate's degree,free/reduced,none,57,56,57
female,group A,some high school,standard,none,48,66,65
male,group C,some high school,standard,none,51,52,44
male,group D,some college,free/reduced,none,63,61,60
male,group D,some high school,free/reduced,none,45,37,37
male,group C,bachelor's degree,standard,none,83,78,73
female,group C,some college,standard,none,60,72,74
male,group B,bachelor's degree,standard,none,63,71,69
female,group C,high school,free/reduced,none,62,67,64
female,group D,some college,standard,completed,68,78,77
female,group B,some high school,standard,completed,60,70,74
female,group C,some high school,standard,completed,77,90,85
male,group A,some college,free/reduced,none,28,23,19
male,group C,master's degree,free/reduced,none,79,81,71
female,group E,some college,standard,none,100,92,97
male,group D,bachelor's degree,standard,none,69,58,57
male,group B,high school,free/reduced,none,66,77,70
female,group B,some college,standard,none,19,38,32
male,group D,associate's degree,standard,none,75,68,64
male,group D,some college,standard,none,60,63,59
female,group A,some college,standard,none,58,70,67
female,group C,associate's degree,standard,none,69,80,71
female,group C,associate's degree,free/reduced,completed,56,68,70
male,group C,associate's degree,standard,completed,73,78,72
male,group E,some college,standard,none,66,57,52
male,group A,associate's degree,standard,none,67,57,53
female,group C,associate's degree,free/reduced,none,64,73,68
male,group A,high school,standard,none,71,74,64
male,group B,high school,standard,none,70,65,60
male,group E,associate's degree,standard,none,53,45,40
male,group C,high school,standard,none,75,81,71
female,group B,high school,standard,completed,68,83,78
female,group C,high school,standard,none,44,61,52
female,group D,bachelor's degree,free/reduced,none,29,41,47
female,group B,high school,free/reduced,none,71,87,82
male,group A,high school,standard,none,57,51,54
female,group A,bachelor's degree,standard,none,45,59,64
female,group C,some college,free/reduced,none,76,83,88
male,group C,high school,standard,none,61,56,55
male,group C,some high school,free/reduced,completed,45,52,49
male,group D,high school,standard,completed,55,41,48
male,group B,high school,standard,completed,73,69,68
male,group D,high school,free/reduced,completed,78,77,80
female,group A,master's degree,free/reduced,none,50,67,73
female,group C,some college,free/reduced,none,62,67,62
male,group D,master's degree,standard,none,81,81,84
female,group C,some high school,free/reduced,completed,64,79,77
female,group D,some high school,standard,completed,64,60,74
male,group D,some high school,standard,none,73,66,62
female,group D,associate's degree,standard,completed,73,75,80
female,group C,some high school,standard,completed,67,74,77
male,group B,associate's degree,standard,none,61,42,41
male,group C,some high school,standard,completed,67,73,68
female,group D,some high school,standard,none,65,82,81
male,group D,associate's degree,standard,none,80,75,69
male,group D,some high school,free/reduced,none,59,42,41
female,group E,master's degree,standard,completed,88,99,95
female,group C,associate's degree,standard,none,62,74,70
female,group C,high school,free/reduced,none,33,41,43
female,group C,bachelor's degree,standard,completed,79,92,89
male,group B,some high school,standard,completed,84,83,75
male,group A,master's degree,free/reduced,none,73,74,72
female,group A,associate's degree,free/reduced,none,41,51,48
female,group E,associate's degree,free/reduced,none,50,56,54
female,group B,high school,standard,completed,58,70,68
male,group D,some high school,free/reduced,completed,55,59,59
male,group D,high school,standard,none,45,48,46
male,group D,some high school,standard,completed,88,74,75
female,group B,associate's degree,free/reduced,none,46,61,55
male,group A,some high school,standard,none,51,31,36
male,group D,some high school,standard,none,75,74,69
male,group E,some college,free/reduced,completed,49,52,51
female,group E,high school,standard,none,75,86,79
female,group E,high school,standard,completed,74,79,80
female,group B,associate's degree,standard,completed,61,86,87
male,group C,associate's degree,standard,none,62,65,58
male,group C,some high school,free/reduced,none,68,63,54
female,group D,master's degree,standard,none,78,91,96
female,group E,some college,standard,none,71,70,76
female,group D,high school,free/reduced,none,49,57,52
female,group A,bachelor's degree,standard,none,59,72,70
male,group E,bachelor's degree,free/reduced,completed,79,74,72
female,group E,associate's degree,standard,none,51,51,54
female,group C,bachelor's degree,standard,completed,56,79,72
male,group B,high school,standard,completed,76,62,60
female,group D,some college,standard,completed,69,79,81
male,group C,some high school,free/reduced,completed,51,56,53
male,group D,some college,standard,completed,82,82,88
male,group C,some college,standard,none,73,74,61
male,group C,high school,free/reduced,completed,40,46,50
male,group E,some college,free/reduced,none,93,90,83
female,group C,bachelor's degree,standard,completed,59,64,75
female,group B,associate's degree,standard,none,73,76,80
male,group B,some high school,standard,completed,85,84,78
male,group E,associate's degree,standard,none,76,71,67
female,group D,associate's degree,free/reduced,completed,77,89,98
female,group D,some college,free/reduced,completed,67,86,83
male,group D,some college,free/reduced,none,61,47,56
female,group D,some high school,free/reduced,none,27,34,32
male,group D,high school,standard,none,54,52,52
female,group C,master's degree,free/reduced,completed,65,81,81
female,group E,associate's degree,standard,none,87,94,95
female,group C,some high school,standard,completed,70,82,76
female,group B,high school,standard,none,54,64,68
female,group C,high school,free/reduced,none,66,76,68
female,group D,master's degree,free/reduced,completed,85,95,100
male,group C,some high school,free/reduced,completed,56,61,60
male,group E,master's degree,standard,none,90,85,84
male,group E,high school,standard,none,70,55,56
female,group B,bachelor's degree,standard,none,61,72,70
male,group A,bachelor's degree,free/reduced,completed,49,58,60
male,group C,high school,standard,none,81,66,64
male,group B,some college,standard,completed,87,84,86
male,group B,some high school,free/reduced,completed,49,50,52
male,group B,some high school,standard,none,68,54,53
male,group C,associate's degree,free/reduced,none,77,67,64
female,group B,bachelor's degree,free/reduced,none,78,79,76
male,group C,associate's degree,free/reduced,completed,60,51,56
female,group D,high school,free/reduced,completed,52,57,56
male,group E,associate's degree,standard,completed,62,56,53
female,group B,some college,free/reduced,none,74,81,76
female,group C,associate's degree,standard,none,65,77,74
female,group D,some high school,standard,completed,61,74,72


================================================
File: artifacts/train.csv
================================================
gender,race_ethnicity,parental_level_of_education,lunch,test_preparation_course,math_score,reading_score,writing_score
female,group D,master's degree,standard,none,62,70,75
female,group C,bachelor's degree,free/reduced,completed,66,83,83
female,group D,some college,free/reduced,none,79,89,86
male,group C,master's degree,free/reduced,none,61,67,66
male,group E,high school,standard,none,73,64,57
male,group B,high school,free/reduced,none,30,24,15
female,group C,bachelor's degree,standard,completed,96,100,100
female,group C,associate's degree,standard,completed,57,77,80
male,group D,high school,standard,completed,68,64,66
female,group C,some high school,free/reduced,none,48,58,52
male,group B,some high school,standard,none,67,64,61
female,group C,some college,free/reduced,none,59,62,64
male,group E,some high school,standard,completed,74,64,60
female,group C,some high school,standard,none,65,69,76
female,group B,some college,standard,none,62,67,67
male,group C,associate's degree,standard,none,47,37,35
male,group D,associate's degree,standard,none,80,63,63
male,group C,high school,standard,none,68,60,53
female,group D,some college,standard,completed,79,84,91
male,group D,high school,standard,none,89,87,79
male,group A,some college,standard,none,69,67,69
female,group E,some college,free/reduced,none,53,58,57
female,group C,some college,standard,completed,64,82,77
male,group C,high school,standard,completed,82,84,82
male,group B,high school,free/reduced,none,36,29,27
female,group B,master's degree,standard,none,90,95,93
female,group D,master's degree,standard,none,64,63,66
female,group B,bachelor's degree,standard,none,52,65,69
female,group D,some high school,free/reduced,none,67,84,84
male,group C,associate's degree,standard,completed,51,60,58
male,group D,some college,standard,none,79,73,67
male,group A,high school,standard,none,63,63,62
female,group D,associate's degree,free/reduced,none,52,59,56
male,group A,associate's degree,free/reduced,completed,40,55,53
male,group D,some college,standard,none,40,42,38
female,group B,some college,standard,none,63,65,61
male,group C,associate's degree,standard,none,46,43,42
female,group C,some high school,free/reduced,completed,65,76,75
female,group B,some high school,standard,none,62,64,66
male,group B,associate's degree,standard,none,90,78,81
male,group A,associate's degree,free/reduced,none,47,57,44
male,group C,some college,standard,none,59,41,42
female,group B,master's degree,free/reduced,completed,77,97,94
female,group C,associate's degree,standard,none,52,55,57
male,group E,some college,standard,completed,99,87,81
female,group B,some high school,standard,none,70,64,72
male,group C,associate's degree,free/reduced,none,64,66,59
male,group A,bachelor's degree,standard,completed,80,78,81
male,group D,high school,free/reduced,none,42,39,34
male,group E,associate's degree,standard,completed,97,82,88
male,group A,some college,free/reduced,completed,50,47,54
female,group B,some high school,standard,completed,65,82,78
female,group C,master's degree,free/reduced,none,52,65,61
female,group E,associate's degree,standard,none,59,62,69
male,group B,some high school,standard,none,74,63,57
female,group C,some high school,free/reduced,none,43,53,53
female,group B,high school,free/reduced,completed,67,78,79
male,group E,bachelor's degree,standard,completed,100,100,100
male,group D,high school,standard,none,72,66,66
female,group B,associate's degree,standard,none,71,83,78
male,group A,some high school,free/reduced,none,55,46,43
female,group C,some college,standard,none,84,87,91
female,group E,some college,standard,completed,63,72,70
female,group C,bachelor's degree,standard,none,63,75,81
female,group C,some college,free/reduced,completed,42,66,69
male,group E,associate's degree,free/reduced,completed,78,74,72
male,group E,some college,standard,none,69,60,54
female,group B,associate's degree,standard,none,80,86,83
male,group B,high school,standard,none,79,60,65
male,group C,associate's degree,standard,completed,87,100,95
female,group A,associate's degree,free/reduced,none,65,85,76
female,group D,some high school,standard,none,51,63,61
male,group D,master's degree,standard,none,85,84,89
male,group A,some high school,standard,completed,47,49,49
male,group C,master's degree,free/reduced,none,54,59,50
female,group B,high school,free/reduced,none,38,60,50
male,group C,some high school,free/reduced,completed,59,69,65
male,group D,high school,free/reduced,none,60,57,51
male,group B,high school,free/reduced,none,49,45,45
female,group C,associate's degree,standard,completed,52,59,62
female,group C,bachelor's degree,free/reduced,none,44,63,62
female,group E,associate's degree,free/reduced,none,70,84,81
male,group C,associate's degree,standard,none,84,80,80
male,group C,associate's degree,standard,none,76,70,68
male,group C,some college,free/reduced,none,35,28,27
female,group C,associate's degree,standard,completed,96,96,99
female,group D,some college,standard,none,80,90,89
male,group B,associate's degree,standard,none,81,73,72
male,group D,high school,standard,none,57,50,54
male,group E,high school,standard,none,94,73,71
male,group B,high school,standard,none,82,82,80
male,group D,high school,standard,none,70,70,70
female,group B,associate's degree,standard,completed,94,87,92
male,group A,bachelor's degree,standard,completed,75,58,62
female,group C,some college,standard,none,52,58,58
female,group A,high school,free/reduced,completed,77,88,85
male,group D,some college,free/reduced,none,70,63,58
male,group A,some high school,free/reduced,none,65,59,53
male,group B,some college,free/reduced,none,40,43,39
female,group C,some college,standard,completed,70,89,88
male,group D,associate's degree,free/reduced,completed,79,82,80
female,group C,some college,standard,completed,67,81,79
male,group C,some college,free/reduced,none,68,68,61
female,group D,master's degree,free/reduced,completed,47,58,67
female,group A,some college,standard,completed,72,79,82
female,group E,high school,free/reduced,completed,57,75,73
female,group C,bachelor's degree,standard,none,83,93,95
male,group A,some college,standard,completed,61,51,52
male,group C,associate's degree,standard,completed,98,87,90
female,group D,master's degree,free/reduced,completed,61,71,78
female,group C,bachelor's degree,standard,completed,92,100,99
female,group C,associate's degree,standard,completed,68,67,73
female,group E,some high school,standard,none,77,79,80
male,group C,some college,standard,none,66,59,52
male,group B,high school,standard,none,47,46,42
male,group C,some college,free/reduced,none,65,58,49
male,group A,some high school,free/reduced,none,68,72,64
female,group C,some college,standard,completed,63,78,80
female,group D,high school,free/reduced,none,73,92,84
female,group C,high school,free/reduced,none,42,62,60
female,group E,master's degree,standard,none,62,68,68
female,group D,bachelor's degree,standard,completed,68,75,81
female,group C,associate's degree,standard,completed,67,84,81
male,group D,some college,free/reduced,none,49,57,46
female,group C,some college,free/reduced,none,35,44,43
male,group A,high school,standard,none,68,70,66
female,group B,high school,standard,completed,69,76,74
male,group C,high school,standard,none,70,74,71
female,group E,some high school,standard,completed,80,85,85
female,group C,some college,standard,completed,75,81,84
female,group D,some college,standard,none,63,64,67
male,group B,bachelor's degree,standard,none,66,60,57
female,group B,some high school,free/reduced,completed,74,90,88
female,group C,some high school,standard,completed,44,51,55
female,group B,bachelor's degree,standard,none,72,72,74
female,group D,master's degree,standard,completed,77,82,91
male,group D,high school,standard,none,46,34,36
male,group C,high school,standard,completed,72,67,64
male,group B,associate's degree,standard,completed,82,84,78
male,group E,associate's degree,standard,completed,62,61,58
male,group D,associate's degree,standard,none,80,68,72
female,group C,high school,standard,none,54,59,62
female,group D,some college,standard,none,79,86,81
female,group B,high school,standard,none,87,95,86
female,group C,some high school,standard,completed,65,74,77
female,group C,associate's degree,free/reduced,completed,82,93,93
female,group B,associate's degree,standard,none,40,48,50
female,group D,bachelor's degree,free/reduced,completed,93,100,100
female,group C,bachelor's degree,standard,none,65,72,74
male,group D,some high school,standard,completed,77,68,69
female,group C,some college,free/reduced,none,46,64,66
male,group B,some college,standard,completed,88,85,76
female,group E,some high school,free/reduced,none,32,34,38
female,group C,associate's degree,standard,none,39,64,57
male,group D,some high school,standard,none,86,73,70
male,group C,some high school,free/reduced,completed,53,37,40
male,group A,some college,free/reduced,completed,81,78,81
male,group B,some college,free/reduced,none,41,39,34
male,group C,some college,standard,none,61,61,62
male,group D,some college,standard,none,88,73,78
female,group A,some high school,standard,completed,59,85,80
female,group D,some high school,standard,none,81,97,96
male,group C,bachelor's degree,standard,none,58,55,48
female,group C,some college,free/reduced,completed,64,85,85
female,group E,master's degree,standard,none,81,92,91
male,group C,high school,standard,none,70,70,65
female,group C,bachelor's degree,free/reduced,none,62,78,79
male,group D,some college,standard,completed,77,62,62
female,group D,high school,standard,none,62,64,64
female,group E,some college,standard,none,87,85,93
female,group C,some college,free/reduced,completed,67,75,70
male,group A,bachelor's degree,free/reduced,none,62,72,65
female,group D,some high school,standard,none,76,72,71
female,group C,associate's degree,standard,none,63,67,70
female,group B,some college,standard,completed,88,95,92
male,group D,associate's degree,standard,none,72,79,74
female,group D,master's degree,standard,none,55,64,70
male,group C,some high school,free/reduced,none,61,57,56
male,group D,bachelor's degree,free/reduced,none,50,42,48
male,group E,some high school,standard,completed,87,84,76
male,group B,some college,standard,none,54,52,51
female,group C,some college,free/reduced,none,22,39,33
male,group D,high school,free/reduced,none,66,74,69
male,group C,bachelor's degree,standard,completed,83,82,84
female,group D,high school,standard,completed,56,68,74
female,group B,associate's degree,free/reduced,none,54,65,65
female,group D,master's degree,standard,none,74,79,82
male,group E,some college,free/reduced,completed,87,74,70
male,group E,high school,free/reduced,completed,86,81,75
male,group B,some college,standard,none,66,65,60
male,group C,associate's degree,free/reduced,completed,65,67,65
female,group C,associate's degree,standard,none,58,73,68
female,group C,associate's degree,standard,completed,75,82,90
female,group B,associate's degree,free/reduced,none,52,76,70
female,group C,some college,standard,none,54,64,65
female,group E,associate's degree,standard,completed,82,85,86
female,group C,some high school,standard,none,63,73,68
female,group A,some high school,free/reduced,none,59,73,69
male,group E,bachelor's degree,free/reduced,completed,70,68,72
female,group C,high school,free/reduced,completed,59,71,65
male,group D,bachelor's degree,free/reduced,completed,74,71,80
male,group A,high school,free/reduced,completed,72,67,65
male,group A,associate's degree,standard,completed,97,92,86
female,group C,some high school,standard,none,47,54,53
male,group D,master's degree,standard,none,95,81,84
female,group C,some high school,standard,none,49,63,56
male,group E,associate's degree,free/reduced,none,64,56,52
female,group B,some high school,free/reduced,none,24,38,27
male,group E,associate's degree,free/reduced,completed,77,69,68
male,group B,bachelor's degree,free/reduced,none,55,59,54
female,group D,some college,standard,none,74,89,84
male,group D,high school,free/reduced,none,69,70,67
female,group B,master's degree,standard,none,77,90,84
male,group D,high school,standard,none,76,73,68
male,group D,bachelor's degree,free/reduced,completed,61,70,76
male,group C,some college,standard,completed,93,84,90
male,group B,high school,standard,none,62,55,54
male,group A,bachelor's degree,standard,none,64,60,58
female,group D,some high school,standard,completed,66,78,78
male,group C,high school,standard,completed,86,81,80
male,group C,master's degree,free/reduced,completed,46,42,46
female,group B,associate's degree,free/reduced,completed,76,94,87
male,group A,high school,standard,none,59,52,46
male,group B,high school,free/reduced,none,63,48,47
female,group A,some high school,free/reduced,none,47,59,50
male,group C,bachelor's degree,free/reduced,none,61,66,61
male,group E,associate's degree,standard,none,72,64,63
male,group A,some high school,free/reduced,none,39,39,34
male,group E,some college,standard,none,86,76,74
female,group D,associate's degree,free/reduced,none,47,53,58
male,group B,associate's degree,standard,completed,81,82,82
female,group B,high school,standard,none,58,62,59
female,group C,some college,standard,none,59,71,70
female,group D,bachelor's degree,standard,none,79,89,89
female,group C,some high school,free/reduced,none,65,86,80
female,group B,high school,standard,none,65,81,73
female,group E,high school,standard,none,50,50,47
female,group A,some high school,free/reduced,none,44,64,58
female,group E,bachelor's degree,standard,completed,71,70,70
female,group C,high school,standard,none,60,68,72
male,group D,some high school,standard,none,86,80,75
female,group C,some college,standard,none,53,62,56
female,group D,bachelor's degree,standard,none,89,100,100
female,group A,associate's degree,standard,completed,65,70,74
male,group E,some high school,free/reduced,completed,78,83,80
female,group D,bachelor's degree,free/reduced,none,63,73,78
female,group C,high school,standard,none,75,88,85
female,group B,high school,free/reduced,completed,46,54,58
female,group C,some high school,free/reduced,completed,50,60,60
female,group C,associate's degree,standard,completed,65,84,84
female,group C,associate's degree,standard,none,65,76,76
male,group E,associate's degree,free/reduced,none,90,90,82
male,group C,associate's degree,standard,completed,57,54,56
male,group C,high school,standard,none,52,53,49
female,group B,high school,standard,none,65,64,62
male,group D,some high school,standard,none,84,84,80
female,group C,associate's degree,standard,completed,62,76,80
male,group D,associate's degree,standard,completed,81,72,77
male,group E,associate's degree,free/reduced,none,46,43,41
male,group D,associate's degree,standard,none,71,66,60
male,group C,some high school,standard,none,49,49,41
female,group D,some college,standard,none,51,58,54
female,group D,high school,standard,none,69,77,73
female,group D,some high school,free/reduced,none,48,54,53
male,group E,some high school,free/reduced,completed,73,67,59
male,group C,some college,standard,completed,98,86,90
female,group E,bachelor's degree,standard,completed,99,100,100
male,group C,associate's degree,standard,none,74,73,67
male,group E,some college,standard,none,68,60,59
male,group D,associate's degree,free/reduced,none,53,54,48
male,group D,associate's degree,standard,completed,87,84,85
male,group C,high school,standard,none,71,79,71
male,group C,some college,free/reduced,completed,67,74,70
female,group D,some high school,standard,none,73,86,82
male,group D,some high school,standard,completed,76,70,69
female,group A,some high school,free/reduced,none,44,45,45
female,group C,high school,free/reduced,none,35,53,46
male,group C,bachelor's degree,free/reduced,completed,70,75,74
male,group C,some high school,standard,none,75,72,62
female,group E,high school,standard,none,74,76,73
female,group C,some college,standard,none,58,59,66
female,group B,some college,standard,none,79,86,92
male,group D,associate's degree,standard,none,40,52,43
female,group B,high school,free/reduced,none,50,67,63
female,group E,associate's degree,standard,completed,79,88,94
male,group B,some college,free/reduced,completed,59,65,66
female,group B,associate's degree,standard,none,53,58,65
female,group B,some high school,standard,none,41,55,51
female,group B,master's degree,free/reduced,completed,58,76,78
female,group D,some college,free/reduced,completed,59,78,76
male,group D,some college,standard,none,81,82,84
male,group A,some high school,standard,none,53,54,48
male,group D,some college,standard,none,55,58,52
male,group B,some college,standard,none,79,67,67
male,group C,bachelor's degree,standard,none,86,83,86
female,group B,master's degree,free/reduced,completed,52,70,62
male,group A,some high school,free/reduced,none,79,82,73
male,group C,bachelor's degree,standard,completed,71,74,68
male,group B,high school,standard,none,59,58,47
female,group B,high school,free/reduced,none,64,73,71
female,group D,high school,standard,none,67,72,74
female,group C,associate's degree,standard,completed,71,77,77
male,group A,high school,free/reduced,none,48,45,41
female,group A,some college,standard,none,69,84,82
male,group E,some high school,standard,completed,89,84,77
female,group A,some college,standard,none,56,58,64
male,group B,some college,standard,completed,62,66,68
female,group E,some high school,free/reduced,none,38,49,45
male,group C,some college,standard,none,84,87,81
male,group E,some college,standard,none,68,72,65
male,group C,associate's degree,standard,completed,78,77,77
male,group E,bachelor's degree,standard,completed,85,66,71
female,group B,some college,free/reduced,none,61,68,66
female,group C,some high school,standard,none,69,75,78
female,group C,high school,free/reduced,none,41,46,43
female,group D,some college,free/reduced,completed,52,59,65
female,group C,some high school,free/reduced,none,55,65,62
female,group D,some high school,standard,completed,97,100,100
female,group A,some college,standard,completed,78,87,91
male,group D,some college,standard,none,44,54,53
male,group A,associate's degree,standard,none,63,61,61
female,group C,some college,free/reduced,completed,63,73,71
female,group E,master's degree,free/reduced,none,81,86,87
male,group C,high school,free/reduced,none,58,61,52
female,group C,high school,standard,none,61,72,70
male,group B,bachelor's degree,free/reduced,completed,87,90,88
female,group B,high school,standard,completed,77,82,89
female,group B,associate's degree,standard,none,57,69,68
male,group D,some college,standard,none,67,64,70
male,group C,associate's degree,free/reduced,completed,43,45,50
female,group B,associate's degree,free/reduced,none,53,70,70
male,group B,associate's degree,free/reduced,none,61,58,56
male,group C,high school,free/reduced,completed,58,51,52
female,group B,some high school,standard,none,37,46,46
female,group D,some high school,free/reduced,completed,40,65,64
male,group C,some high school,standard,none,73,66,66
male,group D,bachelor's degree,standard,none,54,49,47
male,group B,associate's degree,free/reduced,none,44,41,38
female,group B,associate's degree,free/reduced,completed,68,77,80
male,group D,some college,free/reduced,none,69,66,60
male,group B,some high school,free/reduced,none,48,52,45
male,group C,some college,standard,none,58,49,42
male,group D,bachelor's degree,free/reduced,none,63,66,67
female,group C,associate's degree,free/reduced,none,60,75,74
female,group D,bachelor's degree,standard,none,78,82,79
male,group A,some high school,free/reduced,completed,61,62,61
male,group D,some high school,free/reduced,none,62,49,52
male,group C,high school,standard,none,62,67,58
male,group A,some high school,standard,none,71,62,50
male,group B,some high school,standard,none,72,68,67
female,group B,bachelor's degree,free/reduced,none,75,85,82
female,group D,some high school,standard,none,59,67,61
female,group D,associate's degree,standard,none,77,77,73
male,group D,associate's degree,standard,none,52,55,49
female,group C,some college,standard,completed,71,71,80
female,group C,bachelor's degree,standard,completed,52,61,66
female,group D,some high school,standard,none,73,84,85
female,group D,associate's degree,standard,completed,88,92,95
female,group A,associate's degree,standard,completed,55,65,62
male,group E,associate's degree,standard,none,87,74,76
male,group D,associate's degree,standard,none,61,55,52
female,group D,some college,free/reduced,none,77,86,86
male,group B,some college,standard,none,58,50,45
male,group C,associate's degree,standard,none,92,79,84
female,group E,high school,standard,none,99,93,90
female,group B,associate's degree,standard,none,73,83,76
female,group B,some college,standard,completed,50,64,66
female,group C,associate's degree,standard,completed,74,75,83
female,group C,high school,standard,none,61,73,63
male,group A,some high school,standard,completed,66,68,64
male,group E,associate's degree,free/reduced,completed,100,100,93
male,group E,some college,standard,none,83,80,73
female,group E,some high school,free/reduced,none,72,79,77
male,group E,some college,standard,none,53,55,48
female,group C,associate's degree,standard,none,46,58,57
female,group D,high school,free/reduced,completed,65,61,71
female,group E,some college,free/reduced,completed,42,55,54
female,group C,associate's degree,standard,completed,83,85,90
male,group D,some high school,standard,none,60,59,54
male,group D,some college,standard,completed,100,97,99
female,group C,high school,free/reduced,completed,67,79,84
female,group C,associate's degree,free/reduced,none,54,58,61
male,group B,bachelor's degree,standard,none,59,54,51
female,group B,high school,standard,none,48,62,60
male,group C,high school,standard,none,90,75,69
female,group B,associate's degree,standard,none,82,80,77
female,group D,high school,standard,none,51,66,62
female,group C,high school,free/reduced,none,35,61,54
female,group D,associate's degree,free/reduced,completed,75,90,88
female,group C,master's degree,standard,completed,81,91,87
male,group C,associate's degree,standard,none,85,76,71
female,group D,some high school,free/reduced,completed,69,86,81
female,group B,bachelor's degree,free/reduced,none,77,85,87
male,group C,associate's degree,free/reduced,none,58,55,53
male,group B,high school,standard,completed,52,49,46
male,group D,some high school,standard,none,62,67,61
female,group B,some high school,standard,none,67,89,82
female,group E,some college,standard,none,76,78,80
male,group D,bachelor's degree,free/reduced,none,68,68,67
female,group C,associate's degree,standard,completed,59,73,72
female,group B,some high school,free/reduced,none,18,32,28
male,group D,some college,standard,completed,65,77,74
female,group C,some college,standard,none,71,81,80
female,group E,some college,standard,none,62,73,70
male,group D,some high school,standard,none,69,66,61
male,group D,some college,standard,none,72,57,58
female,group E,associate's degree,standard,none,66,65,69
male,group C,high school,standard,none,84,77,74
female,group E,bachelor's degree,standard,none,37,45,38
female,group C,associate's degree,standard,none,85,84,82
male,group C,master's degree,free/reduced,completed,62,68,75
male,group D,some high school,free/reduced,none,56,54,52
male,group B,some college,standard,completed,69,77,77
female,group D,some college,standard,none,98,100,99
male,group C,high school,standard,none,50,48,42
female,group E,master's degree,standard,completed,94,99,100
female,group C,associate's degree,standard,none,91,95,94
female,group E,some college,standard,completed,66,74,73
female,group C,some high school,standard,none,74,75,82
male,group B,associate's degree,standard,none,65,54,57
male,group E,bachelor's degree,standard,completed,70,64,70
male,group B,some college,free/reduced,none,60,60,60
female,group A,high school,standard,none,61,68,63
male,group E,some high school,standard,none,94,88,78
male,group C,high school,standard,none,88,89,86
male,group A,some high school,standard,none,64,50,43
female,group D,associate's degree,free/reduced,completed,57,74,76
male,group C,some high school,standard,completed,78,72,69
male,group C,master's degree,free/reduced,completed,72,66,72
female,group C,some high school,standard,completed,76,87,85
female,group E,some high school,free/reduced,none,74,74,72
male,group B,high school,standard,completed,73,71,68
female,group D,some college,free/reduced,completed,70,78,78
female,group C,high school,standard,none,76,76,74
male,group C,some high school,standard,none,57,61,54
female,group E,master's degree,free/reduced,none,45,56,54
male,group B,some college,standard,none,69,54,55
female,group C,some college,standard,none,62,69,69
male,group D,associate's degree,free/reduced,none,75,66,73
female,group D,some college,standard,completed,75,77,83
male,group C,some college,standard,none,59,60,58
female,group C,some college,standard,completed,88,95,94
female,group D,some high school,free/reduced,none,50,64,59
female,group D,some college,standard,none,62,70,72
female,group D,bachelor's degree,standard,none,59,70,73
male,group C,some college,standard,none,91,74,76
male,group C,some college,standard,none,63,63,60
male,group C,master's degree,standard,none,71,67,67
female,group B,some high school,free/reduced,completed,59,63,64
male,group C,some high school,standard,none,64,58,51
female,group C,master's degree,standard,completed,69,84,85
male,group C,some high school,standard,none,62,64,55
male,group C,associate's degree,standard,none,97,93,91
female,group E,associate's degree,standard,completed,95,89,92
female,group B,bachelor's degree,standard,none,75,84,80
female,group A,some college,free/reduced,none,61,60,57
female,group D,master's degree,standard,none,53,61,68
female,group E,associate's degree,standard,none,68,76,67
male,group B,master's degree,free/reduced,none,49,53,52
female,group C,bachelor's degree,free/reduced,none,67,75,72
female,group B,associate's degree,standard,completed,59,70,66
male,group C,some high school,standard,completed,76,80,73
female,group D,bachelor's degree,free/reduced,none,62,72,74
female,group E,associate's degree,standard,none,84,95,92
male,group C,high school,standard,none,62,55,49
female,group C,some college,standard,none,72,72,71
male,group A,high school,free/reduced,none,53,58,44
male,group B,high school,standard,completed,60,44,47
female,group D,high school,standard,completed,57,58,64
male,group E,high school,free/reduced,completed,57,56,54
male,group D,some high school,standard,completed,71,69,68
female,group A,high school,standard,none,55,73,73
female,group D,associate's degree,free/reduced,none,46,56,57
female,group C,some college,standard,none,69,78,76
female,group C,some college,standard,none,55,69,65
male,group D,high school,standard,none,88,78,75
male,group A,bachelor's degree,standard,none,77,67,68
female,group D,high school,standard,completed,88,99,100
female,group C,associate's degree,standard,none,54,61,58
male,group E,high school,standard,completed,81,80,76
female,group D,associate's degree,free/reduced,none,43,60,58
male,group B,some college,free/reduced,completed,74,77,76
male,group D,some high school,standard,completed,78,81,86
male,group D,high school,free/reduced,completed,64,64,67
female,group E,high school,free/reduced,none,41,45,40
female,group D,associate's degree,standard,none,74,81,83
female,group C,associate's degree,free/reduced,none,65,77,74
female,group A,high school,standard,completed,68,80,76
male,group D,master's degree,standard,none,80,80,72
male,group B,associate's degree,standard,none,80,76,64
male,group D,high school,standard,none,69,75,71
male,group A,bachelor's degree,standard,none,91,96,92
male,group A,some college,standard,completed,100,96,86
female,group C,some college,standard,completed,87,89,94
female,group E,associate's degree,standard,none,85,92,85
female,group C,some high school,free/reduced,none,44,50,51
male,group D,some college,free/reduced,completed,69,60,63
male,group E,associate's degree,standard,completed,71,74,68
female,group C,bachelor's degree,free/reduced,completed,51,72,79
male,group A,some high school,standard,completed,62,67,69
male,group C,associate's degree,free/reduced,none,68,65,61
male,group D,high school,standard,none,41,52,51
male,group D,high school,free/reduced,none,44,51,48
male,group C,some high school,free/reduced,none,79,76,65
female,group E,associate's degree,standard,completed,93,100,95
male,group B,some high school,standard,completed,64,53,57
male,group C,associate's degree,free/reduced,none,73,68,66
male,group B,some high school,standard,none,88,84,75
female,group C,some college,free/reduced,none,62,72,70
male,group D,some college,free/reduced,none,62,57,62
male,group C,high school,free/reduced,none,61,60,55
male,group D,associate's degree,standard,none,90,87,85
male,group D,high school,free/reduced,none,75,74,66
female,group C,some college,free/reduced,none,77,90,91
female,group C,some college,standard,none,82,90,94
male,group E,high school,standard,none,80,76,65
male,group D,high school,free/reduced,none,63,57,56
male,group E,some high school,standard,none,82,67,61
female,group E,some college,standard,none,61,64,62
male,group A,high school,standard,none,57,43,47
female,group D,high school,standard,none,45,63,59
male,group E,high school,free/reduced,none,55,56,51
female,group B,associate's degree,standard,none,58,63,65
male,group B,bachelor's degree,free/reduced,none,73,56,57
female,group E,bachelor's degree,standard,none,65,73,75
female,group C,some high school,standard,completed,59,54,67
female,group C,some college,standard,completed,88,93,93
female,group D,associate's degree,standard,none,65,69,70
male,group C,associate's degree,standard,none,69,77,69
male,group C,high school,standard,none,70,56,51
male,group E,associate's degree,standard,none,89,76,74
male,group E,high school,standard,none,84,73,69
male,group D,associate's degree,free/reduced,completed,61,71,73
female,group B,high school,standard,none,74,72,72
male,group B,high school,standard,none,57,48,51
female,group C,high school,standard,completed,60,64,74
male,group C,high school,free/reduced,none,54,72,59
female,group A,bachelor's degree,standard,none,51,49,51
female,group D,some high school,standard,completed,80,92,88
female,group A,some college,free/reduced,none,49,65,55
male,group C,bachelor's degree,standard,completed,91,81,79
male,group B,high school,standard,none,52,48,49
male,group C,master's degree,standard,none,67,57,59
female,group C,bachelor's degree,free/reduced,completed,47,62,66
male,group B,some high school,standard,completed,61,56,56
female,group D,associate's degree,free/reduced,completed,74,88,90
female,group E,some college,standard,none,68,70,66
male,group C,master's degree,free/reduced,completed,79,77,75
female,group D,some college,free/reduced,none,64,74,75
male,group B,some college,standard,completed,91,96,91
female,group E,bachelor's degree,free/reduced,none,61,58,62
female,group C,bachelor's degree,free/reduced,none,43,62,61
female,group C,high school,free/reduced,none,53,72,64
female,group E,bachelor's degree,standard,none,64,73,70
female,group A,high school,free/reduced,completed,53,50,60
female,group C,bachelor's degree,standard,none,86,92,87
female,group D,high school,standard,none,69,72,77
female,group C,some high school,standard,none,77,91,88
female,group D,high school,standard,none,78,81,80
female,group C,some college,standard,none,54,48,52
male,group A,associate's degree,standard,none,54,53,47
female,group E,associate's degree,free/reduced,none,73,76,78
female,group B,bachelor's degree,standard,none,67,86,83
male,group C,associate's degree,free/reduced,none,49,51,51
female,group C,master's degree,free/reduced,none,40,58,54
male,group D,associate's degree,free/reduced,none,52,57,50
female,group D,some college,standard,completed,82,97,96
male,group D,some high school,standard,none,81,78,78
female,group B,high school,free/reduced,completed,23,44,36
male,group E,some high school,standard,none,92,87,78
female,group B,some high school,standard,completed,32,51,44
female,group E,some college,standard,completed,73,78,76
male,group C,associate's degree,standard,none,83,72,78
female,group B,high school,standard,none,50,53,55
female,group D,master's degree,standard,completed,70,71,74
male,group D,associate's degree,standard,completed,67,54,63
female,group D,associate's degree,free/reduced,completed,42,61,58
male,group D,some college,free/reduced,none,59,62,61
female,group B,some college,standard,none,70,75,78
male,group B,some college,free/reduced,none,55,55,47
male,group D,associate's degree,standard,none,61,48,46
female,group B,bachelor's degree,standard,completed,66,74,81
female,group D,bachelor's degree,free/reduced,none,73,79,84
female,group D,some high school,standard,none,80,90,82
female,group A,some high school,free/reduced,none,38,43,43
female,group B,associate's degree,standard,completed,52,66,73
male,group C,high school,standard,completed,58,52,54
female,group C,high school,standard,none,72,80,83
female,group C,associate's degree,standard,completed,68,86,84
female,group C,bachelor's degree,standard,completed,77,94,95
female,group B,some high school,standard,none,82,82,80
female,group D,associate's degree,standard,none,76,74,73
male,group E,some high school,standard,completed,81,75,76
male,group E,associate's degree,free/reduced,completed,46,43,44
male,group D,some college,standard,none,88,77,77
male,group C,associate's degree,free/reduced,completed,65,73,68
female,group B,bachelor's degree,standard,none,97,97,96
female,group B,some college,standard,none,82,85,87
female,group B,bachelor's degree,standard,completed,65,81,81
male,group C,master's degree,standard,completed,91,85,85
female,group C,associate's degree,standard,completed,55,72,79
female,group D,master's degree,standard,none,92,100,100
female,group B,high school,standard,none,81,91,89
female,group E,associate's degree,free/reduced,completed,57,68,73
female,group C,some college,standard,none,73,80,82
female,group D,high school,standard,none,56,52,55
female,group D,associate's degree,standard,completed,57,78,79
male,group D,associate's degree,free/reduced,none,66,62,64
male,group C,high school,standard,completed,53,52,49
male,group E,associate's degree,standard,completed,81,81,79
male,group C,high school,standard,completed,75,69,68
male,group A,high school,standard,completed,72,73,74
female,group B,some high school,standard,none,73,79,79
female,group E,some college,standard,completed,86,85,91
female,group C,bachelor's degree,standard,none,65,79,81
male,group D,high school,standard,none,64,54,50
female,group B,high school,standard,none,58,68,61
male,group D,some high school,standard,none,55,47,44
male,group C,associate's degree,free/reduced,completed,78,81,82
female,group D,some college,free/reduced,completed,63,80,80
male,group D,high school,free/reduced,completed,73,68,66
female,group C,high school,standard,none,81,84,82
female,group E,high school,standard,none,74,81,71
female,group C,associate's degree,standard,none,49,53,53
male,group C,bachelor's degree,free/reduced,none,53,58,55
male,group D,some college,free/reduced,none,77,62,64
female,group D,some college,free/reduced,none,69,65,74
male,group E,bachelor's degree,standard,none,82,62,62
male,group E,some college,standard,none,76,67,67
female,group B,high school,standard,none,42,52,51
female,group C,associate's degree,standard,none,85,89,95
female,group C,high school,standard,completed,58,75,77
female,group C,high school,standard,none,59,72,68
female,group C,high school,free/reduced,none,34,42,39
male,group C,some college,standard,none,76,78,75
female,group D,some high school,standard,none,68,71,75
female,group B,some high school,free/reduced,none,72,81,79
female,group C,some high school,free/reduced,none,48,56,51
male,group C,bachelor's degree,standard,completed,94,90,91
male,group D,associate's degree,standard,none,81,71,73
female,group A,some high school,standard,completed,92,100,97
male,group E,some college,standard,completed,81,74,71
female,group C,some high school,free/reduced,completed,29,40,44
female,group D,some college,free/reduced,none,58,67,62
female,group C,some college,standard,none,73,76,78
male,group E,some high school,standard,completed,68,51,57
female,group C,high school,free/reduced,completed,50,66,64
male,group B,associate's degree,standard,completed,65,65,63
male,group C,some college,free/reduced,none,63,61,54
female,group C,high school,standard,none,66,71,76
female,group E,master's degree,free/reduced,none,56,72,65
male,group E,associate's degree,standard,completed,94,85,82
female,group C,associate's degree,standard,none,66,77,73
female,group C,associate's degree,standard,completed,67,84,86
male,group D,bachelor's degree,free/reduced,completed,74,79,75
female,group C,bachelor's degree,standard,none,67,69,75
male,group C,bachelor's degree,standard,completed,63,64,66
male,group D,some college,standard,none,76,64,66
male,group A,associate's degree,free/reduced,completed,79,82,82
female,group B,some high school,free/reduced,completed,52,67,72
female,group A,some high school,standard,none,71,83,77
male,group B,bachelor's degree,free/reduced,none,88,75,76
male,group D,some college,standard,none,68,59,62
female,group D,high school,standard,completed,69,77,78
female,group D,some college,standard,none,77,68,77
male,group E,bachelor's degree,standard,none,68,68,64
female,group B,some high school,standard,none,66,69,68
female,group C,associate's degree,standard,none,59,66,67
male,group A,associate's degree,free/reduced,none,62,61,55
female,group C,high school,standard,none,63,69,74
female,group E,high school,free/reduced,none,64,62,68
male,group D,master's degree,standard,none,82,82,74
male,group B,some college,free/reduced,completed,60,62,60
male,group D,some college,standard,none,71,49,52
male,group B,associate's degree,free/reduced,completed,58,57,53
female,group E,associate's degree,standard,none,100,100,100
female,group D,some high school,standard,none,59,58,59
female,group C,master's degree,standard,completed,54,64,67
female,group A,master's degree,standard,none,50,53,58
female,group E,high school,free/reduced,completed,66,74,78
male,group C,associate's degree,free/reduced,none,55,61,54
female,group C,some college,standard,none,83,83,90
male,group A,bachelor's degree,standard,none,66,64,62
male,group D,some high school,standard,completed,62,66,68
female,group B,high school,standard,none,62,62,63
female,group E,associate's degree,free/reduced,completed,83,86,88
female,group D,some college,free/reduced,none,60,66,70
male,group C,some college,standard,none,53,44,42
female,group D,some high school,standard,none,59,72,80
male,group C,associate's degree,standard,none,49,51,43
female,group C,bachelor's degree,free/reduced,none,50,60,59
male,group E,associate's degree,free/reduced,completed,91,73,80
male,group B,some college,standard,none,47,43,41
male,group B,associate's degree,free/reduced,none,67,62,60
female,group B,some high school,standard,none,57,67,72
female,group D,some college,free/reduced,none,71,83,83
female,group E,associate's degree,standard,completed,65,75,77
male,group B,some college,free/reduced,none,54,54,45
male,group C,bachelor's degree,free/reduced,none,37,56,47
male,group C,high school,free/reduced,none,62,55,55
male,group B,some high school,standard,completed,94,86,87
male,group D,bachelor's degree,free/reduced,completed,39,42,38
female,group E,some college,free/reduced,none,71,76,70
female,group D,some college,free/reduced,none,65,81,77
female,group E,some college,free/reduced,completed,75,88,85
female,group C,some college,free/reduced,none,32,39,33
male,group C,some college,standard,none,53,39,37
male,group A,some college,standard,none,53,43,43
male,group A,bachelor's degree,standard,completed,87,84,87
male,group E,bachelor's degree,standard,completed,76,62,66
female,group D,bachelor's degree,free/reduced,none,78,90,93
male,group D,bachelor's degree,standard,none,75,73,74
female,group C,some college,standard,none,58,67,72
male,group B,associate's degree,standard,none,48,43,45
male,group D,master's degree,standard,none,73,70,75
female,group C,some college,standard,completed,69,90,88
female,group E,high school,free/reduced,none,57,58,57
male,group B,some high school,standard,completed,79,85,86
female,group C,some college,standard,none,63,74,74
female,group B,associate's degree,standard,none,47,49,50
male,group D,some high school,standard,completed,74,71,78
male,group E,some college,standard,none,97,87,82
female,group B,some high school,free/reduced,none,49,58,55
male,group C,master's degree,standard,none,79,78,77
male,group C,some high school,free/reduced,none,69,71,65
female,group C,associate's degree,free/reduced,none,53,61,62
male,group C,high school,standard,completed,69,58,53
male,group C,high school,free/reduced,none,27,34,36
female,group D,some high school,free/reduced,completed,35,55,60
female,group B,some high school,free/reduced,completed,63,78,79
male,group B,bachelor's degree,free/reduced,none,48,51,46
female,group C,high school,standard,none,72,80,75
female,group B,high school,standard,none,66,72,70
female,group E,bachelor's degree,standard,none,80,83,83
male,group A,some college,standard,completed,78,72,70
male,group C,high school,standard,none,71,66,65
female,group D,master's degree,standard,none,54,60,63
female,group C,associate's degree,free/reduced,none,57,78,67
female,group D,some college,standard,none,65,70,71
male,group C,high school,free/reduced,completed,53,51,51
female,group D,high school,free/reduced,none,39,52,46
female,group D,associate's degree,free/reduced,none,55,76,76
female,group D,associate's degree,standard,none,59,70,65
female,group B,high school,free/reduced,none,60,72,68
female,group B,associate's degree,standard,none,49,52,54
female,group B,high school,free/reduced,none,8,24,23
female,group D,master's degree,free/reduced,none,40,59,54
female,group C,bachelor's degree,free/reduced,completed,74,86,89
male,group E,some college,standard,none,59,51,43
female,group E,bachelor's degree,free/reduced,completed,92,100,100
male,group C,some college,free/reduced,none,80,64,66
male,group C,bachelor's degree,standard,completed,96,90,92
male,group E,some college,standard,completed,85,75,68
female,group C,bachelor's degree,standard,none,77,88,87
female,group B,high school,free/reduced,completed,76,85,82
male,group C,high school,free/reduced,none,66,66,59
female,group D,bachelor's degree,standard,completed,71,76,83
male,group B,high school,standard,none,60,68,60
male,group D,some college,standard,none,76,71,73
male,group D,some college,standard,completed,58,59,58
female,group B,associate's degree,standard,completed,90,90,91
female,group D,some college,standard,completed,74,75,79
male,group B,some college,free/reduced,none,75,68,65
male,group C,some college,standard,none,69,64,68
female,group B,some high school,standard,completed,60,70,70
female,group B,some college,free/reduced,completed,65,75,70
female,group C,associate's degree,free/reduced,completed,68,67,69
male,group B,high school,standard,completed,72,65,68
male,group B,associate's degree,free/reduced,completed,82,78,74
female,group C,some high school,standard,completed,85,92,93
male,group E,associate's degree,standard,none,72,57,62
male,group D,some college,standard,completed,76,83,79
female,group E,some college,standard,none,67,76,75
male,group A,some college,free/reduced,none,75,81,74
male,group B,some high school,standard,completed,63,67,67
female,group C,associate's degree,standard,none,64,64,70
male,group D,associate's degree,standard,completed,67,72,67
male,group A,some college,free/reduced,none,58,60,57
female,group B,associate's degree,free/reduced,none,53,71,67
male,group C,some high school,standard,none,73,66,63
male,group D,master's degree,standard,none,89,84,82
female,group C,high school,standard,none,65,69,67
female,group C,some college,standard,completed,70,72,76
female,group D,bachelor's degree,standard,none,65,67,62
male,group D,some high school,standard,none,74,74,72
female,group D,associate's degree,standard,none,71,71,74
female,group E,bachelor's degree,standard,none,100,100,100
male,group C,high school,standard,none,71,60,61
male,group E,high school,standard,completed,87,91,81
female,group D,associate's degree,free/reduced,none,26,31,38
male,group B,associate's degree,standard,completed,91,89,92
female,group A,associate's degree,standard,none,82,93,93
male,group D,high school,standard,none,66,69,63
female,group E,bachelor's degree,standard,completed,79,81,82
male,group D,some college,standard,completed,63,55,63
female,group D,master's degree,standard,none,87,100,100
male,group C,bachelor's degree,standard,none,69,63,61
female,group C,associate's degree,standard,none,53,62,53
male,group C,some college,free/reduced,completed,50,48,53
female,group D,associate's degree,standard,none,85,91,89


================================================
File: catboost_info/learn_error.tsv
================================================
iter	RMSE
0	14.61268541
1	14.23196558
2	13.86674177
3	13.49946468
4	13.16661202
5	12.85632158
6	12.56784336
7	12.31617801
8	12.01509946
9	11.71512571
10	11.43264457
11	11.17973129
12	10.93746433
13	10.69387075
14	10.4615922
15	10.2299235
16	10.01419731
17	9.795856918
18	9.60040151
19	9.418664989
20	9.249278645
21	9.090113396
22	8.909548708
23	8.740140208
24	8.59377682
25	8.453885414
26	8.317732234
27	8.180896085
28	8.053220863
29	7.924234474
30	7.818879511
31	7.719346395
32	7.624163594
33	7.524952543
34	7.424464745
35	7.327482321
36	7.238666841
37	7.158842395
38	7.074348246
39	6.992269093
40	6.925219745
41	6.86013671
42	6.786739622
43	6.73057926
44	6.666356486
45	6.606182832
46	6.547854615
47	6.49240675
48	6.431317261
49	6.377586395
50	6.327558015
51	6.282040317
52	6.237655841
53	6.192792003
54	6.150579347
55	6.115693845
56	6.078780782
57	6.045146957
58	6.008080327
59	5.977575734
60	5.948036502
61	5.914888282
62	5.884957796
63	5.849539885
64	5.821387627
65	5.797012279
66	5.768685095
67	5.755484587
68	5.731887055
69	5.707148487
70	5.685365039
71	5.663541834
72	5.638108797
73	5.619204893
74	5.598472793
75	5.577007075
76	5.562637084
77	5.54729991
78	5.530737623
79	5.513704873
80	5.501575529
81	5.484080734
82	5.472178894
83	5.458923342
84	5.451382371
85	5.441410748
86	5.434047468
87	5.422107497
88	5.40731265
89	5.396360181
90	5.383793239
91	5.369605034
92	5.357467333
93	5.341109968
94	5.330529276
95	5.319862075
96	5.313052243
97	5.299677207
98	5.287546698
99	5.279803754
100	5.272254612
101	5.268813888
102	5.258113767
103	5.251018576
104	5.24171237
105	5.233984943
106	5.226240449
107	5.220541644
108	5.211712096
109	5.203388597
110	5.196026544
111	5.188716772
112	5.182163132
113	5.17764747
114	5.170548056
115	5.165901827
116	5.160333668
117	5.148953243
118	5.144729126
119	5.139939946
120	5.132898872
121	5.129073077
122	5.127123916
123	5.121282057
124	5.114489821
125	5.107777291
126	5.103518754
127	5.101133351
128	5.096462705
129	5.08562082
130	5.080483682
131	5.07437499
132	5.070683732
133	5.06432037
134	5.057436484
135	5.053254656
136	5.04801663
137	5.045680836
138	5.041857933
139	5.035264888
140	5.028771948
141	5.024912247
142	5.02239068
143	5.017110572
144	5.010707886
145	5.004389868
146	4.997628683
147	4.991497851
148	4.988345253
149	4.98460423
150	4.982441783
151	4.977461771
152	4.973601586
153	4.968328804
154	4.9640736
155	4.960865882
156	4.957014156
157	4.952244852
158	4.949695675
159	4.946584698
160	4.940192834
161	4.936727994
162	4.932342063
163	4.925830838
164	4.922571773
165	4.918395814
166	4.911142452
167	4.908276131
168	4.904584645
169	4.89795517
170	4.895556491
171	4.890591233
172	4.884629277
173	4.880550786
174	4.876384842
175	4.872638238
176	4.869137118
177	4.863587157
178	4.858028614
179	4.853838176
180	4.853462329
181	4.851573196
182	4.848442087
183	4.845547104
184	4.839868817
185	4.836336958
186	4.831124789
187	4.827407333
188	4.824088918
189	4.820934539
190	4.816975227
191	4.812340486
192	4.806402747
193	4.801521788
194	4.797601172
195	4.792694629
196	4.789794613
197	4.785367055
198	4.780376856
199	4.776691261
200	4.773470753
201	4.77207327
202	4.767347177
203	4.763983068
204	4.761588849
205	4.758558086
206	4.753678804
207	4.746510889
208	4.744840601
209	4.741702222
210	4.739352853
211	4.735539287
212	4.730726751
213	4.726416092
214	4.721293082
215	4.718443695
216	4.711627962
217	4.709790764
218	4.706913073
219	4.703846389
220	4.701001135
221	4.700380864
222	4.695837486
223	4.691242996
224	4.690284609
225	4.68827119
226	4.686922425
227	4.686085874
228	4.683230741
229	4.6773487
230	4.671462906
231	4.664799601
232	4.660979386
233	4.656281279
234	4.651997094
235	4.650151977
236	4.648478046
237	4.646364602
238	4.642635873
239	4.640860821
240	4.639446822
241	4.637120072
242	4.633769267
243	4.630491517
244	4.627813333
245	4.622496304
246	4.618979301
247	4.616563458
248	4.614759385
249	4.61251363
250	4.608796693
251	4.60847774
252	4.605195659
253	4.60103766
254	4.599325598
255	4.597012808
256	4.594161533
257	4.592197259
258	4.58952828
259	4.585816572
260	4.582934052
261	4.582234842
262	4.57571562
263	4.571197349
264	4.56723703
265	4.560335567
266	4.554709345
267	4.552806306
268	4.550766241
269	4.549216045
270	4.547687452
271	4.545642435
272	4.543930359
273	4.541029943
274	4.537781449
275	4.53066128
276	4.526763956
277	4.522036004
278	4.515307629
279	4.51109143
280	4.508233494
281	4.502555629
282	4.500081355
283	4.495281743
284	4.492138869
285	4.491708261
286	4.489398865
287	4.485796373
288	4.479542205
289	4.475986993
290	4.468661855
291	4.463785664
292	4.460445831
293	4.45692492
294	4.454031688
295	4.451672391
296	4.45031553
297	4.446782666
298	4.444369689
299	4.44411683
300	4.440876943
301	4.439336453
302	4.436666901
303	4.433043336
304	4.431419305
305	4.428859646
306	4.424552441
307	4.422706391
308	4.421427994
309	4.418733158
310	4.415656941
311	4.410747417
312	4.408126521
313	4.407843853
314	4.405219398
315	4.402708465
316	4.40104971
317	4.400830038
318	4.39784288
319	4.393741845
320	4.393425936
321	4.390505974
322	4.390305136
323	4.38568203
324	4.383057135
325	4.380662684
326	4.37805727
327	4.373708319
328	4.36887921
329	4.367895929
330	4.362140573
331	4.361931924
332	4.359656377
333	4.354281355
334	4.348589447
335	4.346355698
336	4.343894047
337	4.341561356
338	4.338869061
339	4.333720063
340	4.330892223
341	4.328311474
342	4.324580133
343	4.323354432
344	4.323159553
345	4.318639682
346	4.312712208
347	4.311461062
348	4.311178356
349	4.311102505
350	4.310232308
351	4.305128197
352	4.304595496
353	4.3004992
354	4.297581524
355	4.297355825
356	4.295758715
357	4.292500657
358	4.290797586
359	4.286410608
360	4.280278245
361	4.279941283
362	4.274426275
363	4.273567658
364	4.269488271
365	4.266838205
366	4.263456509
367	4.260360204
368	4.255120549
369	4.252582211
370	4.247857931
371	4.245288911
372	4.24048515
373	4.2378365
374	4.236751063
375	4.233240093
376	4.230358421
377	4.225853583
378	4.224702126
379	4.224361982
380	4.223787954
381	4.22144054
382	4.217739335
383	4.215611665
384	4.210236863
385	4.208454294
386	4.207446501
387	4.204985442
388	4.202796622
389	4.199774004
390	4.199458358
391	4.19751254
392	4.195201346
393	4.190593508
394	4.187212054
395	4.184984466
396	4.182832144
397	4.179752812
398	4.17878934
399	4.174548229
400	4.168872556
401	4.163926494
402	4.159485076
403	4.155926102
404	4.153540115
405	4.150983044
406	4.146539132
407	4.144869174
408	4.142825962
409	4.142467378
410	4.141800768
411	4.140107084
412	4.136243196
413	4.133618439
414	4.131053025
415	4.128379975
416	4.127068571
417	4.125969877
418	4.123893633
419	4.119643722
420	4.11465686
421	4.111565986
422	4.106322452
423	4.104222175
424	4.103008002
425	4.099176337
426	4.096525716
427	4.094967724
428	4.094683584
429	4.091096563
430	4.089178677
431	4.085335077
432	4.084852122
433	4.080322952
434	4.077449519
435	4.076568374
436	4.076071793
437	4.075027644
438	4.07020837
439	4.06833694
440	4.063936568
441	4.061734926
442	4.060815619
443	4.056407489
444	4.05238936
445	4.049685254
446	4.045162016
447	4.042955482
448	4.041467663
449	4.038920476
450	4.036694383
451	4.034999007
452	4.031928018
453	4.028718143
454	4.027401978
455	4.025930598
456	4.021074523
457	4.020268486
458	4.017986361
459	4.014814804
460	4.009704796
461	4.008440457
462	4.004854849
463	4.000898028
464	3.996617985
465	3.995443319
466	3.990771995
467	3.989249359
468	3.986599538
469	3.985628919
470	3.985086112
471	3.98466839
472	3.984233894
473	3.981855845
474	3.978684264
475	3.974361269
476	3.972074049
477	3.97191982
478	3.969616848
479	3.968533527
480	3.967568753
481	3.966232553
482	3.961522393
483	3.955829553
484	3.954248095
485	3.951922606
486	3.94978202
487	3.949518999
488	3.945431216
489	3.943249086
490	3.941981548
491	3.936600517
492	3.934476885
493	3.93045798
494	3.925977375
495	3.92457993
496	3.919849146
497	3.919681964
498	3.917633027
499	3.916616419
500	3.912896166
501	3.912769674
502	3.909373788
503	3.906706036
504	3.903895664
505	3.902982967
506	3.900905528
507	3.897463465
508	3.895918033
509	3.893921936
510	3.890237934
511	3.887311613
512	3.884869735
513	3.882101231
514	3.88060368
515	3.877726076
516	3.874938145
517	3.874340833
518	3.873105224
519	3.869800236
520	3.867932518
521	3.866666962
522	3.865527652
523	3.861900081
524	3.858502887
525	3.858335097
526	3.855719306
527	3.855137428
528	3.851985706
529	3.851051809
530	3.850679146
531	3.848179484
532	3.844551071
533	3.841604794
534	3.83969656
535	3.836811463
536	3.834182551
537	3.828012842
538	3.826636176
539	3.826189798
540	3.824104738
541	3.82300218
542	3.817146402
543	3.814638044
544	3.812979573
545	3.811938825
546	3.811479418
547	3.80742981
548	3.805097043
549	3.801577232
550	3.799762083
551	3.796393455
552	3.793486459
553	3.792006091
554	3.78932128
555	3.787581812
556	3.785308865
557	3.78465475
558	3.780498869
559	3.778734909
560	3.778652349
561	3.777928006
562	3.776131406
563	3.772925617
564	3.7727745
565	3.771763073
566	3.768738162
567	3.767220794
568	3.766691102
569	3.766259335
570	3.76116362
571	3.758633056
572	3.754603809
573	3.754070549
574	3.750534319
575	3.746491516
576	3.744210544
577	3.742929991
578	3.740402992
579	3.739644243
580	3.737529404
581	3.737196923
582	3.733954464
583	3.731608623
584	3.729843952
585	3.727223798
586	3.725351667
587	3.725026912
588	3.721974565
589	3.720269493
590	3.718907466
591	3.715602912
592	3.714463059
593	3.710802077
594	3.708032435
595	3.703788939
596	3.702500944
597	3.698196925
598	3.69658891
599	3.696124066
600	3.693537052
601	3.691343275
602	3.690973831
603	3.686151073
604	3.683921982
605	3.679975271
606	3.674047638
607	3.673151794
608	3.670953108
609	3.6703947
610	3.668433805
611	3.668304956
612	3.667880012
613	3.667282264
614	3.664611483
615	3.662543394
616	3.659041959
617	3.656868545
618	3.654785614
619	3.654566096
620	3.651007045
621	3.648921786
622	3.647655706
623	3.647241144
624	3.644195689
625	3.642632246
626	3.642523322
627	3.641494659
628	3.63976496
629	3.637945652
630	3.636233667
631	3.635304404
632	3.632234951
633	3.631630987
634	3.630301009
635	3.629077964
636	3.628004522
637	3.625062876
638	3.624917986
639	3.622887087
640	3.621220865
641	3.621143844
642	3.620822821
643	3.619202826
644	3.618204882
645	3.614259666
646	3.613415661
647	3.611096773
648	3.610820695
649	3.609290065
650	3.60897947
651	3.606570702
652	3.606292637
653	3.602753838
654	3.599755869
655	3.599509575
656	3.595337722
657	3.595168561
658	3.59290559
659	3.592015928
660	3.590680403
661	3.590361532
662	3.589069739
663	3.585010186
664	3.582997552
665	3.582119256
666	3.579019356
667	3.57719559
668	3.574050046
669	3.572204518
670	3.569508025
671	3.566268704
672	3.562564052
673	3.561173586
674	3.559551904
675	3.558593054
676	3.555647008
677	3.553100059
678	3.55085578
679	3.550737965
680	3.549408772
681	3.547390589
682	3.54723575
683	3.543988891
684	3.543868508
685	3.542752124
686	3.541334315
687	3.540208266
688	3.537730983
689	3.53419867
690	3.532628335
691	3.532204987
692	3.530618147
693	3.528909306
694	3.526700678
695	3.526439574
696	3.525784216
697	3.524112693
698	3.522813541
699	3.519148203
700	3.517236747
701	3.515603888
702	3.51263524
703	3.51123442
704	3.50828027
705	3.508265537
706	3.506774876
707	3.50552068
708	3.501973993
709	3.498880354
710	3.497370959
711	3.496050455
712	3.495630067
713	3.49550669
714	3.493652375
715	3.491915684
716	3.490303956
717	3.488363853
718	3.485483936
719	3.483763466
720	3.481642352
721	3.479826119
722	3.478588728
723	3.475390887
724	3.472636852
725	3.47061883
726	3.46953149
727	3.467217162
728	3.465686216
729	3.462631216
730	3.461451053
731	3.458950469
732	3.458072145
733	3.454364268
734	3.453243406
735	3.453017963
736	3.45201581
737	3.449510107
738	3.447189189
739	3.446423605
740	3.445531987
741	3.445328199
742	3.443327544
743	3.441475798
744	3.439046501
745	3.435899469
746	3.433988352
747	3.433413263
748	3.429944699
749	3.429291634
750	3.429175603
751	3.428780323
752	3.423893341
753	3.421024316
754	3.420969872
755	3.419544785
756	3.417057977
757	3.415160245
758	3.412854597
759	3.410926347
760	3.408963843
761	3.407629634
762	3.404384724
763	3.403041656
764	3.401101731
765	3.400890668
766	3.399289941
767	3.399202366
768	3.397447845
769	3.394655341
770	3.393228229
771	3.389691197
772	3.389626772
773	3.388480222
774	3.388215066
775	3.386324358
776	3.383913061
777	3.382839768
778	3.381961844
779	3.381521703
780	3.380918963
781	3.377139006
782	3.376004372
783	3.373826468
784	3.373546786
785	3.372109181
786	3.370764924
787	3.367833107
788	3.36450404
789	3.361794558
790	3.356880529
791	3.353843509
792	3.353640253
793	3.353055088
794	3.351415617
795	3.349169061
796	3.346211014
797	3.344560452
798	3.344445461
799	3.342608898
800	3.339572899
801	3.339309302
802	3.33698088
803	3.334746342
804	3.332048511
805	3.331932544
806	3.330541133
807	3.329124989
808	3.32583918
809	3.325287299
810	3.322745162
811	3.319733662
812	3.318080007
813	3.317179538
814	3.315685949
815	3.314944576
816	3.312369052
817	3.312321621
818	3.309465776
819	3.308307377
820	3.308076804
821	3.305643326
822	3.304154814
823	3.303076321
824	3.301746372
825	3.301345455
826	3.299181039
827	3.297989357
828	3.294775182
829	3.293840636
830	3.289279397
831	3.286204979
832	3.284833593
833	3.283519914
834	3.282853026
835	3.281944673
836	3.279764865
837	3.277468839
838	3.277124974
839	3.275636037
840	3.274413635
841	3.272636087
842	3.269644466
843	3.26852083
844	3.267820927
845	3.267331099
846	3.267231014
847	3.267158695
848	3.264718705
849	3.261678512
850	3.261368381
851	3.260128799
852	3.258972952
853	3.258724846
854	3.25686724
855	3.255750259
856	3.254915866
857	3.254348625
858	3.253408514
859	3.253291714
860	3.250186342
861	3.249141318
862	3.246300388
863	3.245205135
864	3.244403535
865	3.244211781
866	3.242286347
867	3.239606746
868	3.236773361
869	3.234552446
870	3.233724647
871	3.230851485
872	3.227728941
873	3.225787575
874	3.222797487
875	3.22137014
876	3.219701963
877	3.21783327
878	3.217713355
879	3.217666091
880	3.214962005
881	3.214081828
882	3.213358337
883	3.210850401
884	3.207810328
885	3.204935073
886	3.20280198
887	3.200663495
888	3.197301669
889	3.195666943
890	3.193886506
891	3.190601248
892	3.188983797
893	3.187365749
894	3.186160771
895	3.185358981
896	3.183832164
897	3.183106966
898	3.182838152
899	3.180835453
900	3.179957348
901	3.179752966
902	3.178851983
903	3.176748586
904	3.172960988
905	3.172794225
906	3.169996507
907	3.16813015
908	3.166920871
909	3.16531606
910	3.162035247
911	3.16189467
912	3.161008416
913	3.160560787
914	3.160416261
915	3.15844291
916	3.155040501
917	3.154937548
918	3.153721161
919	3.153170511
920	3.153113601
921	3.150607379
922	3.148408359
923	3.147495728
924	3.146946235
925	3.14472738
926	3.144469477
927	3.142975516
928	3.14079113
929	3.138173032
930	3.136352813
931	3.133735004
932	3.1319496
933	3.129865343
934	3.129830206
935	3.126676562
936	3.12645665
937	3.126378007
938	3.124621914
939	3.123379018
940	3.123332451
941	3.122432444
942	3.121889006
943	3.121070914
944	3.118444705
945	3.116820458
946	3.114971041
947	3.114685138
948	3.113399363
949	3.110575399
950	3.110466553
951	3.108691641
952	3.107898017
953	3.105551729
954	3.104824945
955	3.103186704
956	3.102330655
957	3.102291298
958	3.100363529
959	3.097871596
960	3.097372213
961	3.095981813
962	3.095347452
963	3.094728409
964	3.094118537
965	3.093793695
966	3.092581602
967	3.091694202
968	3.090298312
969	3.090020107
970	3.088949869
971	3.086700204
972	3.086584317
973	3.084850742
974	3.084239948
975	3.082561673
976	3.07991814
977	3.079350277
978	3.077637462
979	3.076733301
980	3.07594065
981	3.072594655
982	3.071977674
983	3.069532634
984	3.068870784
985	3.068801463
986	3.067433249
987	3.065611456
988	3.063039162
989	3.06177873
990	3.061689148
991	3.058984039
992	3.057492984
993	3.057290731
994	3.056068348
995	3.054987307
996	3.054200234
997	3.052732869
998	3.051787862
999	3.051366222


================================================
File: catboost_info/time_left.tsv
================================================
iter	Passed	Remaining
0	188	187904
1	188	94310
2	189	63092
3	190	47464
4	191	38096
5	192	31863
6	193	27406
7	194	24073
8	194	21460
9	195	19377
10	196	17672
11	197	16251
12	198	15055
13	199	14028
14	200	13135
15	200	12354
16	201	11669
17	202	11054
18	203	10512
19	204	10018
20	205	9574
21	206	9170
22	207	8798
23	208	8458
24	208	8144
25	209	7857
26	210	7588
27	211	7340
28	212	7108
29	213	6891
30	213	6686
31	214	6494
32	215	6314
33	216	6143
34	216	5982
35	217	5831
36	218	5688
37	219	5554
38	220	5428
39	221	5307
40	222	5192
41	222	5085
42	223	4980
43	224	4884
44	225	4788
45	226	4695
46	227	4610
47	228	4529
48	229	4450
49	230	4374
50	231	4301
51	232	4229
52	232	4158
53	233	4091
54	234	4027
55	235	3965
56	236	3906
57	237	3852
58	238	3801
59	239	3748
60	240	3698
61	241	3648
62	241	3598
63	242	3551
64	243	3505
65	244	3461
66	245	3417
67	245	3370
68	246	3331
69	247	3290
70	248	3253
71	249	3216
72	250	3179
73	251	3145
74	252	3111
75	253	3081
76	254	3052
77	255	3022
78	256	2991
79	257	2960
80	258	2929
81	259	2899
82	259	2871
83	260	2843
84	261	2812
85	262	2785
86	262	2756
87	263	2731
88	264	2706
89	265	2683
90	266	2659
91	267	2639
92	268	2618
93	269	2595
94	270	2573
95	270	2551
96	271	2531
97	272	2509
98	273	2489
99	274	2470
100	275	2449
101	275	2427
102	276	2408
103	277	2389
104	278	2371
105	279	2353
106	279	2336
107	280	2318
108	281	2302
109	282	2287
110	283	2271
111	284	2258
112	285	2242
113	286	2227
114	287	2212
115	288	2195
116	288	2181
117	289	2166
118	290	2151
119	291	2137
120	292	2122
121	293	2109
122	293	2093
123	294	2079
124	295	2066
125	296	2053
126	297	2042
127	297	2028
128	298	2015
129	299	2002
130	300	1990
131	300	1977
132	301	1965
133	302	1952
134	302	1941
135	303	1929
136	304	1917
137	304	1905
138	305	1893
139	306	1882
140	307	1871
141	307	1860
142	308	1849
143	309	1838
144	310	1828
145	310	1817
146	311	1809
147	312	1800
148	313	1790
149	314	1780
150	314	1770
151	315	1760
152	316	1751
153	317	1742
154	317	1732
155	318	1723
156	319	1714
157	320	1705
158	320	1696
159	321	1687
160	322	1679
161	322	1670
162	323	1662
163	324	1653
164	325	1645
165	325	1637
166	327	1631
167	327	1623
168	328	1615
169	329	1607
170	330	1600
171	330	1592
172	331	1584
173	332	1576
174	332	1569
175	333	1562
176	334	1554
177	335	1547
178	335	1540
179	336	1533
180	337	1524
181	337	1518
182	338	1511
183	339	1504
184	340	1498
185	340	1492
186	341	1486
187	342	1480
188	343	1474
189	344	1467
190	344	1461
191	345	1454
192	346	1448
193	347	1442
194	348	1436
195	348	1430
196	349	1424
197	350	1418
198	350	1412
199	351	1406
200	352	1400
201	353	1394
202	353	1388
203	354	1383
204	355	1378
205	356	1373
206	357	1368
207	357	1362
208	358	1357
209	359	1351
210	360	1346
211	360	1341
212	361	1336
213	362	1330
214	363	1325
215	363	1320
216	364	1315
217	365	1310
218	366	1305
219	366	1301
220	367	1296
221	368	1291
222	369	1287
223	371	1286
224	372	1282
225	373	1277
226	373	1272
227	374	1267
228	375	1263
229	375	1258
230	376	1253
231	377	1248
232	378	1244
233	378	1239
234	379	1235
235	380	1231
236	381	1226
237	381	1222
238	382	1217
239	383	1213
240	383	1209
241	384	1204
242	385	1202
243	386	1198
244	387	1194
245	388	1190
246	389	1186
247	389	1182
248	390	1177
249	391	1173
250	392	1170
251	392	1165
252	393	1161
253	394	1157
254	394	1153
255	395	1149
256	396	1146
257	397	1142
258	397	1138
259	398	1134
260	399	1130
261	400	1127
262	401	1124
263	402	1120
264	402	1116
265	403	1113
266	404	1109
267	404	1105
268	405	1101
269	406	1098
270	406	1094
271	407	1091
272	408	1087
273	409	1084
274	410	1081
275	410	1077
276	411	1074
277	412	1070
278	413	1067
279	413	1064
280	414	1061
281	415	1058
282	416	1055
283	417	1052
284	418	1048
285	418	1045
286	419	1042
287	420	1038
288	420	1035
289	421	1032
290	422	1029
291	423	1026
292	423	1023
293	424	1019
294	425	1016
295	426	1013
296	426	1010
297	427	1006
298	428	1003
299	428	1000
300	429	998
301	430	995
302	431	992
303	432	989
304	433	986
305	433	983
306	434	980
307	435	977
308	435	974
309	436	971
310	437	968
311	438	965
312	438	963
313	439	960
314	440	957
315	440	954
316	441	951
317	442	948
318	443	946
319	443	943
320	444	940
321	445	938
322	446	935
323	447	933
324	447	930
325	448	927
326	449	924
327	450	922
328	450	919
329	451	916
330	452	914
331	452	911
332	453	908
333	454	906
334	455	903
335	455	901
336	456	898
337	457	895
338	458	893
339	459	891
340	460	889
341	460	886
342	461	884
343	462	881
344	463	879
345	464	877
346	464	874
347	465	872
348	466	870
349	467	868
350	468	865
351	468	863
352	469	860
353	470	858
354	470	855
355	471	853
356	472	850
357	473	848
358	474	846
359	475	844
360	475	842
361	476	840
362	477	837
363	478	835
364	478	833
365	479	830
366	480	828
367	481	826
368	481	823
369	482	821
370	483	819
371	483	817
372	484	814
373	485	812
374	486	810
375	486	808
376	487	805
377	488	803
378	489	801
379	489	799
380	490	797
381	491	794
382	492	792
383	492	790
384	493	788
385	494	786
386	495	784
387	495	782
388	496	779
389	497	777
390	498	775
391	498	773
392	499	771
393	500	769
394	500	767
395	501	765
396	502	763
397	503	760
398	503	758
399	504	756
400	505	754
401	506	752
402	506	750
403	507	748
404	508	746
405	509	744
406	509	742
407	510	740
408	511	738
409	512	737
410	512	734
411	513	733
412	514	731
413	515	729
414	515	727
415	516	725
416	517	723
417	518	721
418	518	719
419	519	717
420	520	715
421	521	714
422	522	712
423	522	710
424	523	708
425	524	706
426	525	705
427	526	703
428	527	701
429	527	699
430	528	697
431	529	696
432	530	694
433	530	692
434	531	690
435	532	688
436	532	686
437	533	684
438	534	682
439	535	681
440	535	679
441	536	677
442	537	675
443	538	673
444	538	671
445	539	670
446	540	668
447	541	666
448	541	664
449	542	663
450	543	661
451	543	659
452	544	657
453	545	655
454	546	654
455	546	652
456	547	650
457	548	648
458	549	647
459	549	645
460	550	643
461	551	641
462	551	640
463	552	638
464	553	636
465	554	635
466	554	633
467	555	631
468	556	629
469	557	628
470	557	626
471	558	624
472	559	623
473	559	621
474	560	619
475	561	617
476	562	616
477	562	614
478	563	612
479	564	611
480	565	609
481	565	608
482	566	606
483	567	604
484	567	602
485	568	601
486	569	599
487	570	598
488	570	596
489	571	594
490	572	593
491	572	591
492	573	590
493	574	588
494	575	586
495	575	585
496	576	583
497	577	582
498	578	580
499	578	578
500	579	577
501	580	575
502	580	574
503	581	572
504	582	570
505	583	569
506	583	567
507	584	566
508	585	564
509	586	563
510	586	561
511	587	559
512	588	558
513	589	556
514	589	555
515	590	553
516	591	552
517	592	550
518	592	549
519	593	547
520	594	546
521	595	544
522	595	543
523	596	541
524	597	540
525	597	538
526	598	537
527	599	535
528	600	534
529	600	532
530	601	531
531	602	529
532	602	528
533	603	526
534	604	525
535	605	523
536	605	522
537	606	520
538	607	519
539	608	518
540	608	516
541	609	515
542	610	513
543	611	512
544	612	510
545	612	509
546	613	508
547	614	506
548	614	505
549	615	503
550	616	502
551	617	500
552	617	499
553	618	497
554	619	496
555	619	495
556	620	493
557	621	492
558	622	490
559	622	489
560	623	487
561	624	486
562	624	485
563	625	483
564	626	482
565	627	480
566	627	479
567	628	478
568	629	476
569	629	475
570	630	473
571	631	472
572	632	470
573	632	469
574	633	468
575	634	466
576	634	465
577	635	464
578	636	462
579	637	461
580	637	460
581	638	458
582	639	457
583	640	455
584	640	454
585	641	453
586	642	451
587	643	450
588	643	449
589	644	447
590	645	446
591	645	445
592	646	443
593	647	442
594	647	441
595	648	439
596	649	438
597	650	436
598	650	435
599	651	434
600	652	432
601	652	431
602	653	430
603	654	428
604	655	427
605	655	426
606	656	425
607	657	423
608	657	422
609	658	421
610	659	419
611	659	418
612	660	417
613	661	415
614	662	414
615	662	413
616	663	411
617	664	410
618	665	409
619	665	408
620	666	406
621	667	405
622	667	404
623	668	402
624	669	401
625	670	400
626	670	398
627	671	397
628	672	396
629	672	395
630	673	393
631	674	392
632	674	391
633	675	389
634	676	388
635	676	387
636	677	386
637	678	384
638	679	383
639	679	382
640	680	381
641	681	379
642	681	378
643	682	377
644	683	376
645	684	374
646	684	373
647	685	372
648	686	371
649	686	369
650	687	368
651	688	367
652	689	366
653	689	364
654	690	363
655	691	362
656	692	361
657	692	360
658	693	358
659	694	357
660	695	356
661	695	355
662	696	353
663	697	352
664	697	351
665	698	350
666	699	349
667	700	347
668	700	346
669	701	345
670	702	344
671	702	343
672	703	341
673	704	340
674	705	339
675	705	338
676	706	337
677	707	335
678	708	334
679	708	333
680	709	332
681	710	331
682	710	329
683	711	328
684	712	327
685	713	326
686	713	325
687	714	323
688	715	322
689	715	321
690	716	320
691	717	319
692	718	318
693	718	316
694	719	315
695	720	314
696	721	313
697	722	312
698	722	311
699	723	310
700	724	308
701	725	307
702	725	306
703	726	305
704	727	304
705	727	303
706	728	301
707	729	300
708	730	299
709	730	298
710	731	297
711	732	296
712	732	294
713	733	293
714	734	292
715	735	291
716	735	290
717	736	289
718	737	288
719	738	287
720	738	285
721	739	284
722	740	283
723	740	282
724	741	281
725	742	280
726	743	279
727	744	278
728	745	276
729	745	275
730	746	274
731	747	273
732	747	272
733	748	271
734	749	270
735	750	269
736	750	267
737	751	266
738	752	265
739	752	264
740	753	263
741	754	262
742	754	261
743	755	260
744	756	258
745	757	257
746	757	256
747	758	255
748	759	254
749	759	253
750	760	252
751	761	251
752	762	250
753	762	248
754	763	247
755	764	246
756	765	245
757	765	244
758	766	243
759	767	242
760	768	241
761	768	240
762	769	239
763	770	237
764	771	236
765	771	235
766	772	234
767	773	233
768	773	232
769	774	231
770	775	230
771	776	229
772	776	228
773	777	227
774	778	225
775	778	224
776	779	223
777	780	222
778	781	221
779	781	220
780	782	219
781	783	218
782	784	217
783	784	216
784	785	215
785	786	214
786	787	213
787	787	211
788	788	210
789	789	209
790	790	208
791	790	207
792	791	206
793	792	205
794	792	204
795	793	203
796	794	202
797	795	201
798	796	200
799	796	199
800	797	198
801	798	197
802	798	196
803	799	194
804	800	193
805	801	192
806	801	191
807	802	190
808	803	189
809	803	188
810	804	187
811	805	186
812	806	185
813	806	184
814	807	183
815	808	182
816	809	181
817	809	180
818	810	179
819	811	178
820	811	176
821	812	175
822	813	174
823	813	173
824	814	172
825	815	171
826	816	170
827	816	169
828	817	168
829	818	167
830	819	166
831	819	165
832	820	164
833	821	163
834	822	162
835	822	161
836	823	160
837	824	159
838	824	158
839	825	157
840	826	156
841	827	155
842	827	154
843	828	153
844	829	152
845	830	151
846	830	150
847	831	149
848	832	147
849	832	146
850	833	145
851	834	144
852	834	143
853	835	142
854	836	141
855	837	140
856	837	139
857	838	138
858	839	137
859	840	136
860	840	135
861	841	134
862	842	133
863	843	132
864	843	131
865	844	130
866	845	129
867	845	128
868	846	127
869	847	126
870	848	125
871	849	124
872	849	123
873	850	122
874	851	121
875	852	120
876	852	119
877	853	118
878	854	117
879	855	116
880	855	115
881	856	114
882	857	113
883	858	112
884	858	111
885	859	110
886	860	109
887	861	108
888	862	107
889	862	106
890	863	105
891	864	104
892	865	103
893	865	102
894	866	101
895	867	100
896	867	99
897	868	98
898	869	97
899	870	96
900	870	95
901	871	94
902	872	93
903	872	92
904	873	91
905	874	90
906	875	89
907	875	88
908	876	87
909	877	86
910	878	85
911	878	84
912	879	83
913	880	82
914	881	81
915	881	80
916	882	79
917	883	78
918	884	77
919	884	76
920	885	75
921	886	74
922	886	73
923	887	73
924	888	72
925	889	71
926	890	70
927	890	69
928	891	68
929	892	67
930	893	66
931	893	65
932	894	64
933	895	63
934	895	62
935	896	61
936	897	60
937	898	59
938	899	58
939	899	57
940	900	56
941	901	55
942	902	54
943	903	53
944	903	52
945	904	51
946	905	50
947	906	49
948	906	48
949	907	47
950	908	46
951	909	45
952	909	44
953	910	43
954	911	42
955	912	41
956	913	41
957	913	40
958	914	39
959	915	38
960	915	37
961	916	36
962	917	35
963	918	34
964	918	33
965	919	32
966	920	31
967	921	30
968	922	29
969	922	28
970	923	27
971	924	26
972	925	25
973	925	24
974	926	23
975	927	22
976	927	21
977	928	20
978	929	19
979	929	18
980	930	18
981	931	17
982	932	16
983	932	15
984	933	14
985	934	13
986	935	12
987	935	11
988	936	10
989	937	9
990	938	8
991	938	7
992	939	6
993	940	5
994	940	4
995	941	3
996	942	2
997	943	1
998	943	0
999	944	0


================================================
File: notebook/catboost_info/learn_error.tsv
================================================
iter	RMSE
0	14.59871775
1	14.22518863
2	13.8866124
3	13.52356875
4	13.18870211
5	12.9124226
6	12.60003351
7	12.32990573
8	12.0660619
9	11.77309809
10	11.49227636
11	11.26264833
12	11.04260393
13	10.79916926
14	10.5541002
15	10.31918115
16	10.10004441
17	9.894556723
18	9.690174082
19	9.506034866
20	9.338524633
21	9.170203312
22	9.010299969
23	8.843866678
24	8.690548059
25	8.555307687
26	8.413851302
27	8.29256019
28	8.162193833
29	8.040736294
30	7.921220278
31	7.809589039
32	7.699675801
33	7.60645267
34	7.498880302
35	7.401158744
36	7.306431077
37	7.223863468
38	7.143477682
39	7.062817131
40	6.990793435
41	6.931862224
42	6.858296558
43	6.786268161
44	6.717118549
45	6.652948106
46	6.589686076
47	6.531283097
48	6.471368031
49	6.416481395
50	6.369870822
51	6.325678067
52	6.275766122
53	6.224773967
54	6.182249254
55	6.139904173
56	6.100525536
57	6.062568564
58	6.030559288
59	5.997931376
60	5.964103835
61	5.933729724
62	5.901932711
63	5.873727795
64	5.846485734
65	5.820630862
66	5.793646238
67	5.765388646
68	5.747073912
69	5.722315217
70	5.699970599
71	5.682732647
72	5.663525791
73	5.642381613
74	5.621404779
75	5.602460129
76	5.587337574
77	5.572393026
78	5.558425806
79	5.541518902
80	5.526157283
81	5.511746977
82	5.504975022
83	5.493980522
84	5.480745463
85	5.464052013
86	5.449143727
87	5.438255432
88	5.425927931
89	5.412331217
90	5.400691499
91	5.391618744
92	5.380766572
93	5.372207183
94	5.362918837
95	5.348292324
96	5.339772295
97	5.331515983
98	5.323861644
99	5.313277281
100	5.30685627
101	5.29652059
102	5.288350766
103	5.27993346
104	5.271789139
105	5.262684686
106	5.251619772
107	5.241012171
108	5.232684032
109	5.224267141
110	5.216324968
111	5.211102527
112	5.206364257
113	5.196937641
114	5.190746047
115	5.181885213
116	5.178293568
117	5.170991438
118	5.166257212
119	5.161421246
120	5.158223542
121	5.15470988
122	5.146388143
123	5.139529364
124	5.134276023
125	5.128536065
126	5.123300353
127	5.116200518
128	5.108728704
129	5.104075866
130	5.100317606
131	5.095814182
132	5.092146915
133	5.085258329
134	5.079219421
135	5.076476829
136	5.072133867
137	5.067031564
138	5.061358312
139	5.056759124
140	5.050254
141	5.048376474
142	5.042603751
143	5.037477349
144	5.03230489
145	5.025976187
146	5.01952451
147	5.017569424
148	5.008962367
149	5.003880266
150	4.996387564
151	4.993361683
152	4.99005752
153	4.986908764
154	4.985207175
155	4.979003494
156	4.973142217
157	4.967056696
158	4.961460696
159	4.954240363
160	4.952320707
161	4.948337777
162	4.943228639
163	4.938545759
164	4.935330572
165	4.932051543
166	4.928964153
167	4.922860227
168	4.917668772
169	4.910365386
170	4.906457206
171	4.901996508
172	4.896816173
173	4.891123132
174	4.887630202
175	4.885259712
176	4.880659728
177	4.877299723
178	4.874038772
179	4.86943698
180	4.861268711
181	4.858673056
182	4.854036207
183	4.85130244
184	4.847510084
185	4.845691673
186	4.842058646
187	4.838623675
188	4.834602161
189	4.830858976
190	4.827590511
191	4.822246226
192	4.816686395
193	4.814797121
194	4.811639392
195	4.804813432
196	4.80251613
197	4.800087884
198	4.794773135
199	4.791664833
200	4.785989873
201	4.782387142
202	4.778623704
203	4.775772178
204	4.77023101
205	4.764496689
206	4.763138716
207	4.758053283
208	4.755063788
209	4.752465267
210	4.748067377
211	4.744163298
212	4.736670896
213	4.73349255
214	4.731864595
215	4.728398311
216	4.723865975
217	4.723483365
218	4.72095416
219	4.716173382
220	4.710563974
221	4.707160263
222	4.70408351
223	4.702501831
224	4.700269738
225	4.695346302
226	4.689769299
227	4.687919514
228	4.682982074
229	4.679578702
230	4.675751423
231	4.672710726
232	4.6690533
233	4.667595702
234	4.664241291
235	4.662246999
236	4.658668443
237	4.658017572
238	4.657656705
239	4.653242871
240	4.6507302
241	4.647671336
242	4.645506007
243	4.641297906
244	4.63798107
245	4.635062318
246	4.631606017
247	4.630215494
248	4.62729814
249	4.624691756
250	4.621840053
251	4.619602091
252	4.613584029
253	4.6119388
254	4.60870702
255	4.604771557
256	4.604231277
257	4.600750354
258	4.596488253
259	4.59470784
260	4.588818005
261	4.585054174
262	4.581471435
263	4.577555199
264	4.57457145
265	4.573029224
266	4.571326203
267	4.571019224
268	4.565766631
269	4.563383471
270	4.558165805
271	4.55629209
272	4.549383848
273	4.546227503
274	4.540620977
275	4.532898237
276	4.529757367
277	4.52579294
278	4.521572155
279	4.518818099
280	4.514012334
281	4.508331416
282	4.506038537
283	4.505805643
284	4.501550352
285	4.494692643
286	4.489330245
287	4.484826955
288	4.478285428
289	4.475311671
290	4.471581298
291	4.469164956
292	4.462068024
293	4.458540976
294	4.455757458
295	4.453027414
296	4.450659338
297	4.444696901
298	4.442052813
299	4.440036756
300	4.435017051
301	4.429962125
302	4.427666256
303	4.425030004
304	4.422588771
305	4.418936667
306	4.416887976
307	4.413773354
308	4.411129391
309	4.40850819
310	4.404880292
311	4.404593827
312	4.402139942
313	4.400284725
314	4.395805521
315	4.392838833
316	4.389213061
317	4.3861981
318	4.385054234
319	4.379364607
320	4.378387619
321	4.375816727
322	4.374600202
323	4.37098897
324	4.367418484
325	4.363584638
326	4.360770706
327	4.358675808
328	4.354228195
329	4.353773425
330	4.351648009
331	4.348209927
332	4.345852733
333	4.34237339
334	4.340814362
335	4.339707068
336	4.335430786
337	4.33521887
338	4.328507238
339	4.326140732
340	4.323785636
341	4.320632534
342	4.31973724
343	4.318000924
344	4.312713628
345	4.308220789
346	4.305890947
347	4.305137856
348	4.304787172
349	4.30350134
350	4.30293809
351	4.299548459
352	4.294768408
353	4.294581479
354	4.292551604
355	4.288628894
356	4.287802767
357	4.284566905
358	4.27890232
359	4.276938023
360	4.273425094
361	4.269726505
362	4.265565857
363	4.262117173
364	4.259185148
365	4.257668507
366	4.253432436
367	4.250216667
368	4.24514828
369	4.244015772
370	4.240329192
371	4.236718014
372	4.233495944
373	4.230357491
374	4.227381215
375	4.226169067
376	4.225963861
377	4.224346548
378	4.222595381
379	4.219700938
380	4.216356207
381	4.214326494
382	4.212901398
383	4.211331091
384	4.208624791
385	4.204659106
386	4.202968852
387	4.197261935
388	4.196934403
389	4.192479929
390	4.190243116
391	4.185852431
392	4.180967944
393	4.178283162
394	4.176341157
395	4.176041504
396	4.175564513
397	4.170830274
398	4.167804808
399	4.165860548
400	4.160950571
401	4.158544115
402	4.154448767
403	4.15197457
404	4.149562679
405	4.149327961
406	4.147444889
407	4.147254809
408	4.144589165
409	4.141691034
410	4.138893468
411	4.136964597
412	4.133773523
413	4.131363444
414	4.130228592
415	4.12884186
416	4.12413494
417	4.119422508
418	4.117038244
419	4.116067631
420	4.110343455
421	4.105604544
422	4.103390131
423	4.096724402
424	4.093431279
425	4.091412744
426	4.091192901
427	4.088994581
428	4.087845355
429	4.083650279
430	4.079840652
431	4.075707828
432	4.071345409
433	4.067460475
434	4.064914749
435	4.063823175
436	4.058898471
437	4.054120355
438	4.052715901
439	4.052114167
440	4.051377267
441	4.049275336
442	4.047099198
443	4.044348685
444	4.042822379
445	4.041258611
446	4.040044254
447	4.036860137
448	4.036372284
449	4.03368231
450	4.032439519
451	4.032008634
452	4.030034932
453	4.028173412
454	4.023806097
455	4.020239186
456	4.017384838
457	4.016468229
458	4.014530169
459	4.012884822
460	4.008924707
461	4.008286778
462	4.006444202
463	4.003284363
464	3.999811484
465	3.997483319
466	3.99708333
467	3.995860263
468	3.994131111
469	3.993125499
470	3.992954084
471	3.990635743
472	3.986678254
473	3.986393291
474	3.986152671
475	3.985596662
476	3.983853457
477	3.983263928
478	3.980067202
479	3.978115407
480	3.972661644
481	3.969938254
482	3.968211434
483	3.966113368
484	3.962246044
485	3.961319703
486	3.96078191
487	3.958913633
488	3.957428286
489	3.952641965
490	3.951486786
491	3.94938344
492	3.947297924
493	3.946284886
494	3.943288204
495	3.943076488
496	3.942347078
497	3.940251541
498	3.939011019
499	3.938760512
500	3.934522727
501	3.930091193
502	3.927671752
503	3.927280041
504	3.925987561
505	3.921645957
506	3.920333743
507	3.918659461
508	3.916124385
509	3.913849268
510	3.912826119
511	3.910034145
512	3.90792379
513	3.907823851
514	3.904527401
515	3.903378283
516	3.902170135
517	3.899435429
518	3.897497505
519	3.896426119
520	3.896044669
521	3.892795942
522	3.892413888
523	3.891738985
524	3.88822057
525	3.883837053
526	3.881769742
527	3.880356898
528	3.879250273
529	3.877174493
530	3.875854201
531	3.873189396
532	3.871583368
533	3.869622679
534	3.866860164
535	3.864498444
536	3.861416322
537	3.860265769
538	3.859328725
539	3.858521278
540	3.85599111
541	3.852379816
542	3.851327537
543	3.847854898
544	3.847083414
545	3.845397429
546	3.841933528
547	3.840996474
548	3.838355966
549	3.83342799
550	3.828161535
551	3.823502399
552	3.820068862
553	3.817951642
554	3.815785958
555	3.81503012
556	3.813529315
557	3.808881326
558	3.808740434
559	3.80714352
560	3.80573346
561	3.803370052
562	3.803215211
563	3.801309667
564	3.800003847
565	3.798026138
566	3.794745847
567	3.794312041
568	3.792205926
569	3.788991061
570	3.787391572
571	3.7826445
572	3.781700331
573	3.780045374
574	3.775462644
575	3.773711943
576	3.772415806
577	3.770170875
578	3.765166479
579	3.763976102
580	3.760761708
581	3.758155662
582	3.756844548
583	3.753098923
584	3.751618946
585	3.751398464
586	3.749920103
587	3.748237465
588	3.746021536
589	3.74417648
590	3.738788389
591	3.736021611
592	3.734546151
593	3.733527479
594	3.733026392
595	3.730314752
596	3.727342596
597	3.725459604
598	3.723158842
599	3.722362756
600	3.721443615
601	3.720743486
602	3.720215334
603	3.719436563
604	3.715046574
605	3.71443396
606	3.712938941
607	3.712814764
608	3.71235348
609	3.711223749
610	3.710755648
611	3.708668039
612	3.705623591
613	3.704045802
614	3.699915209
615	3.696182221
616	3.694263278
617	3.693810229
618	3.690900932
619	3.688020895
620	3.685491894
621	3.684328128
622	3.680640153
623	3.678932783
624	3.674167041
625	3.672200976
626	3.670606604
627	3.665506042
628	3.664214007
629	3.66261643
630	3.662118449
631	3.660871366
632	3.656695243
633	3.654497801
634	3.654189134
635	3.653957699
636	3.651920725
637	3.65106705
638	3.650612844
639	3.649884732
640	3.646276182
641	3.645808194
642	3.645042989
643	3.639864499
644	3.637318478
645	3.636344905
646	3.634754643
647	3.630724975
648	3.629408252
649	3.62790646
650	3.627388973
651	3.625163557
652	3.623701985
653	3.622342639
654	3.621130639
655	3.619125665
656	3.618878783
657	3.616207429
658	3.61206486
659	3.611955873
660	3.610713743
661	3.610044025
662	3.607342669
663	3.602510618
664	3.600492392
665	3.597171065
666	3.594358798
667	3.593945251
668	3.593607748
669	3.592238714
670	3.590144654
671	3.587151536
672	3.586070865
673	3.585102746
674	3.584362847
675	3.581407561
676	3.57919422
677	3.577632204
678	3.573911195
679	3.573408216
680	3.571076482
681	3.568177107
682	3.564824621
683	3.562475026
684	3.561226889
685	3.559383467
686	3.555209766
687	3.55345383
688	3.552300968
689	3.551253986
690	3.549807725
691	3.547992926
692	3.545438607
693	3.543313763
694	3.542962896
695	3.539800465
696	3.53953187
697	3.536332425
698	3.534993327
699	3.531637011
700	3.531040284
701	3.529973872
702	3.527325183
703	3.527200815
704	3.523459911
705	3.522729934
706	3.522260582
707	3.518641591
708	3.516856405
709	3.515083413
710	3.514671214
711	3.512617621
712	3.510747971
713	3.508149266
714	3.504557165
715	3.501367208
716	3.497593362
717	3.496291169
718	3.49330374
719	3.490044385
720	3.488891789
721	3.486869081
722	3.486303638
723	3.486035862
724	3.482668951
725	3.47820549
726	3.474392283
727	3.472268163
728	3.471209164
729	3.470049971
730	3.469078654
731	3.468643292
732	3.468134848
733	3.467820182
734	3.465776156
735	3.463037608
736	3.45965637
737	3.457275091
738	3.455522955
739	3.451270508
740	3.450978947
741	3.450608492
742	3.448122004
743	3.445646714
744	3.44411263
745	3.443264164
746	3.439521363
747	3.436809225
748	3.435509465
749	3.434450277
750	3.431786753
751	3.430150302
752	3.42696281
753	3.426870334
754	3.425200309
755	3.423383783
756	3.422752979
757	3.420845924
758	3.418917862
759	3.417095735
760	3.414391848
761	3.41426711
762	3.411976013
763	3.408614844
764	3.407906721
765	3.407494015
766	3.406287128
767	3.404600442
768	3.404012662
769	3.400398762
770	3.399737714
771	3.399185251
772	3.397245411
773	3.396229831
774	3.394002836
775	3.389617243
776	3.385426488
777	3.385072258
778	3.383842366
779	3.380110746
780	3.378595218
781	3.377371219
782	3.375814782
783	3.374195887
784	3.373704309
785	3.372971757
786	3.371565136
787	3.3700309
788	3.368186663
789	3.366710398
790	3.364598042
791	3.36348442
792	3.362001759
793	3.361580401
794	3.359543675
795	3.35766119
796	3.354592969
797	3.350985986
798	3.349589624
799	3.349110856
800	3.345315441
801	3.34397474
802	3.343800902
803	3.342329465
804	3.34220347
805	3.341279219
806	3.337329521
807	3.336708453
808	3.333252703
809	3.332186769
810	3.330594851
811	3.327867442
812	3.327563111
813	3.326308994
814	3.326249879
815	3.322336238
816	3.320491154
817	3.320434271
818	3.319609638
819	3.316182682
820	3.313126612
821	3.311222803
822	3.309627335
823	3.307974895
824	3.307321505
825	3.306759087
826	3.30484037
827	3.30201994
828	3.299302297
829	3.298132671
830	3.296976183
831	3.293144745
832	3.290284404
833	3.288285745
834	3.286662667
835	3.285692682
836	3.284565611
837	3.283897383
838	3.280712855
839	3.280468341
840	3.277814895
841	3.276316385
842	3.27557063
843	3.27499628
844	3.273569958
845	3.269665595
846	3.267984685
847	3.267702508
848	3.266671924
849	3.265359825
850	3.2618983
851	3.260456253
852	3.258389147
853	3.258108469
854	3.256424936
855	3.254817983
856	3.253898449
857	3.253023652
858	3.252845542
859	3.252316026
860	3.24975301
861	3.249151075
862	3.246257135
863	3.243826471
864	3.243190653
865	3.24008797
866	3.240036811
867	3.238676314
868	3.237848782
869	3.236405498
870	3.233378068
871	3.233169169
872	3.232434885
873	3.229978115
874	3.229648848
875	3.227208761
876	3.226258869
877	3.225331995
878	3.221968268
879	3.221166317
880	3.220047503
881	3.21982781
882	3.216693405
883	3.214798761
884	3.214395085
885	3.213470696
886	3.211595017
887	3.210965255
888	3.209413583
889	3.208331683
890	3.207110711
891	3.204519413
892	3.201421296
893	3.200691259
894	3.200485067
895	3.197467568
896	3.195838544
897	3.195671746
898	3.194213094
899	3.192884691
900	3.188364239
901	3.185219478
902	3.184120387
903	3.183725321
904	3.181264554
905	3.180741589
906	3.179782639
907	3.176377564
908	3.174078182
909	3.171242379
910	3.170421012
911	3.170087224
912	3.167303059
913	3.16705966
914	3.165667683
915	3.164689733
916	3.163311455
917	3.159425016
918	3.159327058
919	3.15819426
920	3.157086603
921	3.153354154
922	3.153261317
923	3.153168454
924	3.152080399
925	3.151635144
926	3.147762655
927	3.147172317
928	3.146254125
929	3.144869828
930	3.143466218
931	3.141919643
932	3.141054135
933	3.139390417
934	3.139025283
935	3.137824183
936	3.135620453
937	3.13443178
938	3.132765713
939	3.131434124
940	3.128384262
941	3.128120455
942	3.126314615
943	3.125409666
944	3.123072589
945	3.122921099
946	3.122182662
947	3.120040951
948	3.118767799
949	3.118203218
950	3.116343362
951	3.11480829
952	3.112607903
953	3.111661241
954	3.110249676
955	3.107333579
956	3.107149851
957	3.106867336
958	3.10364681
959	3.102703811
960	3.101469264
961	3.09863569
962	3.09645939
963	3.096240186
964	3.094838828
965	3.094176802
966	3.093182279
967	3.090150687
968	3.088612829
969	3.087153684
970	3.085866843
971	3.082669094
972	3.081047548
973	3.079684322
974	3.07613431
975	3.074005935
976	3.072988853
977	3.072344268
978	3.070654438
979	3.069724506
980	3.067763166
981	3.066737893
982	3.065248174
983	3.063186837
984	3.062880371
985	3.061288645
986	3.059970575
987	3.059576341
988	3.058175919
989	3.056830702
990	3.056336579
991	3.055810414
992	3.055476518
993	3.053331734
994	3.050890374
995	3.04972623
996	3.047217327
997	3.047155483
998	3.04430671
999	3.042664195


================================================
File: notebook/catboost_info/time_left.tsv
================================================
iter	Passed	Remaining
0	144	143938
1	144	72343
2	145	48459
3	146	36507
4	147	29372
5	149	24694
6	149	21278
7	150	18700
8	151	16689
9	152	15085
10	153	13768
11	154	12690
12	155	11790
13	156	10997
14	156	10303
15	157	9708
16	158	9178
17	159	8702
18	160	8273
19	161	7891
20	161	7548
21	163	7248
22	164	6967
23	164	6708
24	165	6464
25	166	6235
26	167	6025
27	168	5833
28	168	5657
29	169	5493
30	170	5333
31	171	5187
32	172	5048
33	173	4918
34	173	4794
35	174	4679
36	175	4567
37	176	4462
38	177	4367
39	178	4278
40	179	4190
41	179	4103
42	180	4021
43	181	3943
44	182	3867
45	183	3795
46	184	3736
47	185	3677
48	186	3615
49	187	3556
50	187	3496
51	188	3439
52	189	3387
53	190	3337
54	191	3288
55	192	3243
56	193	3202
57	194	3157
58	195	3112
59	196	3070
60	196	3029
61	197	2989
62	198	2953
63	199	2916
64	200	2882
65	201	2846
66	201	2812
67	202	2778
68	203	2746
69	204	2714
70	205	2682
71	205	2654
72	206	2628
73	207	2601
74	208	2576
75	209	2550
76	210	2523
77	211	2498
78	212	2475
79	213	2451
80	214	2430
81	215	2407
82	215	2380
83	216	2359
84	217	2337
85	217	2315
86	218	2295
87	219	2275
88	220	2256
89	221	2236
90	222	2220
91	223	2203
92	224	2186
93	224	2168
94	225	2151
95	226	2134
96	227	2118
97	228	2102
98	229	2086
99	230	2070
100	230	2055
101	231	2039
102	232	2025
103	233	2010
104	234	1996
105	235	1982
106	235	1968
107	236	1955
108	237	1943
109	238	1930
110	239	1918
111	240	1905
112	241	1892
113	241	1879
114	242	1867
115	243	1855
116	244	1841
117	244	1829
118	245	1818
119	246	1806
120	247	1794
121	247	1782
122	248	1771
123	249	1760
124	249	1749
125	250	1738
126	251	1728
127	252	1720
128	253	1713
129	254	1702
130	255	1692
131	255	1682
132	256	1673
133	257	1663
134	258	1653
135	258	1644
136	259	1635
137	260	1625
138	260	1616
139	261	1607
140	262	1598
141	262	1588
142	263	1579
143	264	1571
144	265	1562
145	265	1554
146	266	1548
147	267	1542
148	268	1535
149	269	1527
150	270	1520
151	271	1512
152	271	1505
153	272	1497
154	273	1490
155	274	1483
156	274	1476
157	275	1468
158	276	1461
159	277	1454
160	277	1446
161	278	1439
162	279	1432
163	279	1426
164	280	1419
165	281	1413
166	282	1408
167	283	1402
168	283	1396
169	284	1390
170	285	1384
171	286	1377
172	286	1371
173	287	1366
174	288	1361
175	289	1355
176	290	1349
177	291	1344
178	291	1338
179	292	1333
180	293	1327
181	294	1322
182	294	1316
183	295	1312
184	296	1307
185	297	1303
186	298	1299
187	299	1293
188	300	1288
189	301	1283
190	301	1279
191	302	1274
192	303	1270
193	304	1264
194	305	1260
195	305	1255
196	306	1250
197	307	1245
198	308	1240
199	308	1235
200	309	1231
201	310	1226
202	311	1222
203	312	1219
204	313	1215
205	314	1211
206	315	1206
207	315	1202
208	316	1198
209	317	1193
210	318	1189
211	318	1185
212	319	1180
213	320	1176
214	321	1172
215	321	1168
216	322	1164
217	323	1159
218	324	1155
219	324	1151
220	325	1147
221	326	1144
222	327	1141
223	328	1137
224	329	1133
225	329	1129
226	330	1126
227	331	1122
228	332	1118
229	333	1114
230	333	1111
231	334	1107
232	335	1104
233	336	1100
234	336	1096
235	337	1093
236	338	1090
237	339	1086
238	340	1083
239	341	1079
240	342	1077
241	343	1075
242	344	1072
243	345	1069
244	345	1065
245	346	1062
246	347	1058
247	348	1055
248	348	1051
249	349	1048
250	350	1045
251	351	1042
252	351	1039
253	352	1035
254	353	1032
255	354	1029
256	354	1026
257	355	1023
258	357	1021
259	357	1018
260	358	1015
261	359	1012
262	360	1009
263	361	1006
264	361	1003
265	362	1000
266	363	997
267	363	993
268	364	990
269	365	988
270	366	985
271	367	982
272	367	979
273	368	976
274	369	973
275	370	970
276	371	968
277	372	966
278	372	963
279	373	960
280	374	958
281	375	955
282	376	952
283	376	950
284	377	947
285	378	945
286	379	942
287	380	941
288	381	938
289	382	936
290	383	933
291	383	930
292	384	928
293	385	925
294	386	923
295	387	921
296	388	919
297	389	916
298	390	914
299	390	911
300	391	909
301	392	906
302	393	904
303	394	902
304	394	899
305	395	897
306	396	894
307	397	892
308	398	890
309	398	887
310	399	885
311	400	883
312	401	881
313	402	879
314	403	877
315	404	874
316	404	872
317	405	870
318	406	867
319	407	865
320	408	863
321	408	860
322	409	858
323	410	855
324	410	853
325	411	851
326	412	848
327	413	846
328	413	844
329	414	841
330	415	839
331	416	838
332	417	836
333	418	834
334	419	832
335	419	829
336	420	827
337	421	825
338	422	823
339	423	821
340	423	819
341	424	817
342	425	814
343	426	812
344	426	810
345	427	808
346	428	806
347	429	803
348	429	801
349	430	799
350	431	798
351	432	796
352	433	794
353	434	792
354	434	790
355	435	788
356	436	785
357	437	783
358	437	781
359	438	779
360	439	777
361	440	775
362	440	773
363	441	771
364	442	769
365	443	767
366	443	765
367	444	763
368	445	761
369	446	760
370	447	758
371	448	756
372	448	754
373	449	752
374	450	750
375	451	748
376	451	746
377	452	744
378	453	742
379	454	741
380	454	739
381	455	737
382	456	735
383	457	733
384	457	731
385	458	729
386	459	727
387	460	726
388	462	726
389	463	725
390	464	723
391	465	721
392	466	719
393	467	718
394	467	716
395	468	714
396	469	712
397	470	711
398	471	709
399	471	707
400	472	705
401	473	704
402	474	702
403	474	700
404	475	699
405	476	697
406	477	695
407	478	694
408	479	692
409	479	690
410	480	688
411	481	687
412	482	685
413	483	683
414	483	682
415	484	680
416	485	678
417	486	677
418	487	675
419	488	673
420	488	672
421	489	670
422	490	669
423	491	667
424	492	666
425	493	664
426	493	662
427	494	660
428	495	659
429	496	657
430	497	656
431	497	654
432	498	653
433	499	651
434	500	649
435	501	648
436	501	646
437	502	644
438	503	643
439	504	641
440	504	640
441	506	639
442	507	637
443	507	636
444	508	634
445	509	632
446	510	631
447	511	629
448	511	628
449	512	626
450	513	625
451	514	623
452	515	622
453	515	620
454	516	618
455	517	617
456	518	615
457	518	614
458	519	612
459	520	611
460	521	609
461	522	608
462	523	606
463	523	605
464	524	603
465	525	602
466	526	600
467	527	599
468	527	597
469	528	596
470	529	594
471	530	593
472	531	591
473	531	590
474	532	588
475	533	587
476	534	585
477	534	584
478	535	582
479	536	581
480	537	579
481	538	578
482	538	576
483	539	575
484	540	573
485	541	572
486	542	571
487	543	570
488	544	568
489	545	567
490	545	565
491	546	564
492	547	562
493	548	561
494	548	559
495	549	558
496	550	556
497	551	555
498	552	554
499	552	552
500	553	551
501	554	550
502	555	548
503	556	547
504	556	545
505	557	544
506	558	543
507	559	541
508	560	540
509	560	538
510	561	537
511	562	535
512	563	534
513	563	533
514	564	531
515	565	530
516	566	529
517	567	527
518	568	526
519	568	525
520	569	523
521	570	522
522	571	520
523	572	519
524	573	518
525	574	517
526	574	515
527	575	514
528	576	513
529	577	511
530	577	510
531	578	508
532	579	507
533	580	506
534	581	505
535	582	504
536	583	502
537	583	501
538	584	500
539	585	498
540	586	497
541	586	495
542	587	494
543	588	493
544	589	492
545	590	490
546	590	489
547	591	488
548	592	486
549	593	485
550	594	484
551	594	482
552	595	481
553	596	480
554	597	479
555	598	477
556	599	476
557	600	475
558	600	474
559	601	472
560	602	471
561	603	470
562	604	468
563	604	467
564	605	466
565	606	464
566	607	463
567	607	462
568	608	461
569	609	459
570	610	458
571	611	457
572	612	456
573	613	455
574	613	453
575	614	452
576	615	451
577	616	449
578	616	448
579	617	447
580	618	446
581	619	444
582	620	443
583	620	442
584	621	440
585	622	439
586	622	438
587	623	437
588	624	435
589	625	434
590	626	433
591	627	432
592	628	431
593	628	429
594	629	428
595	630	427
596	630	425
597	631	424
598	632	423
599	633	422
600	634	420
601	634	419
602	635	418
603	636	417
604	637	415
605	637	414
606	638	413
607	639	412
608	640	411
609	641	409
610	642	408
611	642	407
612	643	406
613	644	405
614	645	403
615	645	402
616	646	401
617	647	400
618	648	398
619	648	397
620	649	396
621	650	395
622	651	393
623	651	392
624	652	391
625	653	390
626	653	389
627	654	387
628	655	386
629	656	385
630	657	384
631	658	383
632	658	382
633	659	380
634	660	379
635	661	378
636	662	377
637	663	376
638	663	375
639	664	373
640	665	372
641	666	371
642	666	370
643	667	369
644	668	367
645	669	366
646	669	365
647	670	364
648	671	363
649	672	362
650	673	360
651	674	359
652	674	358
653	675	357
654	676	356
655	677	355
656	677	353
657	678	352
658	679	351
659	680	350
660	680	349
661	681	347
662	682	346
663	683	345
664	683	344
665	684	343
666	686	342
667	686	341
668	687	340
669	688	339
670	689	337
671	689	336
672	690	335
673	691	334
674	692	333
675	692	332
676	693	330
677	694	329
678	695	328
679	695	327
680	696	326
681	697	325
682	698	324
683	699	322
684	700	322
685	701	321
686	702	319
687	702	318
688	703	317
689	704	316
690	705	315
691	705	314
692	706	313
693	707	311
694	708	310
695	709	309
696	709	308
697	710	307
698	711	306
699	712	305
700	712	304
701	713	302
702	715	302
703	715	300
704	716	299
705	717	298
706	718	297
707	719	296
708	719	295
709	720	294
710	721	293
711	722	292
712	723	291
713	723	289
714	724	288
715	725	287
716	726	286
717	726	285
718	727	284
719	728	283
720	729	282
721	730	281
722	731	280
723	732	279
724	732	277
725	733	276
726	734	275
727	735	274
728	736	273
729	737	272
730	738	271
731	738	270
732	739	269
733	740	268
734	741	267
735	742	266
736	742	265
737	743	264
738	744	263
739	746	262
740	746	261
741	747	260
742	748	259
743	749	257
744	750	256
745	751	255
746	752	254
747	753	253
748	753	252
749	754	251
750	755	250
751	756	249
752	756	248
753	757	247
754	758	246
755	759	245
756	760	244
757	761	243
758	762	241
759	762	240
760	763	239
761	764	238
762	765	237
763	765	236
764	766	235
765	767	234
766	768	233
767	769	232
768	769	231
769	770	230
770	771	229
771	772	228
772	773	227
773	774	226
774	775	225
775	775	223
776	776	222
777	777	221
778	778	220
779	779	219
780	780	218
781	780	217
782	781	216
783	782	215
784	783	214
785	784	213
786	784	212
787	785	211
788	786	210
789	787	209
790	788	208
791	789	207
792	790	206
793	790	205
794	791	204
795	792	203
796	793	202
797	794	200
798	794	199
799	795	198
800	796	197
801	797	196
802	797	195
803	798	194
804	799	193
805	800	192
806	801	191
807	801	190
808	802	189
809	803	188
810	804	187
811	805	186
812	806	185
813	807	184
814	807	183
815	808	182
816	809	181
817	810	180
818	810	179
819	811	178
820	812	177
821	813	176
822	814	175
823	815	174
824	816	173
825	817	172
826	817	171
827	818	170
828	819	169
829	820	168
830	821	167
831	822	166
832	823	164
833	823	163
834	824	162
835	825	161
836	826	160
837	826	159
838	827	158
839	828	157
840	828	156
841	829	155
842	830	154
843	831	153
844	832	152
845	832	151
846	833	150
847	834	149
848	835	148
849	836	147
850	837	146
851	837	145
852	838	144
853	839	143
854	840	142
855	841	141
856	842	140
857	842	139
858	843	138
859	844	137
860	845	136
861	846	135
862	846	134
863	847	133
864	848	132
865	849	131
866	850	130
867	851	129
868	852	128
869	852	127
870	853	126
871	854	125
872	855	124
873	855	123
874	856	122
875	857	121
876	858	120
877	858	119
878	859	118
879	860	117
880	860	116
881	861	115
882	862	114
883	863	113
884	864	112
885	865	111
886	866	110
887	866	109
888	867	108
889	868	107
890	869	106
891	870	105
892	870	104
893	871	103
894	872	102
895	873	101
896	874	100
897	874	99
898	875	98
899	876	97
900	877	96
901	877	95
902	878	94
903	879	93
904	880	92
905	881	91
906	882	90
907	882	89
908	883	88
909	884	87
910	885	86
911	885	85
912	886	84
913	887	83
914	888	82
915	889	81
916	889	80
917	890	79
918	891	78
919	892	77
920	892	76
921	893	75
922	894	74
923	895	73
924	896	72
925	896	71
926	897	70
927	898	69
928	899	68
929	900	67
930	901	66
931	901	65
932	902	64
933	903	63
934	904	62
935	904	61
936	905	60
937	906	59
938	907	58
939	907	57
940	909	57
941	910	56
942	911	55
943	911	54
944	912	53
945	913	52
946	914	51
947	915	50
948	915	49
949	916	48
950	917	47
951	918	46
952	918	45
953	919	44
954	920	43
955	921	42
956	921	41
957	922	40
958	924	39
959	925	38
960	925	37
961	926	36
962	927	35
963	928	34
964	929	33
965	930	32
966	930	31
967	931	30
968	932	29
969	933	28
970	934	27
971	934	26
972	935	25
973	936	24
974	937	24
975	937	23
976	938	22
977	939	21
978	940	20
979	941	19
980	942	18
981	943	17
982	943	16
983	945	15
984	946	14
985	946	13
986	947	12
987	948	11
988	949	10
989	950	9
990	950	8
991	951	7
992	952	6
993	953	5
994	954	4
995	955	3
996	956	2
997	957	1
998	957	0
999	958	0


================================================
File: src/exception.py
================================================
import sys
from src.logger import logging

def error_message_detail(error,error_detail:sys):  ## error_detail will be present inside sys library
    _,_,exc_tb=error_detail.exc_info()  ##Talking about execution info and gives 3 important information and we are intrested in 3rd information
    file_name=exc_tb.tb_frame.f_code.co_filename ## This will fetch the required filename
    error_message='Error occured in python script name [{0}] line number [{1}] error message[{2}]'.format(
     file_name,exc_tb.tb_lineno,str(error)) ##exc_tb.tb_lineno --This will fetch the exact line number where exception has occured
    #str(error)) is the actual error which we provide here.

    return error_message

    

class CustomException(Exception): ##Inherting the parent Exception
    def __init__(self,error_message,error_detail:sys): ## Initialzing the constructor.
        super().__init__(error_message)
        self.error_message=error_message_detail(error_message,error_detail=error_detail) ## error_message will be getting from our function
                                               ##error_detail will be tracked by sys library
    
    def __str__(self):
        return self.error_message ## For printing purpose


if __name__ == '__main__':
    try:
        a=1/0
    except Exception as e:
        logging.info('Division by zero')
        raise CustomException(e,sys)       

================================================
File: src/logger.py
================================================
import logging
import os
from datetime import datetime

LOG_FILE=f'{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log' ## Ensures how the log file strucure has to be
logs_path=os.path.join(os.getcwd(),'logs',LOG_FILE) ## Creating folder and naming convetion of folder
os.makedirs(logs_path,exist_ok=True) ## exist_ok=True means eventhough there is a file and folder it will keep on appending the files inside

LOG_FILE_PATH=os.path.join(logs_path,LOG_FILE) ## Logs files path inside the Logs folder

logging.basicConfig(
    filename=LOG_FILE_PATH, ## Where you want to basically save it
    format='[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO,


)




================================================
File: src/utils.py
================================================
import os
import sys

import numpy as np 
import pandas as pd
import dill
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV
from src.exception import CustomException

## Save the pickle file
def save_object(file_path, obj):
    try:
        dir_path = os.path.dirname(file_path)

        os.makedirs(dir_path, exist_ok=True)

        with open(file_path, 'wb') as file_obj:
            dill.dump(obj, file_obj)

    except Exception as e:
        raise CustomException(e, sys)
    
##Load the pickle file
def load_object(file_path):
    try:
        with open(file_path, 'rb') as file_obj:
            return dill.load(file_obj)

    except Exception as e:
        raise CustomException(e, sys)     
    


def evaluate_models(X_train, y_train,X_test,y_test,models,param):
    try:
        report = {}

        for i in range(len(list(models))):
            model = list(models.values())[i]  ##Inside the loop, the code extracts the current machine learning model by using the list() function to get the i-th element of the list of models and assigns it to the model variable. 
            #The list() function is used here to convert the dictionary values to a list so that we can access them by index.

            #model.fit(X_train, y_train)  # Train model
            para=param[list(models.keys())[i]]

            gs = GridSearchCV(model,para,cv=3)
            gs.fit(X_train,y_train)

            model.set_params(**gs.best_params_)
            '''
            In the given code snippet, ** is used to unpack a dictionary. Specifically, **gs.best_params_ is used to unpack the dictionary gs.best_params_ and pass its contents as keyword arguments to the set_params method of the model object.
            The best_params_ attribute of the GridSearchCV object gs returns a dictionary of the best hyperparameters found during the grid search. The double asterisks (**) are used to unpack this dictionary into a series of keyword arguments, which can then be passed to the set_params method of the model object.
            By using ** to unpack the dictionary, each key-value pair in the dictionary becomes a separate keyword argument that is passed to the method. This allows you to dynamically set the hyperparameters of the model using the best parameters found during the grid search.   
            '''
            model.fit(X_train,y_train)

            y_train_pred = model.predict(X_train)

            y_test_pred = model.predict(X_test)

            train_model_score = r2_score(y_train, y_train_pred)

            test_model_score = r2_score(y_test, y_test_pred)

            report[list(models.keys())[i]] = test_model_score
            '''
            This code snippet creates an empty dictionary called report. It then adds a new key-value pair to the dictionary where the key is the ith key in a dictionary called models (which is not shown in the code snippet), and the value is test_model_score.
            To break it down further:
            models.keys() returns a list of all the keys in the models dictionary.
            list(models.keys())[i] gets the ith key in that list. The list() function is used to convert the keys from a dictionary view object to a list so that we can access them by index.
            test_model_score is a variable that presumably contains some kind of test score for the model.
            So, the overall effect of this code is to create a dictionary called report with keys corresponding to the names of the models in the models dictionary, and values corresponding to their test scores. The i variable determines which key in models to use as the key in report.
            '''

        return report

    except Exception as e:
        raise CustomException(e, sys)
    


    '''
    # Define a dictionary
    params = {'param1': 10, 'param2': 20, 'param3': 30}

    # Define a function that takes three parameters
    def my_func(param1, param2, param3):
        print(f'param1={param1}, param2={param2}, param3={param3}')

    # Call the function using the dictionary as keyword arguments
    my_func(**params)

    In this example, the params dictionary contains three key-value pairs that correspond to the three parameters expected by the my_func function. By using the double asterisk (**) operator to unpack the dictionary, we can pass its contents as keyword arguments to the function.

    When we call my_func(**params), the dictionary contents are unpacked and passed as separate keyword arguments to the function. The output of the function call will be:



    o/p is
    param1=10, param2=20, param3=30

    This demonstrates how the double asterisk (**) operator can be used to unpack a dictionary and pass its contents as keyword arguments to a function.
    '''
   

================================================
File: src/components/data_ingestion.py
================================================
import os
import sys
from src.exception import CustomException
from src.logger import logging
import pandas as pd

from sklearn.model_selection import train_test_split
from dataclasses import dataclass

from src.components.data_transformation import DataTransformation
from src.components.data_transformation import DataTransformationConfig


from src.components.model_trainer import ModelTrainerConfig
from src.components.model_trainer import ModelTrainer

@dataclass ## Decorator  ## Decators can be used for defining a variable inside a class
class DataIngestionConfig:
    train_data_path: str=os.path.join('artifacts','train.csv') ## str= is string type and the path where the files are getting stored and artifacts is a folder
    test_data_path: str=os.path.join('artifacts','test.csv')
    raw_data_path: str=os.path.join('artifacts','data.csv')

class DataIngestion:
    def __init__(self):
        self.ingestion_config=DataIngestionConfig()

    def initiate_data_ingestion(self):
        logging.info('Enter the data ingestion method or component')
        try:
            df=pd.read_csv('notebook\data\stud.csv')
            logging.info('Read the dataset as dataframe')

            os.makedirs(os.path.dirname(self.ingestion_config.raw_data_path),exist_ok=True) ## os.path.dirname  is basically getting directory name with respect to specific path
            ##exist_ok =True is if the directory is already there we can keep the particular folder and we dont want to delete or create it again and again
            df.to_csv(self.ingestion_config.raw_data_path,index=False,header=True) ## saving raw data

            logging.info('Train test split initiated')
            train_set,test_set=train_test_split(df,test_size=0.2,random_state=42) ##train test split initiated

            train_set.to_csv(self.ingestion_config.train_data_path,index=False,header=True) ##saving train_csv

            test_set.to_csv(self.ingestion_config.test_data_path,index=False,header=True) ##saving test_csv

            logging.info('Ingestion of the data is completed')

            return(
                self.ingestion_config.train_data_path,
                self.ingestion_config.test_data_path

            )
        except Exception as e:
            raise CustomException(e,sys)
        
if __name__=='__main__':
    obj=DataIngestion()
    train_data,test_data=obj.initiate_data_ingestion()

    data_transformation=DataTransformation()
    train_arr,test_arr,_= data_transformation.initiate_data_transformation(train_data,test_data)
    modeltrainer=ModelTrainer()
    print(modeltrainer.initiate_model_trainer(train_arr,test_arr))

================================================
File: src/components/data_transformation.py
================================================
import sys
from dataclasses import dataclass

import numpy as np 
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer ##for missing values
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder,StandardScaler

from src.exception import CustomException
from src.logger import logging
import os

from src.utils import save_object

@dataclass
class DataTransformationConfig:
    preprocessor_obj_file_path=os.path.join('artifacts','proprocessor.pkl')

class DataTransformation:
    def __init__(self):
        self.data_transformation_config=DataTransformationConfig()

    def get_data_transformer_object(self):
        '''
        This function is responsible for data transformation
        
        '''
        try:
            numerical_columns = ['writing_score', 'reading_score']
            categorical_columns = [
                'gender',
                'race_ethnicity',
                'parental_level_of_education',
                'lunch',
                'test_preparation_course',
            ]

            num_pipeline= Pipeline(
                steps=[
                ('imputer',SimpleImputer(strategy='median')), ##Handling missing values
                ('scaler',StandardScaler())

                ]
            )

            cat_pipeline=Pipeline(

                steps=[
                ('imputer',SimpleImputer(strategy='most_frequent')),##most_frequent is MODE.
                ('one_hot_encoder',OneHotEncoder()), ##converting categorical to numerical orlse we can use target guided encoder also. If less number of categories are involved then go with one hot encoder
                ('scaler',StandardScaler(with_mean=False)) ##Not neccessary for categorical part.
                ]

            )

            logging.info(f'Categorical columns: {categorical_columns}')
            logging.info(f'Numerical columns: {numerical_columns}')

            preprocessor=ColumnTransformer(
                [
                ('num_pipeline',num_pipeline,numerical_columns),
                ('cat_pipelines',cat_pipeline,categorical_columns)

                ]


            )

            return preprocessor
        
        except Exception as e:
            raise CustomException(e,sys)
        
    def initiate_data_transformation(self,train_path,test_path): ##train_path and test_path are coming from data_ingestion.py

        try:
            train_df=pd.read_csv(train_path)
            test_df=pd.read_csv(test_path)

            logging.info('Read train and test data completed')

            logging.info('Obtaining preprocessing object')

            preprocessing_obj=self.get_data_transformer_object()

            target_column_name='math_score'
            numerical_columns = ['writing_score', 'reading_score']

            input_feature_train_df=train_df.drop(columns=[target_column_name],axis=1)
            target_feature_train_df=train_df[target_column_name]

            input_feature_test_df=test_df.drop(columns=[target_column_name],axis=1)
            target_feature_test_df=test_df[target_column_name]

            logging.info(
                f'Applying preprocessing object on training dataframe and testing dataframe.'
            )

            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)
            input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df)

            train_arr = np.c_[
                input_feature_train_arr, np.array(target_feature_train_df)
            ]
            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)] ##np.c means numpy in column wise concatenation
            #np.c_[] is a numpy function that is used to concatenate arrays along the second axis (horizontally)
            logging.info(f'Saved preprocessing object.')
            
            ##save_object is a function call and is coming from utils.py
            save_object(

                file_path=self.data_transformation_config.preprocessor_obj_file_path, ##path to save the pkl file
                obj=preprocessing_obj ##This is the object with the model that has all transformations.In short we are saving this pickle name in hard disk.

            )

            return (
                train_arr,
                test_arr,
                self.data_transformation_config.preprocessor_obj_file_path,
            )
        except Exception as e:
            raise CustomException(e,sys)
        

'''
The os.path.dirname() function and the os.makedirs() function are both used for creating directories, but they serve different purposes.

os.path.dirname() function returns the directory part of a file path. For example, if the file_path is /home/user/filename.txt, the os.path.dirname(file_path) call will return /home/user. Essentially, this function extracts the directory component from the given file path.

On the other hand, os.makedirs() function creates a directory and all its parent directories if they do not already exist. The exist_ok=True argument ensures that the function will not raise an error if the directory already exists.

In the code snippet you provided, dir_path = os.path.dirname(file_path) extracts the directory component from the given file_path and saves it to the dir_path variable. Then, os.makedirs(dir_path, exist_ok=True) creates the directory and its parent directories if they don't already exist.

Overall, os.path.dirname() and os.makedirs() are used together in this code to ensure that the directory where the pickle file will be saved exists before attempting to write the file to disk.
'''        

================================================
File: src/components/model_trainer.py
================================================
import os
import sys
from dataclasses import dataclass

from catboost import CatBoostRegressor
from sklearn.ensemble import (
    AdaBoostRegressor,
    GradientBoostingRegressor,
    RandomForestRegressor,
)
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

from src.exception import CustomException
from src.logger import logging

from src.utils import save_object,evaluate_models

@dataclass
class ModelTrainerConfig:
    trained_model_file_path=os.path.join('artifacts','model.pkl')

class ModelTrainer:
    def __init__(self):
        self.model_trainer_config=ModelTrainerConfig()


    def initiate_model_trainer(self,train_array,test_array):
        try:
            logging.info('Split training and test input data')
            X_train,y_train,X_test,y_test=(
                train_array[:,:-1],
                train_array[:,-1],
                test_array[:,:-1],
                test_array[:,-1]
            )
            models = {
                'Random Forest': RandomForestRegressor(),
                'Decision Tree': DecisionTreeRegressor(),
                'Gradient Boosting': GradientBoostingRegressor(),
                'Linear Regression': LinearRegression(),
                'K-Neighbors Classifier': KNeighborsRegressor(),
                'XGBClassifier': XGBRegressor(),
                'CatBoosting Classifier': CatBoostRegressor(verbose=False),
                'AdaBoost Classifier': AdaBoostRegressor(),
            }
            

            params={
                'Decision Tree': {
                    'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],
                    # 'splitter':['best','random'],
                    # 'max_features':['sqrt','log2'],
                },
                'Random Forest':{
                    # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],
                 
                    # 'max_features':['sqrt','log2',None],
                    'n_estimators': [8,16,32,64,128,256]
                },
                'Gradient Boosting':{
                    # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'],
                    'learning_rate':[.1,.01,.05,.001],
                    'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],
                    # 'criterion':['squared_error', 'friedman_mse'],
                    # 'max_features':['auto','sqrt','log2'],
                    'n_estimators': [8,16,32,64,128,256]
                },
                'Linear Regression':{},
                'XGBRegressor':{
                    'learning_rate':[.1,.01,.05,.001],
                    'n_estimators': [8,16,32,64,128,256]
                },
                'CatBoosting Regressor':{
                    'depth': [6,8,10],
                    'learning_rate': [0.01, 0.05, 0.1],
                    'iterations': [30, 50, 100]
                },
                'AdaBoost Regressor':{
                    'learning_rate':[.1,.01,0.5,.001],
                    # 'loss':['linear','square','exponential'],
                    'n_estimators': [8,16,32,64,128,256]
                }
                
            }

            model_report:dict=evaluate_models(X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,
                                             models=models,param=params)
            
            ## To get best model score from dict
            best_model_score = max(sorted(model_report.values()))

            ## To get best model name from dict

            best_model_name = list(model_report.keys())[list(model_report.values()).index(best_model_score)]
            '''
            list(model_report.keys()) creates a list of all the model names in model_report.
            list(model_report.values()) creates a list of all the test scores in model_report.
            list(model_report.values()).index(best_model_score) finds the index of best_model_score in the list of test scores. For example, if best_model_score is 0.81, this would return the index 1 (since 0.81 is the second element in the list).
            list(model_report.keys())[index] selects the model name corresponding to the index found in the previous step. For example, if best_model_score is 0.81 and 'Model B' has index 1 in the list of test scores, this would return 'Model B'.
            '''
            best_model = models[best_model_name] ## Returns the best model object key and this line is used for making pickle file and is passed as an arguement in save_object.
            '''
            This means that 'Model A' is a LinearRegression object, 'Model B' is a RandomForestRegressor object, and 'Model C' is an XGBRegressor object.
            #The second line of code selects the best-performing model object by looking up its name in the models dictionary.
            '''

            if best_model_score<0.6:
                raise CustomException('No best model found')
            logging.info(f'Best found model on both training and testing dataset')

            save_object(
                file_path=self.model_trainer_config.trained_model_file_path,
                obj=best_model
            )

            predicted=best_model.predict(X_test)

            r2_square = r2_score(y_test, predicted)
            return r2_square
            



            
        except Exception as e:
            raise CustomException(e,sys)

================================================
File: src/pipeline/predict_pipeline.py
================================================
import sys
import pandas as pd
from src.exception import CustomException
from src.utils import load_object ##Load our pickle file from utils.py


class PredictPipeline:
    def __init__(self):  ##By deafult empty constructor
        pass

    def predict(self,features):
        try:
            model_path='artifacts\model.pkl'
            preprocessor_path='artifacts\preprocessor.pkl' ##Responsible  for handling categorical and numerical features
            model=load_object(file_path=model_path)
            preprocessor=load_object(file_path=preprocessor_path)
            data_scaled=preprocessor.transform(features) ## feature Scaling
            preds=model.predict(data_scaled)  ## Model Prediction
            return preds
        
        except Exception as e:
            raise CustomException(e,sys)


##Custom class is responsible for mapping our values from html page to backend in order to process prediciton
class CustomData:
    def __init__(self,
        gender: str,
        race_ethnicity: str,
        parental_level_of_education,
        lunch: str,
        test_preparation_course: str,
        reading_score: int,
        writing_score: int):

        self.gender = gender

        self.race_ethnicity = race_ethnicity

        self.parental_level_of_education = parental_level_of_education

        self.lunch = lunch

        self.test_preparation_course = test_preparation_course

        self.reading_score = reading_score

        self.writing_score = writing_score

## get_data_as_data_frame will return in a dataframe
    def get_data_as_data_frame(self):
        try:
            custom_data_input_dict = {
                'gender': [self.gender],
                'race_ethnicity': [self.race_ethnicity],
                'parental_level_of_education': [self.parental_level_of_education],
                'lunch': [self.lunch],
                'test_preparation_course': [self.test_preparation_course],
                'reading_score': [self.reading_score],
                'writing_score': [self.writing_score],
            }

            return pd.DataFrame(custom_data_input_dict)

        except Exception as e:
            raise CustomException(e, sys)

================================================
File: templates/home.html
================================================
<html>
<body>
    <div class='login'>
       <h1>Student Exam Performance Indicator</h1>
   
       <form action='{{ url_for('predict_datapoint')}}' method='post'>  ##Here we need to call same function which was defined inside /predictdata route 
        <h1>
            <legend>Predicting Math score of a student</legend>
        </h1>
        <div class='mb-3'>
            <label class='form-label'>Gender</label>
            <select class='form-control' name='gender' placeholder='Enter you Gender' required>
                <option class='placeholder' selected disabled value=''>Select your Gender</option>
                <option value='male'>
                    Male
                </option>
                <option value='female'>
                    Female
                </option>
            </select>
        </div>
        <div class='mb-3'>
            <label class='form-label'>Race or Ethnicity</label>
            <select class='form-control' name='ethnicity' placeholder='Enter you ethnicity' required>
                <option class='placeholder' selected disabled value=''>Select Ethnicity</option>
                <option value='group A'>
                    Group A
                </option>
                <option value='group B'>
                    Group B
                </option>
                <option value='group C'>
                    Group C
                </option>
                <option value='group D'>
                    Group D
                </option>
                <option value='group E'>
                    Group E
                </option>
            </select>
        </div>
        <div class='mb-3'>
            <label class='form-label'>Parental Level of Education</label>
            <select class='form-control' name='parental_level_of_education'
                placeholder='Enter you Parent Education' required>
                <option class='placeholder' selected disabled value=''>Select Parent Education</option>
                <option value='associate's degree'>
                    associate's degree
                </option>
                <option value='bachelor's degree'>
                    bachelor's degree
                </option>
                <option value='high school'>
                    high school
                </option>
                <option value='master's degree'>
                    master's degree
                </option>
                <option value='some college'>
                    some college
                </option>
                <option value='some high school'>
                    some high school
                </option>
            </select>
        </div>
        <div class='mb-3'>
            <label class='form-label'>Lunch Type</label>
            <select class='form-control' name='lunch' placeholder='Enter you Lunch' required>
                <option class='placeholder' selected disabled value=''>Select Lunch Type</option>
                <option value='free/reduced'>
                    free/reduced
                </option>
                <option value='standard'>
                    standard
                </option>
            </select>
        </div>
        <div class='mb-3'>
            <label class='form-label'>Test preparation Course</label>
            <select class='form-control' name='test_preparation_course' placeholder='Enter you Course'
                required>
                <option class='placeholder' selected disabled value=''>Select Test_course</option>
                <option value='none'>
                    None
                </option>
                <option value='completed'>
                    Completed
                </option>
            </select>
        </div>
        <div class='mb-3'>
            <label class='form-label'>Writing Score out of 100</label>
            <input class='form-control' type='number' name='reading_score'
                placeholder='Enter your Reading score' min='0' max='100' />
        </div>
        <div class='mb-3'>
            <label class='form-label'>Reading Score out of 100</label>
            <input class='form-control' type='number' name='writing_score'
                placeholder='Enter your Reading Score' min='0' max='100' />
        </div>
        <div class='mb-3'>
            <input class='btn btn-primary' type='submit' value='Predict your Maths Score' required />
        </div>
    </form>
    <h2>  
       The prediction is {{results}}
    </h2>
   <body>
</html>

================================================
File: templates/index.html
================================================
<h1>Welcome to the home page</h1>

================================================
File: .ebextensions/python.config
================================================
option_settings:
  'aws:elasticbeanstalk:container:python':
    WSGIPath: application:application


        """,
        "repo_created_at": "2025-02-12T13:09:20Z",
        "last_updated": "2025-02-12T13:09:30Z",
        "stars": 0
    },
    "Multimodal-Emotion-Recognition-using-Speech-Cues-and-Facial-Expressions": {
        "github_url": "https://github.com/Parthiban-3997/Multimodal-Emotion-Recognition-using-Speech-Cues-and-Facial-Expressions",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Multimodal-Emotion-Recognition-using-Speech-Cues-and-Facial-Expressions",
        "description": """
        ================================================
File: README.md
================================================
# Multimodal Emotion Recognition System

Recognizing and understanding emotions is crucial in human-computer interaction, as it can greatly enhance decision-making and judgment. This project proposes a comprehensive emotion recognition system that focuses on analyzing candidates' expressions during behavioral interviews.

## Table of Contents
- [Introduction](#introduction)
- [Features](#features)
- [Usage](#usage)
- [Technologies Used](#technologies-used)


## Introduction
The Multimodal Emotion Recognition System aims to enhance the hiring process by analyzing candidates' emotions during behavioral interviews. It utilizes both textual data and facial images captured during interviews to analyze underlying sentiments. The system incorporates advanced deep learning techniques such as Bi-directional Long Short-Term Memory (Bi-LSTM) for textual content and Convolutional Neural Networks (CNNs) for facial expressions.

## Features
- Analyzes facial expressions and textual data from speech cues for emotion recognition.
- Utilizes Bi-LSTM and CNNs for accurate emotion analysis.
- Provides real-time analysis to interviewers through a user-friendly web interface.


## Usage
- Access the web interface by opening the provided URL. (multimodal-emotion-detection.azurewebsites.net)
- Capture candidate facial images and speech cues.
- The system will analyze emotions and provide real-time insights.



## Technologies Used
- Streamlit
- Docker
- CI/CD Pipeline
- Microsoft Azure


  ## Application can be accessed via
- https://multimodal-emotion-detection.azurewebsites.net/


================================================
File: DockerFile
================================================
FROM python:3.8
COPY . /app
WORKDIR /app
RUN apt-get update && apt-get install -y --no-install-recommends \
    libsm6 \
    libxext6 \
    ffmpeg \
    libgl1-mesa-glx \
    portaudio19-dev \
    gcc 
RUN apt-get install -y libasound2-dev libpulse-dev
RUN apt-get install -y portaudio19-dev && pip install pyaudio
RUN pip install -r requirements.txt
EXPOSE 80
ENTRYPOINT [ 'streamlit', 'run' ]
CMD [ 'app.py' ]

================================================
File: LICENSE
================================================
MIT License

Copyright (c) 2023 Parthiban Ravichandran

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the 'Software'), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
File: app.py
================================================
import streamlit as st
from src.predict_pipeline import Prediction_NLP, Prediction_CV
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration, WebRtcMode
from twilio.rest import Client
from streamlit_lottie import st_lottie
import requests
import json
import os
from dotenv import load_dotenv

# Load environment variables from the 'twilio.env' file
load_dotenv(dotenv_path='twilio.env')

prediction_nlp = Prediction_NLP()
prediction_cv = Prediction_CV()

account_sid = os.environ['TWILIO_ACCOUNT_SID']
auth_token = os.environ['TWILIO_AUTH_TOKEN']
client = Client(account_sid, auth_token)

token = client.tokens.create()


#RTC_CONFIGURATION = RTCConfiguration({'iceServers': [{'urls': ['stun:stun.l.google.com:19302']}]})
RTC_CONFIGURATION = RTCConfiguration({'iceServers': token.ice_servers})
 

class MyVideoTransformer(VideoTransformerBase):
    def transform(self, frame):
        # Perform your image processing here
        img = frame.to_ndarray(format='bgr24')
        processed_frame = prediction_cv.image_prediction(img)
        return processed_frame

## Setting up animation
def load_mic(path:str):
    with open(path, 'r') as p:
          return json.load(p)
    
load_mic=load_mic('./mic.json')    


def load_face(path:str):
    with open(path, 'r') as p:
          return json.load(p)
    
load_face=load_face('./emotion.json')    



# Set page config to a clean and minimal style with wide layout
st.set_page_config(
        layout='wide',
        page_title='Multimodal Speech and Facial Emotion Detection',
        page_icon='😃'
    )


def run_streamlit_app():
    activity = ['Home', 'Working']
    choice = st.sidebar.selectbox('Select Activity', activity)
    st.sidebar.markdown(
    '''
    - **Email**: [rparthiban729@gmail.com](mailto:rparthiban729@gmail.com)  
    - **LinkedIn**: [Parthiban Ravichandran](https://www.linkedin.com/in/parthiban-ravichandran/)
    - **GitHub**: [Parthiban-3997](https://github.com/Parthiban-3997)
    ''',
    unsafe_allow_html=True
)


    if choice == 'Home':
            
            # Set title and description with altered text size and color
            st.markdown(
                '''
                <div style='display: flex; flex-direction: column; align-items: center; justify-content: center;'>
                    <h1 style='font-size: 30px; color: blue; text-align: center;'>Multimodal emotion recognition using Speech cues 🎙️ and Facial Expressions 😃</h1>
                </div>
                ''',unsafe_allow_html=True
            )
            # Adding gaps 
            st.write('')
            st.write('')
            
            html_temp_home1 = '''<div style='background-color:#e85530;padding:10px'>
                                                <h5 style='color:white;text-align:center;'>
                                                This web application uses speech cues from which it transcribes text and also uses facial expressions to predict emotions.</h5>
                                                </div>
                                                </br>'''
            st.markdown(html_temp_home1, unsafe_allow_html=True)
            
            # Adding gaps 
            st.write('')
            st.write('')
            st.write('')
            st.write('')
            st.write('')
            st.write('')
            st.write('')
            st.write('')
            

            st.markdown('''
            <p style='font-size: 30px; line-height: 1.4; font-weight: bold; color: #eb4034;text-align: center;'>
            The application has two functionalities
            </p>''', unsafe_allow_html=True)

            st.write('')
            st.write('')
            st.write('')
            st.write('')
            st.write('')

            
            col1, col2 = st.columns(2)
            with col1:
             st_lottie(load_mic)
             st.write('')
             st.write('')
             st.markdown('''
                <ul style='font-size: 25px; line-height: 1.2;font-weight: bold; color: #7611fa;'>
                    Detects text from speech cues and predicts emotions using Bi-LSTM 
                </ul>''', unsafe_allow_html=True)
            with col2:
             st_lottie(load_face)
             st.write('')
             st.write('')
             st.markdown('''
                <ul style='font-size: 25px; line-height: 1.2;font-weight: bold; color: #eb0250;'>
                    Detects real-time facial emotion recognition using Mediapipe and OpenCV 
                </ul>''', unsafe_allow_html=True)

        

    elif choice == 'Working':  

                # Set title and description with altered text size and color
                st.markdown(
                '''
                <div style='display: flex; flex-direction: column; align-items: center; justify-content: center;'>
                    <h1 style='font-size: 30px; color: blue; text-align: center;'>Predicting emotions using NLP 🎙️ and CV techniques 😃</h1>
                </div>
                ''',unsafe_allow_html=True
               )  
                        
                # Create a column layout with wider columns
                col1, col2 = st.columns(2)

                # Set the CSS property for the width of the columns
                col1.markdown(
                    f'<style>div.row-widget:nth-child(1) div[role='main'] .element-container {{ width: 600px; }}</style>',
                    unsafe_allow_html=True
                )
                col2.markdown(
                    f'<style>div.row-widget:nth-child(2) div[role='main'] .element-container {{ width: 600px; }}</style>',
                    unsafe_allow_html=True
                )

                # Create a microphone button in the left column for audio processing
                with col1:
                    st.write('')
                    st.write('')
                    st.write('')
                    st.write('')
                    prediction_nlp.AudioRecorderApp()

                # Create a camera button in the right column for video processing
                with col2:
                    st.write('')
                    st.write('')
                    st.write('')
                    st.write('')
                    st.markdown('<h5 style='text-align: center; color: green; font-size: 20px;'>Start Video Capture 📸</h5>', unsafe_allow_html=True)
                    # Start the video stream capture
                    webrtc_streamer(
                        key='example',
                        mode=WebRtcMode.SENDRECV,
                        rtc_configuration=RTC_CONFIGURATION,
                        video_processor_factory=MyVideoTransformer
                    )

    else:
            pass

if __name__ == '__main__':
    run_streamlit_app()


================================================
File: emotion.json
================================================
{'v':'5.5.6','fr':30,'ip':0,'op':161,'w':1080,'h':1080,'nm':'Face Scan','ddd':0,'assets':[],'layers':[{'ddd':0,'ind':1,'ty':4,'nm':'Shape Layer 10','sr':1,'ks':{'o':{'a':1,'k':[{'i':{'x':[0.833],'y':[0.833]},'o':{'x':[0.167],'y':[0.167]},'t':101,'s':[56]},{'i':{'x':[0.833],'y':[0.833]},'o':{'x':[0.167],'y':[0.167]},'t':120,'s':[100]},{'t':126,'s':[0]}],'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':1,'k':[{'i':{'x':0.833,'y':0.833},'o':{'x':0.167,'y':0.167},'t':40,'s':[539,728,0],'to':[-0.333,-22.667,0],'ti':[0.333,51.667,0]},{'i':{'x':0.833,'y':0.833},'o':{'x':0.167,'y':0.167},'t':50,'s':[537,592,0],'to':[-0.333,-51.667,0],'ti':[0,7.333,0]},{'i':{'x':0.833,'y':0.833},'o':{'x':0.167,'y':0.167},'t':67,'s':[537,418,0],'to':[0,-7.333,0],'ti':[0,23,0]},{'i':{'x':0.833,'y':0.833},'o':{'x':0.167,'y':0.167},'t':84,'s':[537,548,0],'to':[0,-23,0],'ti':[0,-29.833,0]},{'i':{'x':0.833,'y':0.833},'o':{'x':0.167,'y':0.167},'t':101,'s':[537,280,0],'to':[0,29.833,0],'ti':[0,-74.5,0]},{'t':120,'s':[537,727,0]}],'ix':2},'a':{'a':0,'k':[-5,188,0],'ix':1},'s':{'a':1,'k':[{'i':{'x':[0.833,0.833,0.833],'y':[0.833,0.833,0.833]},'o':{'x':[0.167,0.167,0.167],'y':[0.167,0.167,0.167]},'t':40,'s':[100,100,100]},{'i':{'x':[0.833,0.833,0.833],'y':[0.833,0.833,0.833]},'o':{'x':[0.167,0.167,0.167],'y':[0.167,0.167,0.167]},'t':50,'s':[91,91,100]},{'i':{'x':[0.833,0.833,0.833],'y':[0.833,0.833,0.833]},'o':{'x':[0.167,0.167,0.167],'y':[0.167,0.167,0.167]},'t':67,'s':[100,100,100]},{'i':{'x':[0.833,0.833,0.833],'y':[0.833,0.833,0.833]},'o':{'x':[0.167,0.167,0.167],'y':[0.167,0.167,0.167]},'t':84,'s':[91,91,100]},{'t':101,'s':[100,100,100]}],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ty':'rc','d':1,'s':{'a':0,'k':[466.844,25.008],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'r':{'a':0,'k':41,'ix':4},'nm':'Rectangle Path 1','mn':'ADBE Vector Shape - Rect','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[-5.523,187.012],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Rectangle 1','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false},{'ty':'tm','s':{'a':0,'k':0,'ix':1},'e':{'a':1,'k':[{'i':{'x':[0.833],'y':[0.833]},'o':{'x':[0.167],'y':[0.167]},'t':30,'s':[0]},{'t':40,'s':[100]}],'ix':2},'o':{'a':0,'k':0,'ix':3},'m':1,'ix':2,'nm':'Trim Paths 1','mn':'ADBE Vector Filter - Trim','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':2,'ty':4,'nm':'Shape Layer 7','sr':1,'ks':{'o':{'a':0,'k':100,'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[540,506,0],'ix':2},'a':{'a':0,'k':[0,0,0],'ix':1},'s':{'a':1,'k':[{'i':{'x':[0.717,0.717,0.833],'y':[0.992,0.992,-8.921]},'o':{'x':[0.06,0.06,0.167],'y':[0.639,0.639,20]},'t':18,'s':[0,0,100]},{'i':{'x':[0.9,0.9,0.833],'y':[0.697,0.697,10.167]},'o':{'x':[0.746,0.746,0.167],'y':[-0.056,-0.056,5.952]},'t':25.5,'s':[124.17,124.17,100]},{'i':{'x':[0.9,0.9,0.833],'y':[1,1,1]},'o':{'x':[0,0,0],'y':[0,0,0]},'t':30,'s':[94.957,94.957,100]},{'i':{'x':[0.855,0.855,0.855],'y':[1,1,1]},'o':{'x':[0,0,0],'y':[0,0,0]},'t':40,'s':[94.957,94.957,100]},{'i':{'x':[0.833,0.833,0.833],'y':[1,1,1]},'o':{'x':[0.167,0.167,0.167],'y':[0,0,0]},'t':50,'s':[83.957,83.957,100]},{'i':{'x':[0.833,0.833,0.833],'y':[1,1,1]},'o':{'x':[0.167,0.167,0.167],'y':[0,0,0]},'t':67.001,'s':[95,95,100]},{'i':{'x':[0.833,0.833,0.833],'y':[1,1,1]},'o':{'x':[0.167,0.167,0.167],'y':[0,0,0]},'t':83.999,'s':[83.957,83.957,100]},{'i':{'x':[0.833,0.833,0.833],'y':[1,1,1]},'o':{'x':[0.167,0.167,0.167],'y':[0,0,0]},'t':101,'s':[95,95,100]},{'i':{'x':[0.833,0.833,0.833],'y':[1,1,1]},'o':{'x':[0.167,0.167,0.167],'y':[0,0,0]},'t':142,'s':[95,95,100]},{'t':149,'s':[0,0,100]}],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[-0.75,0],[0,0],[0,0],[0,0],[0,0]],'o':[[0,0],[0.75,0],[0,0],[0,0],[0,0],[0,0]],'v':[[-291.25,-293.75],[-291.25,-165],[-275,-165],[-274.75,-276.25],[-162.25,-277],[-162.25,-293.5]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 2','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[-223.793,223.023],'ix':2},'a':{'a':0,'k':[-223.564,-224.502],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':270.399,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 4','np':4,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[-0.75,0],[0,0],[0,0],[0,0],[0,0]],'o':[[0,0],[0.75,0],[0,0],[0,0],[0,0],[0,0]],'v':[[-291.25,-293.75],[-291.25,-165],[-275,-165],[-274.75,-276.25],[-162.25,-277],[-162.25,-293.5]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 2','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[224.207,222.023],'ix':2},'a':{'a':0,'k':[-223.564,-224.502],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':180.303,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 3','np':4,'cix':2,'bm':0,'ix':2,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[-0.75,0],[0,0],[0,0],[0,0],[0,0]],'o':[[0,0],[0.75,0],[0,0],[0,0],[0,0],[0,0]],'v':[[-291.25,-293.75],[-291.25,-165],[-275,-165],[-274.75,-276.25],[-162.25,-277],[-162.25,-293.5]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 2','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[223.707,-225.477],'ix':2},'a':{'a':0,'k':[-223.564,-224.502],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':90.225,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 2','np':4,'cix':2,'bm':0,'ix':3,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[-0.75,0],[0,0],[0,0],[0,0],[0,0]],'o':[[0,0],[0.75,0],[0,0],[0,0],[0,0],[0,0]],'v':[[-291.25,-293.75],[-291.25,-165],[-275,-165],[-274.75,-276.25],[-162.25,-277],[-162.25,-293.5]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 2','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 1','np':4,'cix':2,'bm':0,'ix':4,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':3,'ty':4,'nm':'Shape Layer 6','sr':1,'ks':{'o':{'a':1,'k':[{'i':{'x':[0.833],'y':[0.833]},'o':{'x':[0.167],'y':[0.167]},'t':101,'s':[1]},{'i':{'x':[0.833],'y':[0.833]},'o':{'x':[0.167],'y':[0.167]},'t':126,'s':[100]},{'i':{'x':[0.833],'y':[0.833]},'o':{'x':[0.167],'y':[0.167]},'t':142,'s':[100]},{'t':149,'s':[0]}],'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[540,540,0],'ix':2},'a':{'a':0,'k':[0,0,0],'ix':1},'s':{'a':0,'k':[100,100,100],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[0,0]],'o':[[0,0],[0,0]],'v':[[-70,-41.5],[9.5,-133]],'c':false},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'st','c':{'a':0,'k':[1,1,1,1],'ix':3},'o':{'a':0,'k':100,'ix':4},'w':{'a':0,'k':3,'ix':5},'lc':1,'lj':1,'ml':4,'bm':0,'nm':'Stroke 1','mn':'ADBE Vector Graphic - Stroke','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 4','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[0,0],[0,0],[0,0]],'o':[[0,0],[0,0],[0,0],[0,0]],'v':[[89,-41.5],[10,66],[140.5,109],[11.5,202]],'c':false},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'st','c':{'a':0,'k':[1,1,1,1],'ix':3},'o':{'a':0,'k':100,'ix':4},'w':{'a':0,'k':3,'ix':5},'lc':1,'lj':1,'ml':4,'bm':0,'nm':'Stroke 1','mn':'ADBE Vector Graphic - Stroke','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 3','np':3,'cix':2,'bm':0,'ix':2,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[0,0],[0,0],[0,0],[0,0]],'o':[[0,0],[0,0],[0,0],[0,0],[0,0]],'v':[[202,-46.5],[89.5,-42],[194,-190],[10,-133.5],[90.5,-243]],'c':false},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'st','c':{'a':0,'k':[1,1,1,1],'ix':3},'o':{'a':0,'k':100,'ix':4},'w':{'a':0,'k':3,'ix':5},'lc':1,'lj':1,'ml':4,'bm':0,'nm':'Stroke 1','mn':'ADBE Vector Graphic - Stroke','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 2','np':3,'cix':2,'bm':0,'ix':3,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]],'o':[[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]],'v':[[-73,-243],[10,-132],[89,-42],[139,110],[210,-50],[192.5,-190.5],[90.5,-243.5],[-72,-244],[-175,-185.5],[-191,-50],[-123.5,105.5],[8.5,202.5],[9,65],[-123,106],[-70,-42.5],[8.5,65.5],[10,-133.5],[-174.5,-185],[-70,-42.75],[-190.5,-50]],'c':false},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'st','c':{'a':0,'k':[1,1,1,1],'ix':3},'o':{'a':0,'k':100,'ix':4},'w':{'a':0,'k':3,'ix':5},'lc':1,'lj':1,'ml':4,'bm':0,'nm':'Stroke 1','mn':'ADBE Vector Graphic - Stroke','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 1','np':3,'cix':2,'bm':0,'ix':4,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'st','c':{'a':0,'k':[1,1,1,1],'ix':3},'o':{'a':0,'k':100,'ix':4},'w':{'a':0,'k':3,'ix':5},'lc':1,'lj':1,'ml':4,'bm':0,'nm':'Stroke 1','mn':'ADBE Vector Graphic - Stroke','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[8.866,202.856],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 13','np':3,'cix':2,'bm':0,'ix':5,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[143.114,105.97],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 12','np':3,'cix':2,'bm':0,'ix':6,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[8.376,66.204],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 11','np':3,'cix':2,'bm':0,'ix':7,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[-122.845,106.69],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 10','np':3,'cix':2,'bm':0,'ix':8,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[-191.739,-46.798],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 9','np':3,'cix':2,'bm':0,'ix':9,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[-69.821,-41.808],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 8','np':3,'cix':2,'bm':0,'ix':10,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[88.776,-41.562],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 7','np':3,'cix':2,'bm':0,'ix':11,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[211.163,-47.374],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 6','np':3,'cix':2,'bm':0,'ix':12,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[193.011,-187.747],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 5','np':3,'cix':2,'bm':0,'ix':13,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[92.198,-243.386],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 4','np':3,'cix':2,'bm':0,'ix':14,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[9.522,-133.731],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 3','np':3,'cix':2,'bm':0,'ix':15,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[-72.014,-242.744],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 2','np':3,'cix':2,'bm':0,'ix':16,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'d':1,'ty':'el','s':{'a':0,'k':[23.469,23.469],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'nm':'Ellipse Path 1','mn':'ADBE Vector Shape - Ellipse','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[-174.176,-187.343],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Ellipse 1','np':3,'cix':2,'bm':0,'ix':17,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':4,'ty':4,'nm':'Shape Layer 5','parent':9,'sr':1,'ks':{'o':{'a':0,'k':100,'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[0,-6,0],'ix':2},'a':{'a':0,'k':[0,0,0],'ix':1},'s':{'a':0,'k':[100,100,100],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[113.5,-14.5],[0,0],[-9.5,-1.5],[-22.75,3.5],[-51.75,0.5],[-16,-7.75],[-1,-28.5],[-13.75,-15.5],[-1.75,-4.75],[0,0],[1,13],[9.25,15.25],[1.25,2.5],[0.25,29.75],[7,3.5]],'o':[[-101,24.5],[0,0],[29.25,2.75],[22.5,-6.5],[51.75,-0.5],[37.25,14.5],[2.25,31],[13.75,15.5],[1.75,4.75],[0,0],[1.25,-11.25],[-13.25,-8.75],[-1.25,-2.5],[-3.5,-28],[-7,-3.5]],'v':[[38.5,-420],[-114.5,-400.5],[-104.25,-398.75],[-0.75,-405.5],[99.5,-415.75],[182.25,-403.25],[221.75,-329],[251.25,-270.25],[268.5,-235.75],[271.25,-188.75],[272.25,-229.75],[257.75,-271],[232,-310.5],[227.5,-362],[178.5,-414.5]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[0.13333333333333333,0.3411764705882353,0.49411764705882355,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 2','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[81,-3],[0,0],[64.5,44],[20.5,-37.5],[-15.5,-64],[1.5,-8.5],[-42,-78],[-10,2],[0,0],[-44.75,62.5],[-2,8],[-24.75,-10.75],[-19.75,-4.25],[-13,-27.75],[-6.25,-38.75],[-34.25,7.75],[-2,-9.5],[-4.5,30],[59,79.5],[0,0]],'o':[[-78.5,6.5],[0,0],[-66.5,-27],[-20.5,37.5],[11.5,65],[0,11.5],[11.5,-26.5],[10,-2],[0,0],[42.5,-44.5],[11.25,10.5],[22.5,12],[19.75,7],[10,15],[9.25,-30.75],[36.75,4],[10.75,-16.25],[10,-26],[-12.5,-10.5],[0,0]],'v':[[137,-494.5],[19.5,-471.5],[-119.5,-481],[-259,-426.5],[-272,-297.5],[-266.5,-234],[-236.5,-73],[-202.5,-108.5],[-196,-108.5],[-157.5,-212],[-107.75,-284.75],[-27.5,-226.5],[44.5,-195],[147.75,-142.25],[187.25,-45.75],[244.5,-109],[287.25,-59.25],[336.5,-156],[323,-334],[265.5,-395]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[0.043137254902,0.176470588235,0.305882352941,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 1','np':3,'cix':2,'bm':0,'ix':2,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':5,'ty':4,'nm':'Shape Layer 8','parent':9,'sr':1,'ks':{'o':{'a':0,'k':100,'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[0,-6,0],'ix':2},'a':{'a':0,'k':[0,0,0],'ix':1},'s':{'a':0,'k':[100,100,100],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[3.5,65.5],[0,0],[0,0],[42,22.5],[13,-3.5],[2,-0.5],[0,0],[17.5,10.5],[82.5,-86],[-7,-12.5],[0,0]],'o':[[0,0],[-3,-55.5],[0,0],[0,0],[-42,-22.5],[-13,3.5],[-2,0.5],[0,0],[-17.5,-10.5],[-66.5,93],[7,12.5],[0,0]],'v':[[328,-153],[371,-265.5],[306.5,-341],[262.5,-389.5],[202,-490],[104.5,-506],[54.5,-489],[-24.5,-453],[-114,-480.5],[-276.5,-451],[-261,-258.5],[-248,-218.5]],'c':false},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'st','c':{'a':0,'k':[0.043137254902,0.176470588235,0.305882352941,1],'ix':3},'o':{'a':0,'k':100,'ix':4},'w':{'a':0,'k':3,'ix':5},'lc':1,'lj':1,'ml':4,'bm':0,'nm':'Stroke 1','mn':'ADBE Vector Graphic - Stroke','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 1','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':6,'ty':4,'nm':'Shape Layer 4','parent':9,'sr':1,'ks':{'o':{'a':0,'k':100,'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[0,-6,0],'ix':2},'a':{'a':0,'k':[0,0,0],'ix':1},'s':{'a':0,'k':[100,100,100],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[0,0],[0,0],[0,0],[-7.5,-16.5],[-7.75,7.25],[-26.25,29.75],[-31,-12.75],[-42.25,-21.75],[0,0],[0,0],[21,21],[55,22.5]],'o':[[0,0],[0,0],[0,0],[7.5,-24],[7,-3],[24.25,47.25],[33.75,5],[43,23],[0,0],[0,0],[-21,-21],[-55,-22.5]],'v':[[-108,-284.5],[-162.5,-238.5],[-206,-150.5],[-196,-111.5],[-145.25,-184.75],[-92,-235.75],[-4.25,-180.25],[103,-144.5],[187.125,-38],[190,-51.5],[141,-155.5],[-0.5,-213]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[0.898039215686,0.470588235294,0.360784313725,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 2','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[24,42],[12.5,-9],[11,-34],[5.5,-45],[-35.5,-13.5],[0,0],[-65,-49.5],[-38,36],[-14.5,47.5],[-35.5,54],[9.5,37.5],[6,-1],[11,-53.5],[34.5,21.5],[48.5,8.5]],'o':[[-31.5,33.5],[-10,10.5],[-14.5,1.5],[-5,47],[32,10],[0,0],[65,49.5],[38,-36],[6.25,1.75],[10.5,-22.5],[-13,-53],[-15,6.5],[-24.5,-46],[-36,-18],[-48.5,-17]],'v':[[-92,-235],[-144.5,-184.5],[-196.5,-109.5],[-239,-52.5],[-182.5,56.5],[-169,60.5],[-72,194.5],[116,180.5],[196,58],[272.5,19],[286,-64],[236.5,-107.5],[187,-38],[103,-145],[-4,-180.5]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[0.972549019608,0.63137254902,0.478431372549,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 1','np':3,'cix':2,'bm':0,'ix':2,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':7,'ty':4,'nm':'Shape Layer 3','parent':9,'sr':1,'ks':{'o':{'a':0,'k':100,'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[0,-6,0],'ix':2},'a':{'a':0,'k':[0,0,0],'ix':1},'s':{'a':0,'k':[100,100,100],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[108.368,45.154],[0,0],[-139.811,55.924],[0,0]],'o':[[-1.5,-0.625],[0,0],[2.5,-1],[0,0]],'v':[[-98,173.5],[-102,209],[120.5,216],[117,179.5]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[0.898039215686,0.470588235294,0.360784313725,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 1','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':8,'ty':4,'nm':'Shape Layer 2','parent':9,'sr':1,'ks':{'o':{'a':0,'k':100,'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[0,-6,0],'ix':2},'a':{'a':0,'k':[0,0,0],'ix':1},'s':{'a':0,'k':[100,100,100],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[-4.5,-17.5],[10.5,-17.5],[4,-12.5],[-25,-14.5],[-22,6.5],[-3.5,10],[3,5.5],[6.5,25.5]],'o':[[4.5,17.5],[-10.5,17.5],[-12.436,38.863],[68.465,39.71],[61.175,-18.074],[6.012,-17.178],[-27.421,-50.273],[-6.5,-25.5]],'v':[[-99,170.5],[-117.5,254.5],[-142,306.5],[-95.5,392],[70.5,407],[150.5,334],[142.5,271.5],[117,180]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[0.972549019608,0.63137254902,0.478431372549,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 1','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':9,'ty':4,'nm':'Shape Layer 1','sr':1,'ks':{'o':{'a':0,'k':100,'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[536,1110,0],'ix':2},'a':{'a':0,'k':[-4,564,0],'ix':1},'s':{'a':1,'k':[{'i':{'x':[0.52,0.52,0.833],'y':[0.979,0.979,-11.037]},'o':{'x':[0.12,0.12,0.167],'y':[0.234,0.234,16.807]},'t':0,'s':[0,0,100]},{'i':{'x':[0.94,0.94,0.833],'y':[0.799,0.799,1.14]},'o':{'x':[1,1,0.167],'y':[-0.099,-0.099,4.63]},'t':13,'s':[120,120,100]},{'i':{'x':[0.833,0.833,0.833],'y':[1,1,1]},'o':{'x':[0,0,0],'y':[0,0,0]},'t':18,'s':[100,100,100]},{'i':{'x':[0.52,0.52,0.833],'y':[1,1,1]},'o':{'x':[0.167,0.167,0.167],'y':[0,0,0]},'t':142,'s':[100,100,100]},{'i':{'x':[1,1,1],'y':[1,1,1]},'o':{'x':[1,1,0.167],'y':[0,0,0]},'t':155,'s':[120,120,100]},{'t':161,'s':[0,0,100]}],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[243,-10],[0,0],[-3,42],[6,1]],'o':[[-243,10],[0,0],[16.887,-236.417],[-6,-1]],'v':[[-165,271],[-387,544],[400,563],[245,280]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[0.3333333333333333,0.5176470588235295,0.6745098039215687,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 2','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false},{'ty':'gr','it':[{'ind':0,'ty':'sh','ix':1,'ks':{'a':0,'k':{'i':[[298,-18],[0,0],[9,18],[0,0]],'o':[[-288.481,17.425],[0,0],[-104.615,-209.231],[0,0]],'v':[[-194,274],[-548,532],[554,533],[305,288]],'c':true},'ix':2},'nm':'Path 1','mn':'ADBE Vector Shape - Group','hd':false},{'ty':'fl','c':{'a':0,'k':[0.3333333333333333,0.5176470588235295,0.6745098039215687,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[0,0],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[100,100],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Shape 1','np':3,'cix':2,'bm':0,'ix':2,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0},{'ddd':0,'ind':10,'ty':4,'nm':'Shape Layer 9','sr':1,'ks':{'o':{'a':0,'k':100,'ix':11},'r':{'a':0,'k':0,'ix':10},'p':{'a':0,'k':[540,512,0],'ix':2},'a':{'a':0,'k':[0,0,0],'ix':1},'s':{'a':0,'k':[100,107.269,100],'ix':6}},'ao':0,'shapes':[{'ty':'gr','it':[{'ty':'rc','d':1,'s':{'a':0,'k':[1106.719,1170.695],'ix':2},'p':{'a':0,'k':[0,0],'ix':3},'r':{'a':0,'k':0,'ix':4},'nm':'Rectangle Path 1','mn':'ADBE Vector Shape - Rect','hd':false},{'ty':'st','c':{'a':0,'k':[0.043137254902,0.176470588235,0.305882352941,1],'ix':3},'o':{'a':0,'k':100,'ix':4},'w':{'a':0,'k':3,'ix':5},'lc':1,'lj':1,'ml':4,'bm':0,'nm':'Stroke 1','mn':'ADBE Vector Graphic - Stroke','hd':false},{'ty':'fl','c':{'a':0,'k':[1,1,1,1],'ix':4},'o':{'a':0,'k':100,'ix':5},'r':1,'bm':0,'nm':'Fill 1','mn':'ADBE Vector Graphic - Fill','hd':false},{'ty':'tr','p':{'a':0,'k':[3.359,27.348],'ix':2},'a':{'a':0,'k':[0,0],'ix':1},'s':{'a':0,'k':[99.355,87.503],'ix':3},'r':{'a':0,'k':0,'ix':6},'o':{'a':0,'k':100,'ix':7},'sk':{'a':0,'k':0,'ix':4},'sa':{'a':0,'k':0,'ix':5},'nm':'Transform'}],'nm':'Rectangle 1','np':3,'cix':2,'bm':0,'ix':1,'mn':'ADBE Vector Group','hd':false}],'ip':0,'op':300,'st':0,'bm':0}],'markers':[]}

================================================
File: packages.txt
================================================
freeglut3-dev
libgtk2.0-dev
libgl1-mesa-glx
tesseract-ocr
libtesseract-dev
tesseract-ocr-all
portaudio19-dev
libespeak1
python3-opencv

================================================
File: requirements.txt
================================================
numpy
pandas
scikit-learn
tensorflow
opencv-python-headless
streamlit
streamlit-webrtc
speechrecognition
mediapipe
nltk
pyaudio
audio-recorder-streamlit
assemblyai
twilio
python-dotenv
streamlit-lottie
path
requests
#-e .

================================================
File: setup.py
================================================
from setuptools import find_packages,setup
from typing import List

HYPEN_E_DOT='-e .'
def get_requirements(file_path:str)->List[str]:
    '''
    this function will return the list of requirements
    '''
    requirements=[]
    with open(file_path) as file_obj:
        requirements=file_obj.readlines()
        requirements=[req.replace('\n','') for req in requirements]

        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
    
    return requirements

setup(
name='Emotion_Recognition_Using_NLP_CV',
version='0.0.1',
author='Parthiban Ravichandran',
author_email='rparthiban729@gmail.com',
packages=find_packages(),
install_requires=get_requirements('requirements.txt')
)


================================================
File: twilio.env
================================================
import os

# Set your speech key and region
#speech_key = 'eef86f039bdb4ed8a627675d897924e9'
#speech_region = 'eastus'

# Export the environment variables
#os.environ['SPEECH_KEY'] = speech_key
#os.environ['SPEECH_REGION'] = speech_region


## twilio
TWILIO_ACCOUNT_SID=ACd3b01d2afa64dd275e4fa3bb8e06b92c
TWILIO_AUTH_TOKEN=cf396b73e18c0423136260b48c8e5c8f


================================================
File: artifacts/Bi-LSTM.h5
================================================
version https://git-lfs.github.com/spec/v1
oid sha256:b431b8659f28f511259dfb2741321368b5fb5dadc8ae6e0f70c5dd03b4396171
size 97396984


================================================
File: artifacts/CNN_final.h5
================================================
version https://git-lfs.github.com/spec/v1
oid sha256:a08422082a157f5eb49f17ad562eed04219781fc5c6a805a282caa251ab1cc4c
size 78793512


================================================
File: src/data_ingestion.py
================================================
import os
import sys
from src.exception import CustomException
from src.logger import logging
import pandas as pd
import numpy as np
from src.components.data_transformation import  DataTransformation_nlp,DataTransformation_cv
from src.components.model_trainer import ModelTrainer_nlp, ModelTrainer_cv
from dataclasses import dataclass

@dataclass
class DataIngestionConfig:
    train_data_path: str = os.path.join('artifacts', 'train_nlp.csv')
    test_data_path: str = os.path.join('artifacts', 'test_nlp.csv')
    validation_data_path: str = os.path.join('artifacts', 'validation_nlp.csv')
    train_cv_folder_path: str = os.path.join('artifacts', 'train_cv')
    test_cv_folder_path: str = os.path.join('artifacts', 'test_cv')

class DataIngestion:
    def __init__(self):
        self.ingestion_config = DataIngestionConfig()

    def initiate_data_ingestion(self):
        logging.info('Entered the data ingestion method or component')
        try:
            # Create artifacts folder if it doesn't exist
            os.makedirs('artifacts', exist_ok=True)

            # Read the train_nlp.csv file and save it to artifacts folder
            df = pd.read_csv('notebook/data/training_nlp.csv')
            df.to_csv(self.ingestion_config.train_data_path, index=False, header=True)
            logging.info('Saved train_nlp.csv to artifacts folder')

            # Read the test_nlp.csv file and save it to artifacts folder
            df = pd.read_csv('notebook/data/test_nlp.csv')
            df.to_csv(self.ingestion_config.test_data_path, index=False, header=True)
            logging.info('Saved test_nlp.csv to artifacts folder')

            # Read the validation_nlp.csv file and save it to artifacts folder
            df = pd.read_csv('notebook/data/validation_nlp.csv')
            df.to_csv(self.ingestion_config.validation_data_path, index=False, header=True)
            logging.info('Saved validation_nlp.csv to artifacts folder')

            # Create train_cv and test_cv folders under artifacts folder
            os.makedirs(self.ingestion_config.train_cv_folder_path, exist_ok=True)
            os.makedirs(self.ingestion_config.test_cv_folder_path, exist_ok=True)
            logging.info('Created train_cv and test_cv folders under artifacts folder')

            # Move the subfolders and files from notebook/data/train_cv to artifacts/train_cv
            train_cv_path = os.path.join('notebook/data', 'train_cv')
            for folder_name in os.listdir(train_cv_path):
                folder_path = os.path.join(train_cv_path, folder_name)
                if os.path.isdir(folder_path):
                    target_folder_path = os.path.join(self.ingestion_config.train_cv_folder_path, folder_name)
                    os.makedirs(target_folder_path, exist_ok=True)
                    for file_name in os.listdir(folder_path):
                        file_path = os.path.join(folder_path, file_name)
                        target_file_path = os.path.join(target_folder_path, file_name)
                        os.rename(file_path, target_file_path)
                    logging.info(f'Moved subfolder '{folder_name}' from notebook/data/train_cv to artifacts/train_cv')

            # Move the subfolders and files from notebook/data/test_cv to artifacts/test_cv
            test_cv_path = os.path.join('notebook/data', 'test_cv')
            for folder_name in os.listdir(test_cv_path):
                folder_path = os.path.join(test_cv_path, folder_name)
                if os.path.isdir(folder_path):
                    target_folder_path = os.path.join(self.ingestion_config.test_cv_folder_path, folder_name)
                    os.makedirs(target_folder_path, exist_ok=True)
                    for file_name in os.listdir(folder_path):
                        file_path = os.path.join(folder_path, file_name)
                        target_file_path = os.path.join(target_folder_path, file_name)
                        os.rename(file_path, target_file_path)
                    logging.info(f'Moved subfolder '{folder_name}' from notebook/data/test_cv to artifacts/test_cv')

            logging.info('Ingestion of the data is completed')

            return (
                self.ingestion_config.train_data_path,
                self.ingestion_config.test_data_path,
                self.ingestion_config.validation_data_path,
                self.ingestion_config.train_cv_folder_path,
                self.ingestion_config.test_cv_folder_path
            )
        except Exception as e:
            raise CustomException(e, sys)


if __name__ == '__main__':
    obj = DataIngestion()
    train_data_path, test_data_path, validation_data_path, train_cv_folder_path, test_cv_folder_path = obj.initiate_data_ingestion()
    
    # Perform NLP data preprocessing
    #data_transformation_nlp = DataTransformation_nlp(train_data_path, test_data_path, validation_data_path)
    #train_data_nlp, train_labels_nlp, train_max_sequence_length_nlp, train_voc_size_nlp = data_transformation_nlp.preprocess_dataset(train_data_path)
    #test_data_nlp, test_labels_nlp, test_max_sequence_length_nlp, test_voc_size_nlp = data_transformation_nlp.preprocess_dataset(test_data_path)
    #validation_data_nlp, validation_labels_nlp, validation_max_sequence_length_nlp, validation_voc_size_nlp = data_transformation_nlp.preprocess_dataset(validation_data_path)
    #X_train_final,y_train_final= data_transformation_nlp.convert_to_arrays(train_data_nlp, train_labels_nlp)
    #X_valid_final,y_valid_final= data_transformation_nlp.convert_to_arrays(validation_data_nlp, validation_labels_nlp)
    #X_test_final,y_test_final= data_transformation_nlp.convert_to_arrays(test_data_nlp, test_labels_nlp)
    

    ## Perform Model Initialization on NLP
    #model_trainer_nlp = ModelTrainer_nlp(X_train_final,y_train_final,X_valid_final,y_valid_final,X_test_final,y_test_final,train_max_sequence_length_nlp, train_voc_size_nlp)
    #model_trainer_nlp.initiate_model_trainer_nlp()
    

    # Perform CV data preprocessing
    #data_transformation_cv = DataTransformation_cv(train_cv_folder_path, test_cv_folder_path)
    #train_generator_cv, test_generator_cv,input_shape = data_transformation_cv.preprocess_dataset()


    ## Perform Model Initialization on CV
    #model_trainer_cv = ModelTrainer_cv(train_generator_cv, test_generator_cv,input_shape)
    #model_trainer_cv.initiate_model_trainer_cv()


    



================================================
File: src/data_transformation.py
================================================
import sys
from dataclasses import dataclass
from src.exception import CustomException
from src.logger import logging
import numpy as np
import pandas as pd
import re
import nltk
import tensorflow.compat.v1 as tf
from tensorflow.keras.preprocessing.text import one_hot
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing import image
from tensorflow.keras.layers import Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical


class DataTransformation_nlp:
   def __init__(self, train_data_path, test_data_path, validation_data_path):
        self.train_data_path = train_data_path
        self.test_data_path = test_data_path
        self.validation_data_path = validation_data_path

        # Initialize the stop words
        self.stop_words = set(stopwords.words('english'))   

        # Initialize the PorterStemmer
        self.ps = PorterStemmer()

   def preprocess_text(self, texts):
        logging.info('Performing text preprocessing...')
        corpus = []
        for text in texts:
            review = re.sub('[^a-zA-Z]', ' ', text.lower())
            review = review.split()
            review = [self.ps.stem(word) for word in review if word not in self.stop_words]
            review = ' '.join(review)
            corpus.append(review)

        return corpus

   def preprocess_dataset(self, filename):
        try:
            logging.info('Loading dataset...')
            data = pd.read_csv(filename)
            texts = data['text'].values
            labels = data['label'].values

            # Preprocess the text data
            preprocessed_texts = self.preprocess_text(texts)

            # Convert text to sequences
            voc_size = 5000
            onehot_repr = [one_hot(words, voc_size) for words in preprocessed_texts]

            # Pad sequences for uniform length
            max_sequence_length = 200
            padded_sequences = pad_sequences(onehot_repr, padding='pre', maxlen=max_sequence_length)

            # Convert labels to one-hot encoded vectors
            num_classes = len(set(labels))
            labels = tf.keras.utils.to_categorical(labels, num_classes)

            logging.info('Data preprocessing completed.')

            return padded_sequences, labels, max_sequence_length, voc_size

        except Exception as e:
            logging.error(f'Error occurred during data preprocessing: {e}')
            raise CustomException(e, sys)

   def convert_to_arrays(self, padded_sequences, labels):
        try:
            logging.info('Converting texts and labels to arrays')
            padded_sequences_array = np.array(padded_sequences)
            labels_array = np.array(labels)
            logging.info('Conversion completed successfully')
            return padded_sequences_array, labels_array
        except Exception as e:
            logging.error(f'Error occurred during conversion to array format: {e}')
            raise CustomException(e, sys)
   

class DataTransformation_test_data_nlp:   
   def preprocess_text_for_prediction(self,text):
        try:
            # Preprocess the text
            nltk.download('stopwords')
            processed_text = text.lower()
            processed_text = re.sub('[^a-zA-Z]', ' ', processed_text)
            processed_text = processed_text.split()
            ps = PorterStemmer()
            processed_text = [ps.stem(word) for word in processed_text if word not in stopwords.words('english')]
            processed_text = ' '.join(processed_text)

            return processed_text
        
        except Exception as e:
            raise CustomException(e, sys)
        

   def returning_preprocessed_text(self,result):
        try:
            preprocessed_text = self.preprocess_text_for_prediction(result)
            
            # Convert text to one-hot encoding
            voc_size = 5000
            onehot_repr = [one_hot(preprocessed_text, voc_size)]

            # Pad sequence for uniform length
            max_sequence_length = 200
            padded_sequence = pad_sequences(onehot_repr, padding='pre', maxlen=max_sequence_length)

            return padded_sequence
        
        except Exception as e:
            raise CustomException(e, sys)
       
       
class DataTransformation_cv:
    def __init__(self, train_dir, test_dir):
        self.train_dir = train_dir
        self.test_dir = test_dir
        self.img_width, self.img_height = 48, 48
        self.input_shape = (48, 48, 1)

    def preprocess_dataset(self):
        logging.info('Entered the CV data preprocessing method')
        try:
            # Set up the data generators
            train_datagen = ImageDataGenerator(rescale=1.0/255,
                                               shear_range=0.2,
                                               zoom_range=0.2,
                                               horizontal_flip=True)
            test_datagen = ImageDataGenerator(rescale=1.0/255)

            train_generator = train_datagen.flow_from_directory(self.train_dir,
                                                                target_size=(self.img_width, self.img_height),
                                                                batch_size=128,
                                                                color_mode='grayscale',
                                                                class_mode='categorical')

            test_generator = test_datagen.flow_from_directory(self.test_dir,
                                                              target_size=(self.img_width, self.img_height),
                                                              batch_size=128,
                                                              color_mode='grayscale',
                                                              class_mode='categorical')

            logging.info('CV data preprocessing completed')

            return (train_generator, test_generator, self.input_shape)
        except Exception as e:
            raise CustomException(e, sys)


================================================
File: src/exception.py
================================================
import sys
from src.logger import logging

def error_message_detail(error,error_detail:sys):
    _,_,exc_tb=error_detail.exc_info()
    file_name=exc_tb.tb_frame.f_code.co_filename
    error_message='Error occured in python script name [{0}] line number [{1}] error message[{2}]'.format(
     file_name,exc_tb.tb_lineno,str(error))

    return error_message

    

class CustomException(Exception):
    def __init__(self,error_message,error_detail:sys):
        super().__init__(error_message)
        self.error_message=error_message_detail(error_message,error_detail)
        
        
    
    def __str__(self):
        return self.error_message
    

if __name__ == '__main__':
    try:
        a=1/0
    except Exception as e:
        logging.info('Divide by zero error')
        raise CustomException(e,sys)

================================================
File: src/logger.py
================================================
import logging
import os
from datetime import datetime

LOG_FILE=f'{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log'
logs_path=os.path.join(os.getcwd(),'logs',LOG_FILE)  ## Responsible for creating a directory named <date.log> inside log folder
os.makedirs(logs_path,exist_ok=True) ## Creates a directory called logs

LOG_FILE_PATH=os.path.join(logs_path,LOG_FILE) ## Actual file

logging.basicConfig(
    filename=LOG_FILE_PATH,
    format='[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

if __name__ == '__main__':
    logging.info('Logging has started')

================================================
File: src/predict_pipeline.py
================================================
import os
import sys
import pandas as pd
import numpy as np
import nltk
import speech_recognition as sr
import pyaudio
import cv2
import mediapipe as mp
import streamlit as st
import assemblyai as aai
from src.exception import CustomException
from src.logger import logging
from src.utils import load_model_nlp, load_model_cv
from src.data_transformation import  DataTransformation_test_data_nlp
from audio_recorder_streamlit import audio_recorder



class Prediction_NLP:
    def __init__(self):
        # Your API token is already set here
        aai.settings.api_key = 'e3f336e6533b41588c90f4dbf56c317c'

        # Create a transcriber object.
        self.transcriber = aai.Transcriber()

          # Initialize the data transformer
        self.data_transformer = DataTransformation_test_data_nlp()

    def AudioRecorderApp(self):
        audio_bytes = st.markdown('<h5 style='color: red;'>Start Audio Recording</h5>', unsafe_allow_html=True)
        audio_bytes = audio_recorder()

        if audio_bytes:
            st.audio(audio_bytes, format='audio/wav')

            # Save audio to a temporary file
            audio_file = 'audio.wav'
            with open(audio_file, 'wb') as f:
                f.write(audio_bytes)

            # Transcribe the audio file
            transcript = self.transcriber.transcribe(audio_file)
            transcribed_text = transcript.text
            st.markdown(f'<span style='font-weight: bold; color: #3333cc;'>Transcribed Text: </span>{transcribed_text}', unsafe_allow_html=True)

            # Perform emotion prediction only if recognized_text is not empty
            if transcribed_text:
                try:
                    class_names = {
                        0: 'sadness 🥺',
                        1: 'joy 😃',
                        2: 'love 🥰',
                        3: 'anger 😠',
                        4: 'fear 😨',
                        5: 'surprise 😵'
                    }
                    model_path = 'artifacts/Bi-LSTM.h5'
                    nlp_model_load = load_model_nlp(model_path)
                    
                    preprocessed_texts = self.data_transformer.returning_preprocessed_text(transcribed_text)
                    prediction = nlp_model_load.predict(preprocessed_texts)
                    predicted_class_index = prediction.argmax(axis=1)[0]
                    predicted_class_name = class_names[predicted_class_index]
                    st.markdown(f'<span style='font-weight: bold; color:#ff3300;'>Predicted sentiment: </span>{predicted_class_name}', unsafe_allow_html=True)
                    predicted_probabilities = prediction[0]
                    
                    
                    # Create a DataFrame with predicted sentiment and probabilities
                    result_df = pd.DataFrame({
                        'Sentiment': [class_names[class_index] for class_index in class_names],
                        'Probability': ['%.6f' % prob for prob in predicted_probabilities]
                    })

                    # Sort the DataFrame by probabilities in ascending order
                    result_df = result_df.sort_values(by='Probability', ascending=False)

                   
                    # Display the predicted sentiment and probabilities in a table
                    st.markdown('<h4 style='font-size: 16px; color: purple;'>Prediction Results:</h4>', unsafe_allow_html=True)
                    st.table(result_df)

                except Exception as e:
                    st.error('Error occurred during emotion prediction:')
                    st.error(e)



class Prediction_CV:
   
    def image_prediction(self, frame):
        try:
            emotion_dict = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}
            
            mp_face_mesh = mp.solutions.face_mesh
            # Create an instance of FaceMesh
            face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=10, min_detection_confidence=0.6)

            # Convert the frame to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Run face mesh on the frame using MediaPipe
            results = face_mesh.process(frame_rgb)
            
            # Check if any faces are detected
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    # Convert normalized landmarks to pixel coordinates
                    h, w, _ = frame.shape
                    landmarks = []
                    for landmark in face_landmarks.landmark:
                        x = int(landmark.x * w)
                        y = int(landmark.y * h)
                        landmarks.append((x, y))

                    # Approximate the bounding box of the face using landmarks
                    x, y, w, h = cv2.boundingRect(np.array(landmarks))

                    # Draw bounding box on the frame (in BGR format)
                    cv2.rectangle(frame, (x, y), (x + w, y + h), (11, 221, 221), 2)

                    # Extract the face region from the frame
                    face_img = frame[y:y+h, x:x+w]

                    # Check if the face region is empty or has a size of zero
                    if face_img.size == 0:
                        continue

                    # Convert the face image to grayscale
                    gray_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)

                    # Resize the grayscale image to match the expected input shape of the model
                    resized_img = cv2.resize(gray_img, (48, 48))

                    # Expand dimensions to create a batch dimension
                    input_img = np.expand_dims(resized_img, axis=-1)

                    # Normalize the input image
                    input_img = input_img / 255.0

                    # Load the saved model
                    model_path = 'artifacts/CNN_final.h5'
                    cv_model_load = load_model_cv(model_path)

                    # Predict the emotion
                    emotion_prediction = cv_model_load.predict(np.expand_dims(input_img, axis=0))
                    print(emotion_prediction)
                    maxindex = int(np.argmax(emotion_prediction))
                    emotion_label = emotion_dict[maxindex]
                    confidence_level = emotion_prediction[0][maxindex]
                    confidence_formatted = '{:.3f}'.format(confidence_level)
                    text = f'{emotion_label} ({confidence_formatted})'
                    cv2.putText(frame, text, (x + 5, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (58, 221, 55), 2, cv2.LINE_AA)                          
            return frame

        except Exception as e:
            logging.error('Error occurred while trying to capture the image')
            raise CustomException(e, sys)

        


================================================
File: src/utils.py
================================================
import os
import sys

import numpy as np 
import pandas as pd

from keras.callbacks import EarlyStopping
from keras.models import load_model


from src.exception import CustomException
from src.logger import logging

def model_training_nlp(X_train_final, y_train_final, validation_data,
                                epochs, batch_size, callback, model):
    try:
        logging.info('Model training has started...')

        history_nlp = model.fit(X_train_final, y_train_final, validation_data=validation_data, epochs=epochs,
                    batch_size=batch_size,callbacks=[callback])
        
        logging.info('Model training has completed...')

        return history_nlp
    


    except Exception as e:
        logging.error(f'Error occurred during model training: {e}')
        raise CustomException(e, sys)
    
def model_training_cv(train_generator_cv, steps_per_epoch, epochs,
                                validation_data, validation_steps, callback, model):
    try:
        logging.info('Model training has started...')

        history_cv = model.fit_generator(train_generator_cv, 
                            steps_per_epoch=steps_per_epoch,
                            epochs=epochs, 
                            validation_data=validation_data,
                            validation_steps=validation_steps,
                            callbacks=[callback])
        
        logging.info('Model training has completed...')

        return history_cv
    
    except Exception as e:
        logging.error(f'Error occurred during model training: {e}')
        raise CustomException(e, sys)
    


def save_model(file_path, obj):
    try:
        dir_path = os.path.dirname(file_path)
        os.makedirs(dir_path, exist_ok=True)
        obj.save(file_path)
        logging.info('Model has saved')

    except Exception as e:
        logging.error(f'Error occurred while saving the model: {e}')
        raise CustomException(e, sys)    
    
def load_model_nlp(file_path):

    try:
        model_nlp = load_model(file_path)
        logging.info('NLP Model loaded successfully')
        return model_nlp
    
    except Exception as e:
        logging.error(f'Error occurred while loading the NLP model: {e}')
        raise CustomException(e, sys)


def load_model_cv(file_path):

    try:
        model_cv = load_model(file_path)
        logging.info('CV Model loaded successfully')
        return model_cv
    
    except Exception as e:
        logging.error(f'Error occurred while loading the CV model: {e}')
        raise CustomException(e, sys)
        

================================================
File: src/components/data_ingestion.py
================================================
import os
import sys
from src.exception import CustomException
from src.logger import logging
import pandas as pd
import numpy as np
from src.components.data_transformation import  DataTransformation_nlp,DataTransformation_cv
from src.components.model_trainer import ModelTrainer_nlp, ModelTrainer_cv
from dataclasses import dataclass

@dataclass
class DataIngestionConfig:
    train_data_path: str = os.path.join('artifacts', 'train_nlp.csv')
    test_data_path: str = os.path.join('artifacts', 'test_nlp.csv')
    validation_data_path: str = os.path.join('artifacts', 'validation_nlp.csv')
    train_cv_folder_path: str = os.path.join('artifacts', 'train_cv')
    test_cv_folder_path: str = os.path.join('artifacts', 'test_cv')

class DataIngestion:
    def __init__(self):
        self.ingestion_config = DataIngestionConfig()

    def initiate_data_ingestion(self):
        logging.info('Entered the data ingestion method or component')
        try:
            # Create artifacts folder if it doesn't exist
            os.makedirs('artifacts', exist_ok=True)

            # Read the train_nlp.csv file and save it to artifacts folder
            df = pd.read_csv('notebook/data/training_nlp.csv')
            df.to_csv(self.ingestion_config.train_data_path, index=False, header=True)
            logging.info('Saved train_nlp.csv to artifacts folder')

            # Read the test_nlp.csv file and save it to artifacts folder
            df = pd.read_csv('notebook/data/test_nlp.csv')
            df.to_csv(self.ingestion_config.test_data_path, index=False, header=True)
            logging.info('Saved test_nlp.csv to artifacts folder')

            # Read the validation_nlp.csv file and save it to artifacts folder
            df = pd.read_csv('notebook/data/validation_nlp.csv')
            df.to_csv(self.ingestion_config.validation_data_path, index=False, header=True)
            logging.info('Saved validation_nlp.csv to artifacts folder')

            # Create train_cv and test_cv folders under artifacts folder
            os.makedirs(self.ingestion_config.train_cv_folder_path, exist_ok=True)
            os.makedirs(self.ingestion_config.test_cv_folder_path, exist_ok=True)
            logging.info('Created train_cv and test_cv folders under artifacts folder')

            # Move the subfolders and files from notebook/data/train_cv to artifacts/train_cv
            train_cv_path = os.path.join('notebook/data', 'train_cv')
            for folder_name in os.listdir(train_cv_path):
                folder_path = os.path.join(train_cv_path, folder_name)
                if os.path.isdir(folder_path):
                    target_folder_path = os.path.join(self.ingestion_config.train_cv_folder_path, folder_name)
                    os.makedirs(target_folder_path, exist_ok=True)
                    for file_name in os.listdir(folder_path):
                        file_path = os.path.join(folder_path, file_name)
                        target_file_path = os.path.join(target_folder_path, file_name)
                        os.rename(file_path, target_file_path)
                    logging.info(f'Moved subfolder '{folder_name}' from notebook/data/train_cv to artifacts/train_cv')

            # Move the subfolders and files from notebook/data/test_cv to artifacts/test_cv
            test_cv_path = os.path.join('notebook/data', 'test_cv')
            for folder_name in os.listdir(test_cv_path):
                folder_path = os.path.join(test_cv_path, folder_name)
                if os.path.isdir(folder_path):
                    target_folder_path = os.path.join(self.ingestion_config.test_cv_folder_path, folder_name)
                    os.makedirs(target_folder_path, exist_ok=True)
                    for file_name in os.listdir(folder_path):
                        file_path = os.path.join(folder_path, file_name)
                        target_file_path = os.path.join(target_folder_path, file_name)
                        os.rename(file_path, target_file_path)
                    logging.info(f'Moved subfolder '{folder_name}' from notebook/data/test_cv to artifacts/test_cv')

            logging.info('Ingestion of the data is completed')

            return (
                self.ingestion_config.train_data_path,
                self.ingestion_config.test_data_path,
                self.ingestion_config.validation_data_path,
                self.ingestion_config.train_cv_folder_path,
                self.ingestion_config.test_cv_folder_path
            )
        except Exception as e:
            raise CustomException(e, sys)


if __name__ == '__main__':
    obj = DataIngestion()
    train_data_path, test_data_path, validation_data_path, train_cv_folder_path, test_cv_folder_path = obj.initiate_data_ingestion()
    
    # Perform NLP data preprocessing
    #data_transformation_nlp = DataTransformation_nlp(train_data_path, test_data_path, validation_data_path)
    #train_data_nlp, train_labels_nlp, train_max_sequence_length_nlp, train_voc_size_nlp = data_transformation_nlp.preprocess_dataset(train_data_path)
    #test_data_nlp, test_labels_nlp, test_max_sequence_length_nlp, test_voc_size_nlp = data_transformation_nlp.preprocess_dataset(test_data_path)
    #validation_data_nlp, validation_labels_nlp, validation_max_sequence_length_nlp, validation_voc_size_nlp = data_transformation_nlp.preprocess_dataset(validation_data_path)
    #X_train_final,y_train_final= data_transformation_nlp.convert_to_arrays(train_data_nlp, train_labels_nlp)
    #X_valid_final,y_valid_final= data_transformation_nlp.convert_to_arrays(validation_data_nlp, validation_labels_nlp)
    #X_test_final,y_test_final= data_transformation_nlp.convert_to_arrays(test_data_nlp, test_labels_nlp)
    

    ## Perform Model Initialization on NLP
    #model_trainer_nlp = ModelTrainer_nlp(X_train_final,y_train_final,X_valid_final,y_valid_final,X_test_final,y_test_final,train_max_sequence_length_nlp, train_voc_size_nlp)
    #model_trainer_nlp.initiate_model_trainer_nlp()
    

    # Perform CV data preprocessing
    #data_transformation_cv = DataTransformation_cv(train_cv_folder_path, test_cv_folder_path)
    #train_generator_cv, test_generator_cv,input_shape = data_transformation_cv.preprocess_dataset()


    ## Perform Model Initialization on CV
    #model_trainer_cv = ModelTrainer_cv(train_generator_cv, test_generator_cv,input_shape)
    #model_trainer_cv.initiate_model_trainer_cv()


    



================================================
File: src/components/data_transformation.py
================================================
import sys
from dataclasses import dataclass
from src.exception import CustomException
from src.logger import logging

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import re
import nltk
import tensorflow.compat.v1 as tf
from tensorflow.keras.preprocessing.text import one_hot
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.preprocessing import image
from tensorflow.keras.layers import Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

class DataTransformation_nlp:
   def __init__(self, train_data_path, test_data_path, validation_data_path):
        self.train_data_path = train_data_path
        self.test_data_path = test_data_path
        self.validation_data_path = validation_data_path

        # Initialize the stop words
        self.stop_words = set(stopwords.words('english'))   

        # Initialize the PorterStemmer
        self.ps = PorterStemmer()

   def preprocess_text(self, texts):
        logging.info('Performing text preprocessing...')
        corpus = []
        for text in texts:
            review = re.sub('[^a-zA-Z]', ' ', text.lower())
            review = review.split()
            review = [self.ps.stem(word) for word in review if word not in self.stop_words]
            review = ' '.join(review)
            corpus.append(review)

        return corpus

   def preprocess_dataset(self, filename):
        try:
            logging.info('Loading dataset...')
            data = pd.read_csv(filename)
            texts = data['text'].values
            labels = data['label'].values

            # Preprocess the text data
            preprocessed_texts = self.preprocess_text(texts)

            # Convert text to sequences
            voc_size = 5000
            onehot_repr = [one_hot(words, voc_size) for words in preprocessed_texts]

            # Pad sequences for uniform length
            max_sequence_length = 200
            padded_sequences = pad_sequences(onehot_repr, padding='pre', maxlen=max_sequence_length)

            # Convert labels to one-hot encoded vectors
            num_classes = len(set(labels))
            labels = tf.keras.utils.to_categorical(labels, num_classes)

            logging.info('Data preprocessing completed.')

            return padded_sequences, labels, max_sequence_length, voc_size

        except Exception as e:
            logging.error(f'Error occurred during data preprocessing: {e}')
            raise CustomException(e, sys)

   def convert_to_arrays(self, padded_sequences, labels):
        try:
            logging.info('Converting texts and labels to arrays')
            padded_sequences_array = np.array(padded_sequences)
            labels_array = np.array(labels)
            logging.info('Conversion completed successfully')
            return padded_sequences_array, labels_array
        except Exception as e:
            logging.error(f'Error occurred during conversion to array format: {e}')
            raise CustomException(e, sys)
   

class DataTransformation_test_data_nlp:   
   def preprocess_text_for_prediction(self,text):
        try:
            # Preprocess the text
            processed_text = text.lower()
            processed_text = re.sub('[^a-zA-Z]', ' ', processed_text)
            processed_text = processed_text.split()
            ps = PorterStemmer()
            processed_text = [ps.stem(word) for word in processed_text if word not in stopwords.words('english')]
            processed_text = ' '.join(processed_text)

            return processed_text
        
        except Exception as e:
            raise CustomException(e, sys)
        

   def returning_preprocessed_text(self,result):
        try:
            preprocessed_text = self.preprocess_text_for_prediction(result)

            # Convert text to one-hot encoding
            voc_size = 5000
            onehot_repr = [one_hot(preprocessed_text, voc_size)]

            # Pad sequence for uniform length
            max_sequence_length = 200
            padded_sequence = pad_sequences(onehot_repr, padding='pre', maxlen=max_sequence_length)

            return padded_sequence
        
        except Exception as e:
            raise CustomException(e, sys)
       
       
class DataTransformation_cv:
    def __init__(self, train_dir, test_dir):
        self.train_dir = train_dir
        self.test_dir = test_dir
        self.img_width, self.img_height = 48, 48
        self.input_shape = (48, 48, 1)

    def preprocess_dataset(self):
        logging.info('Entered the CV data preprocessing method')
        try:
            # Set up the data generators
            train_datagen = ImageDataGenerator(rescale=1.0/255,
                                               shear_range=0.2,
                                               zoom_range=0.2,
                                               horizontal_flip=True)
            test_datagen = ImageDataGenerator(rescale=1.0/255)

            train_generator = train_datagen.flow_from_directory(self.train_dir,
                                                                target_size=(self.img_width, self.img_height),
                                                                batch_size=128,
                                                                color_mode='grayscale',
                                                                class_mode='categorical')

            test_generator = test_datagen.flow_from_directory(self.test_dir,
                                                              target_size=(self.img_width, self.img_height),
                                                              batch_size=128,
                                                              color_mode='grayscale',
                                                              class_mode='categorical')

            logging.info('CV data preprocessing completed')

            return (train_generator, test_generator, self.input_shape)
        except Exception as e:
            raise CustomException(e, sys)


================================================
File: src/components/model_trainer.py
================================================
import os
import sys
from dataclasses import dataclass
from src.exception import CustomException
from src.logger import logging
from src.components.data_transformation import DataTransformation_nlp, DataTransformation_cv
from src.utils import model_training_nlp, model_training_cv, save_model
import tensorflow.compat.v1 as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Conv2D, MaxPooling2D, Flatten
import tensorflow.keras.optimizers as optimizers
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

@dataclass
class ModelTrainerConfig_nlp:
    trained_model_file_path_nlp = os.path.join('artifacts', 'Bi-LSTM.h5')


class ModelTrainer_nlp:
    def __init__(self, X_train_final, y_train_final, X_valid_final, y_valid_final, X_test_final, y_test_final,
                 train_max_sequence_length_nlp, train_voc_size_nlp):
        self.model_trainer_config_nlp = ModelTrainerConfig_nlp()
        self.X_train_final = X_train_final
        self.y_train_final = y_train_final
        self.X_valid_final = X_valid_final
        self.y_valid_final = y_valid_final
        self.X_test_final = X_test_final
        self.y_test_final = y_test_final
        self.voc_size = train_voc_size_nlp
        self.max_sequence_length = train_max_sequence_length_nlp

    def initiate_model_trainer_nlp(self):
        logging.info('Model Trainer initialization started')
        try:
            embedding_vector_features = 300  # Feature representation
            model_nlp = Sequential()
            model_nlp.add(Embedding(self.voc_size, embedding_vector_features, input_length=self.max_sequence_length))
            model_nlp.add(Bidirectional(LSTM(512, dropout=0.3, return_sequences=True)))
            model_nlp.add(Bidirectional(LSTM(256, dropout=0.3, return_sequences=True)))
            model_nlp.add(Bidirectional(LSTM(128, dropout=0.3)))
            model_nlp.add(Dense(6, activation='sigmoid'))

            optimizer = optimizers.Adam(lr=0.005)
            model_nlp.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

            model_nlp.summary()
            logging.info('Model initialized successfully')

            # Define the callback
            callback = EarlyStopping(monitor='val_loss', patience=4)

            # Call the model_training_nlp function from utils.py
            history_nlp = model_training_nlp(X_train=self.X_train_final, y_train=self.y_train_final,
                                             validation_data=(self.X_valid_final, self.y_valid_final),
                                             epochs=1, batch_size=128, callback=callback, model=model_nlp)

            save_model(
                file_path=self.model_trainer_config_nlp.trained_model_file_path_nlp,
                obj=model_nlp)

            return history_nlp

        except Exception as e:
            logging.error(f'Error occurred during model training: {e}')
            raise CustomException(e, sys)


@dataclass
class ModelTrainerConfig_cv:
    trained_model_file_path_cv = os.path.join('artifacts', 'CNN_final.h5')


class ModelTrainer_cv:
    def __init__(self, train_generator_cv, test_generator_cv, input_shape):
        self.model_trainer_config_cv = ModelTrainerConfig_cv()
        self.train_generator_cv = train_generator_cv
        self.test_generator_cv = test_generator_cv
        self.input_shape = input_shape

    def initiate_model_trainer_cv(self):
        logging.info('Model Trainer initialization started')
        try:
            model_cv = Sequential()
            # Convolutional layers
            model_cv.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))
            model_cv.add(MaxPooling2D(pool_size=(2, 2)))
            model_cv.add(Dropout(0.3))

            model_cv.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))
            model_cv.add(MaxPooling2D(pool_size=(2, 2)))
            model_cv.add(Dropout(0.3))

            model_cv.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))
            model_cv.add(MaxPooling2D(pool_size=(2, 2)))
            model_cv.add(Dropout(0.3))

            model_cv.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))
            model_cv.add(MaxPooling2D(pool_size=(2, 2)))
            model_cv.add(Dropout(0.3))

            model_cv.add(Flatten())
            # Fully connected layers
            model_cv.add(Dense(512, activation='relu'))
            model_cv.add(Dropout(0.3))
            model_cv.add(Dense(512, activation='relu'))
            model_cv.add(Dropout(0.3))
            model_cv.add(Dense(256, activation='relu'))
            model_cv.add(Dropout(0.3))

            # Output layer
            model_cv.add(Dense(7, activation='softmax'))

            optimizer = optimizers.Adam(lr=0.0005)
            model_cv.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
            print(model_cv.summary())
            logging.info('Model initialized successfully')

            callback =EarlyStopping(monitor='val_acc', patience=10)

            # Call the model_training_cv function from utils.py
            history_cv = model_training_cv(train_generator_cv=self.train_generator_cv,
                                           steps_per_epoch=self.train_generator_cv.samples // self.train_generator_cv.batch_size,
                                           epochs=1,
                                           validation_data=self.test_generator_cv,
                                           validation_steps=self.test_generator_cv.samples // self.test_generator_cv.batch_size,
                                           callback=callback,
                                           model=model_cv)

            save_model(
                file_path=self.model_trainer_config_cv.trained_model_file_path_cv,
                obj=model_cv)

            return history_cv

        except Exception as e:
            logging.error(f'Error occurred during model training: {e}')
            raise CustomException(e, sys)


================================================
File: src/pipeline/predict_pipeline.py
================================================
import sys
import pandas as pd
import speech_recognition as sr
from src.exception import CustomException
from src.logger import logging
from src.utils import load_model_nlp, load_model_cv
from src.components.data_transformation import  DataTransformation_test_data_nlp
import cv2
import mediapipe as mp
import numpy as np



class Prediction_NLP:
       
   def audio_recording_prediction(self):
        try:
          # Set the initial flag value
          record_flag = False

          # Set up speech recognition
          recognizer = sr.Recognizer()

          # Specify the device index for the desired microphone
          device_index = 1  # Replace with the appropriate index for your microphone

          while True:
                with sr.Microphone(device_index=device_index) as source:
                    print('Listening...')
                    audio = recognizer.listen(source)
                
                try:
                    # Perform speech recognition on the audio
                    result = recognizer.recognize_google(audio)
                    print('Recognized speech:', result)
                    class_names = {
                                0: 'sadness',
                                1: 'joy',
                                2: 'love',
                                3: 'anger',
                                4: 'fear',
                                5: 'surprise'
                            }
                    model_path= 'artifacts\Bi-LSTM.h5'
                    nlp_model_load = load_model_nlp(model_path)
                    data_transformer = DataTransformation_test_data_nlp() 
                    preprocessed_texts = data_transformer.returning_preprocessed_text(result) 
                    prediction = nlp_model_load.predict(preprocessed_texts)
                    predicted_class_index = prediction.argmax(axis=1)[0]
                    predicted_class_name = class_names[predicted_class_index]
                    predicted_probabilities = prediction[0]
                    #print(predicted_class_name, predicted_probabilities, class_names, result)
                    return predicted_class_name, predicted_probabilities,class_names,result 



                except sr.UnknownValueError:
                    print('No speech detected.')

        except KeyboardInterrupt:
            print('Listening interrupted.')  
             
       

class Prediction_CV:
   
    def image_prediction(self, frame):
        try:
            emotion_dict = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}
            
            mp_face_mesh = mp.solutions.face_mesh
            # Create an instance of FaceMesh
            face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=10, min_detection_confidence=0.6)

            # Convert the frame to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Run face mesh on the frame using MediaPipe
            results = face_mesh.process(frame_rgb)
            
            # Check if any faces are detected
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    # Convert normalized landmarks to pixel coordinates
                    h, w, _ = frame.shape
                    landmarks = []
                    for landmark in face_landmarks.landmark:
                        x = int(landmark.x * w)
                        y = int(landmark.y * h)
                        landmarks.append((x, y))

                    # Approximate the bounding box of the face using landmarks
                    x, y, w, h = cv2.boundingRect(np.array(landmarks))

                    # Draw bounding box on the frame (in BGR format)
                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 4)

                    # Extract the face region from the frame
                    face_img = frame[y:y+h, x:x+w]

                    # Check if the face region is empty or has a size of zero
                    if face_img.size == 0:
                        continue

                    # Convert the face image to grayscale
                    gray_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)

                    # Resize the grayscale image to match the expected input shape of the model
                    resized_img = cv2.resize(gray_img, (48, 48))

                    # Expand dimensions to create a batch dimension
                    input_img = np.expand_dims(resized_img, axis=-1)

                    # Normalize the input image
                    input_img = input_img / 255.0

                    # Load the saved model
                    model_path = 'artifacts/CNN_final.h5'
                    cv_model_load = load_model_cv(model_path)

                    # Predict the emotion
                    emotion_prediction = cv_model_load.predict(np.expand_dims(input_img, axis=0))
                    maxindex = int(np.argmax(emotion_prediction))
                    cv2.putText(frame, emotion_dict[maxindex], (x + 5, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1,
                                (255, 0, 0), 2, cv2.LINE_AA)

            return frame

        except Exception as e:
            logging.error('Error occurred while trying to capture the image')
            raise CustomException(e, sys)

        


================================================
File: .streamlit/config.toml
================================================
[server]
port = 80

[browser]
serverPort = 80


[theme]
primaryColor='#8533ff'
backgroundColor='#e8f8ff'
secondaryBackgroundColor='#F0F2F6'












================================================
File: .streamlit/secrets.toml
================================================
email=''


        """,
        "repo_created_at": "2025-02-12T13:09:25Z",
        "last_updated": "2025-02-13T00:33:37Z",
        "stars": 0
    },
    "Parthiban-3997": {
        "github_url": "https://github.com/Parthiban-3997/Parthiban-3997",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Parthiban-3997",
        "description": """
        ================================================
File: README.md
================================================
👋 Hi there, I’m Parthiban Ravichandran, a passionate Data Scientist with a strong academic background and a thirst for solving real-world problems using data-driven solutions.

🎓 I recently completed my Master's degree in Data Science from the University of Central Lancashire, where I honed my skills in statistics, machine learning, deep learning, and computer vision.

💬 My motive is to give my full potential to build AI-based solutions that solve high-impact problems for people around the globe and simplify everyday living.

💞️ I’m looking to collaborate on Data Science Projects

📫 You can follow me on LinkedIn: https://www.linkedin.com/in/parthiban-ravichandran/

🔬 My technical skills include:
- Languages: Python
- Libraries: Numpy, Pandas, Matplotlib, Seaborn, Scikit-learn, Tensorflow, Keras, NLTK, Mediapipe
- Frameworks: Flask, Streamlit
- Databases: MySQL, MongoDB
- Cloud Platforms: Heroku, Microsoft Azure
- DevOps: Docker, CI/CD Pipeline, GitHub Actions
- Tech Stack: Machine and Deep Learning Algorithms, RNN, OpenCV, Transfer Learning Techniques, OpenAI, LangChain, Statistics

🌱 I'm actively seeking full-time roles related to Data Science, where I can apply my knowledge and expertise to tackle complex challenges.

🤝 I'm eager to collaborate on Data Science projects and contribute to innovative solutions. If you're looking for a Data Scientist who is well-versed in the latest technologies and dedicated to making an impact, let's connect!

💬 For collaborating don't hesitate to reach me on rparthiban729@gmail.com

<!---
Parthiban-3997/Parthiban-3997 is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->



        """,
        "repo_created_at": "2025-02-12T13:09:47Z",
        "last_updated": "2025-02-12T13:19:20Z",
        "stars": 0
    },
    "Real_Time_Based_News_Letter_Generation_Using_CrewAI": {
        "github_url": "https://github.com/Parthiban-3997/Real_Time_Based_News_Letter_Generation_Using_CrewAI",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Real_Time_Based_News_Letter_Generation_Using_CrewAI",
        "description": """
        ================================================
File: README.md
================================================
## AI-Powered Real-Time Newsletter Generation

Welcome to the NewsletterGen Crew project, powered by [crewAI](https://crewai.com). This template is designed to help you set up a multi-agent AI system with ease, leveraging the powerful and flexible framework provided by crewAI. The goal is to enable agents to collaborate effectively on complex tasks, maximizing their collective intelligence and capabilities.

This project is a Streamlit application that leverages AI agents to generate customized newsletters based on real-time news data. It uses the CrewAI framework to coordinate a team of AI agents that research, edit, and design newsletters on specified topics.

## Deployed Link

Newsletter Generation App is Deployed And Available [Here](https://realtimenewslettergeneration.streamlit.app/)


## Screenshots

![crew_1](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/187984bf-a3fc-49cd-9b20-841629baaa12)
![crew_2](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/5419b9d9-2558-49e0-a4c4-e95eaab78b0f)
![crew_3](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/05aabb8b-2f64-4caa-a174-0a8dd7f1ab69)
![crew_4](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/b550c8fe-518a-48ef-9d3b-0cbd998df00a)
![crew_5](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/fd342956-f0cf-490c-8090-0727f877c77a)
![crew_6](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/edcde6df-03fe-4928-8ebb-2d7a806aa592)
![crew_7](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/ea7e67c4-d69c-4075-b2c9-49e1f2d7b69b)


## Key Features
- **Real-Time News Integration**: Utilizes the Exa API to fetch the latest news articles on specified topics based on semantic meaning and not by keyword search.
- **AI-Driven Content Creation**: Employs a team of AI agents to research, curate, and edit content for the newsletter.
- **Customizable Topics**: Users can input any topic of interest to generate a relevant newsletter.
- **Date Range Selection**: Allows users to specify a start date for news searches, ensuring up-to-date content.
- **Personalized Messaging**: Includes an option to add a custom personal message at the top of the newsletter.
- **HTML Newsletter Generation**: Produces a fully formatted HTML newsletter ready for distribution.
- **One-Click Download**: Enables users to download the generated newsletter as an HTML file.


## Project Uniqueness
This project stands out due to its innovative approach to newsletter creation:

- **AI Agent Collaboration**: Utilizes the CrewAI framework to orchestrate a team of AI agents, each with specialized roles in the newsletter creation process.
- **Real-Time Data Integration**: Incorporates the latest news by leveraging the Exa API, ensuring newsletters are always current and relevant.
- **Flexible Content Generation**: Adapts to any topic specified by the user, making it versatile for various industries and interests.
- **Streamlined User Experience**: Offers a simple, intuitive interface for users to generate complex, AI-crafted newsletters with minimal input.
- **Scalable Architecture**: Designed to handle multiple simultaneous requests and can be easily extended to include additional features or data sources.


## How It Works
User Input: Users specify a topic, personal message, and start date for news search.

- **Research Agent**: Searches for and analyzes recent news articles on the specified topic.
- **Editor Agent**: Curates and refines the content, ensuring relevance and coherence.
- **Designer Agent**: Formats the content into an attractive HTML newsletter template.
- **Output**: Users can preview and download the generated newsletter as an HTML file.


## Impact
This tool revolutionizes the newsletter creation process by:

Reducing the time and effort required to create high-quality, topical newsletters.
Ensuring content is always fresh and relevant, improving reader engagement.
Allowing for rapid creation of newsletters on any topic, enhancing responsiveness to current events.
Providing a scalable solution for businesses and individuals who need to produce regular, customized newsletters.
By automating the research, writing, and design processes, this application empowers users to create professional-grade newsletters without the need for a dedicated content team or extensive time investment.


Consider the following diagram to understand how multi agents are built:

![Agent Architecture](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/21e0998b-4d39-4ac2-b47b-cc332bdbca2d)


## Installation

Ensure you have Python >=3.10 <=3.13 installed on your system. This project uses [Poetry](https://python-poetry.org/) for dependency management and package handling, offering a seamless setup and execution experience.

First, if you haven't already, install Poetry:

```bash
pip install poetry
```

Next, navigate to your project directory and install the dependencies:

1. First lock the dependencies and then install them:
```bash
poetry lock
```
```bash
poetry install
```
### Customizing

**Add your  `ANTHROPIC_API_KEY` into the `.env` file**

- Modify `src/newsletter_gen/config/agents.yaml` to define your agents
- Modify `src/newsletter_gen/config/tasks.yaml` to define your tasks
- Modify `src/newsletter_gen/crew.py` to add your own logic, tools and specific args
- Modify `src/newsletter_gen/main.py` to add custom inputs for your agents and tasks

## Running the Project

To kickstart your crew of AI agents and begin task execution, run this from the root folder of your project:

```bash
poetry run newsletter_gen
```

This command initializes the newsletter-gen Crew, assembling the agents and assigning them tasks as defined in your configuration.


## Understanding Your Crew

The newsletter-gen Crew is composed of multiple AI agents, each with unique roles, goals, and tools. These agents collaborate on a series of tasks, defined in `config/tasks.yaml`, leveraging their collective skills to achieve complex objectives. The `config/agents.yaml` file outlines the capabilities and configurations of each agent in your crew.


## Contributing

Contributions to this project are welcome! If you have any ideas, bug fixes, or improvements, feel free to submit a pull request. Please ensure that your code adheres to the project's coding standards and is well-documented.

## License

This project is licensed under the [MIT License](LICENSE).


================================================
File: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      'License' shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      'Licensor' shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      'Legal Entity' shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      'control' means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      'You' (or 'Your') shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      'Source' form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      'Object' form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      'Work' shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      'Derivative Works' shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      'Contribution' shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, 'submitted'
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as 'Not a Contribution.'

      'Contributor' shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a 'NOTICE' text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an 'AS IS' BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets '[]'
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same 'printed page' as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the 'License');
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an 'AS IS' BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


================================================
File: app.py
================================================
import sys
import subprocess
import pkg_resources

# Install pysqlite3-binary if not present
required = {'pysqlite3-binary'}
installed = {pkg.key for pkg in pkg_resources.working_set}
missing = required - installed

if missing:
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])

__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import streamlit as st
from datetime import datetime, timedelta
from src.newsletter_gen.crew import NewsletterGenCrew
from crewai.crews.crew_output import CrewOutput

class NewsletterGenUI:

    def load_html_template(self):
        with open('src/newsletter_gen/config/newsletter_template.html', 'r') as file:
            html_template = file.read()
        return html_template

    def generate_newsletter(self, topic, personal_message, start_date):
        end_date = datetime.now()

        st.write(f'Generating newsletter starting from {start_date}.')
        st.write(f'End date: {end_date.strftime('%Y-%m-%d')}')
        
        inputs = {
            'topic': topic,
            'personal_message': personal_message,
            'html_template': self.load_html_template(),
            'start_date': start_date,
            'end_date': end_date.strftime('%Y-%m-%d'),
        }
        crew_output = NewsletterGenCrew().crew().kickoff(inputs=inputs)
        
        # Extract content from CrewOutput
        if isinstance(crew_output, CrewOutput):
            # Inspect the structure of crew_output
            st.write('CrewOutput structure:')
            st.write(crew_output)
            
            # Try to access the content (adjust this based on the actual structure)
            if hasattr(crew_output, 'content'):
                return crew_output.content
            elif hasattr(crew_output, 'output'):
                return crew_output.output
            else:
                # If we can't find the content, return the string representation
                return str(crew_output)
        
        return str(crew_output)  # Fallback to string representation if not CrewOutput

    def newsletter_generation(self):
        if st.session_state.generating:
            newsletter_content = self.generate_newsletter(
                st.session_state.topic, st.session_state.personal_message, st.session_state.start_date
            )
            st.session_state.newsletter = newsletter_content

        if st.session_state.newsletter and st.session_state.newsletter != '':
            with st.container():
                st.write('Newsletter generated successfully!')
                st.markdown(st.session_state.newsletter, unsafe_allow_html=True)
                
                current_date = datetime.now().strftime('%Y-%m-%d')
                file_name = f'{st.session_state.topic}_newsletter_{current_date}.html'
                st.download_button(
                    label='Download HTML file',
                    data=st.session_state.newsletter,
                    file_name=file_name,
                    mime='text/html',
                )
            st.session_state.generating = False


    def sidebar(self):
        with st.sidebar:
            st.title('Newsletter Generator')

            st.write(
                '''
                To generate a newsletter, enter a topic, a personal message, and the start publish date for the news search. \n
                Your team of AI agents will generate a newsletter for you!
                '''
            )

            st.text_input('Topic', key='topic', placeholder='World News')

            st.text_area(
                'Your personal message (to include at the top of the newsletter)',
                key='personal_message',
                placeholder='Dear News Reader,',
            )

            # Add a date input for the start publish date
            st.date_input('Start publish date for news search', key='start_date', value=datetime.now() - timedelta(days=7))

            if st.button('Generate Newsletter'):
                st.session_state.generating = True

    def render(self):
        st.set_page_config(page_title='Newsletter Generation', page_icon='📧')

        if 'topic' not in st.session_state:
            st.session_state.topic = ''

        if 'personal_message' not in st.session_state:
            st.session_state.personal_message = ''

        if 'newsletter' not in st.session_state:
            st.session_state.newsletter = ''

        if 'generating' not in st.session_state:
            st.session_state.generating = False

        if 'start_date' not in st.session_state:
            st.session_state.start_date = datetime.now() - timedelta(days=7)  # Default to 7 days ago

        self.sidebar()
        self.newsletter_generation()


if __name__ == '__main__':
    NewsletterGenUI().render()

================================================
File: packages.txt
================================================
sqlite3 

================================================
File: pyproject.toml
================================================
[tool.poetry]
name = 'newsletter_gen'
version = '0.1.0'
description = 'newsletter-gen using crewAI'
authors = ['Your Name <you@example.com>']

[tool.poetry.dependencies]
python = '>=3.10,<=3.13'
crewai = {extras = ['tools'], version = '^0.28.7'}
exa-py = '^1.0.9'
langchain = '0.1.17'
langchain-core = '0.1.52'
langchain-anthropic = '^0.1.11'
streamlit = '^1.34.0'
pillow = '^10.3.0'

[tool.poetry.scripts]
newsletter_gen = 'newsletter_gen.main:run'

[build-system]
requires = ['poetry-core']
build-backend = 'poetry.core.masonry.api'

================================================
File: requirements.txt
================================================
crewai>=0.11.0
crewai-tools>=0.0.9
langchain-anthropic
langchain-openai
langchain-google-genai
langchain-community
exa_py
langtrace-python-sdk
pysqlite3-binary
chromadb



================================================
File: src/newsletter_gen/crew.py
================================================
import os
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from src.newsletter_gen.tools.research import SearchAndContents, FindSimilar, GetContents
from datetime import datetime, date
import streamlit as st
from typing import Union, List, Tuple, Dict
from langchain_core.agents import AgentFinish
import json
from langchain_google_genai import ChatGoogleGenerativeAI
import langtrace_python_sdk.instrumentation.crewai.patch as langtrace_patch
from langtrace_python_sdk import langtrace


from dotenv import load_dotenv
load_dotenv()

# Custom JSON encoder
class DateTimeEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (date, datetime)):
            return obj.isoformat()
        return super().default(obj)

# Monkey patch json.dumps in langtrace_python_sdk
original_json_dumps = json.dumps

def custom_json_dumps(*args, **kwargs):
    kwargs['cls'] = DateTimeEncoder
    return original_json_dumps(*args, **kwargs)

langtrace_patch.json.dumps = custom_json_dumps

# Initialize langtrace with custom JSON encoder
langtrace.init(api_key=os.getenv('LANGTRACE_API_KEY'))

@CrewBase
class NewsletterGenCrew:
    '''NewsletterGen crew'''

    agents_config = 'config/agents.yaml'
    tasks_config = 'config/tasks.yaml'

    def llm(self):
        return ChatGoogleGenerativeAI(model='gemini-1.5-pro')

    def step_callback(
        self,
        agent_output: Union[str, List[Tuple[Dict, str]], AgentFinish],
        agent_name,
        *args,
    ):
        with st.chat_message('AI'):
            if isinstance(agent_output, str):
                try:
                    agent_output = json.loads(agent_output)
                except json.JSONDecodeError:
                    pass

            if isinstance(agent_output, list) and all(
                isinstance(item, tuple) for item in agent_output
            ):
                for action, description in agent_output:
                    st.write(f'Agent Name: {agent_name}')
                    st.write(f'Tool used: {getattr(action, 'tool', 'Unknown')}')
                    st.write(f'Tool input: {getattr(action, 'tool_input', 'Unknown')}')
                    st.write(f'{getattr(action, 'log', 'Unknown')}')
                    with st.expander('Show observation'):
                        st.markdown(f'Observation\n\n{description}')

            elif isinstance(agent_output, AgentFinish):
                st.write(f'Agent Name: {agent_name}')
                output = agent_output.return_values
                st.write(f'I finished my task:\n{output['output']}')

            else:
                st.write(type(agent_output))
                st.write(agent_output)

    @agent
    def researcher(self) -> Agent:
        return Agent(
            config=self.agents_config['researcher'],
            tools=[SearchAndContents(), FindSimilar(), GetContents()],
            verbose=True,
            llm=self.llm(),
            step_callback=lambda step: self.step_callback(step, 'Research Agent'),
        )

    @agent
    def editor(self) -> Agent:
        return Agent(
            config=self.agents_config['editor'],
            verbose=True,
            tools=[SearchAndContents(), FindSimilar(), GetContents()],
            llm=self.llm(),
            step_callback=lambda step: self.step_callback(step, 'Chief Editor'),
        )

    @agent
    def designer(self) -> Agent:
        return Agent(
            config=self.agents_config['designer'],
            verbose=True,
            allow_delegation=False,
            llm=self.llm(),
            step_callback=lambda step: self.step_callback(step, 'HTML Writer'),
        )

    @task
    def research_task(self) -> Task:
        return Task(
            config=self.tasks_config['research_task'],
            agent=self.researcher(),
            output_file=f'logs/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_research_task.md',
        )

    @task
    def edit_task(self) -> Task:
        return Task(
            config=self.tasks_config['edit_task'],
            agent=self.editor(),
            output_file=f'logs/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_edit_task.md',
        )

    @task
    def newsletter_task(self) -> Task:
        return Task(
            config=self.tasks_config['newsletter_task'],
            agent=self.designer(),
            output_file=f'logs/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_newsletter_task.html',
        )

    @crew
    def crew(self) -> Crew:
        '''Creates the NewsletterGen crew'''
        return Crew(
            agents=self.agents,
            tasks=self.tasks,
            process=Process.sequential,
            verbose=2,
        )

class NewsletterGenUI:
    def generate_newsletter(self, **inputs):
        return NewsletterGenCrew().crew().kickoff(inputs=inputs)

    def newsletter_generation(self):
        topic = st.text_input('Enter a topic for the newsletter:')
        if st.button('Generate Newsletter'):
            with st.spinner('Generating newsletter...'):
                st.session_state.newsletter = self.generate_newsletter(topic=topic)
            st.success('Newsletter generated!')

    def render(self):
        st.title('Newsletter Generator')
        self.newsletter_generation()
        if 'newsletter' in st.session_state:
            st.subheader('Generated Newsletter')
            st.markdown(st.session_state.newsletter, unsafe_allow_html=True)

if __name__ == '__main__':
    NewsletterGenUI().render()

================================================
File: src/newsletter_gen/main.py
================================================
#!/usr/bin/env python
from newsletter_gen.crew import NewsletterGenCrew

def load_html_template(): 
    with open('src/newsletter_gen/config/newsletter_template.html', 'r') as file:
        html_template = file.read()
        
    return html_template


def run():
    # Replace with your inputs, it will automatically interpolate any tasks and agents information
    inputs = {
        'topic': input('Enter the topic for yout newsletter: '),
        'personal_message': input('Enter a personal message for your newsletter: '),
        'html_template': load_html_template()
    }
    NewsletterGenCrew().crew().kickoff(inputs=inputs)

================================================
File: src/newsletter_gen/config/agents.yaml
================================================
researcher:
  role: >
    Senior Researcher
  goal: >
    Uncover cutting-edge developments in {topic} within the specified date range.
  backstory: >
    You're a seasoned journalist with a nose for news. You're known for your great research skills and ability to dig up the most interesting stories within specific timeframes. Your reports are always thorough and well-researched, making you a trusted source of information.
    You always follow the rules and guidelines provided to you, never forget to include the complete URL of the article where you found the news, and always ensure that the articles are within the specified date range.

editor:
  role: >
    Editor-in-Chief
  goal: >
    Ensure the quality, accuracy, and timeliness of the final newsletter.
  backstory: >
    You are the Editor-in-Chief of a prestigious news organization. You are responsible for overseeing the production of the newsletter and ensuring that it meets the highest standards of quality, that it is accurate, well-written, engaging, and focuses on the most recent and relevant news within the specified timeframe. 

    You review the news articles provided by the researcher, add context to each article (like why the news story is relevant), and have a great sense of what will resonate with the readers. You use this sense of judgment to reorder the news articles in a way that the most important and recent news is at the top of the list.

designer:
  role: >
    Newsletter Compiler
  goal: >
    Fill the HTML template given to you with the news articles provided, ensuring all date-related information is included.
  backstory: >
    You are responsible for compiling the HTML code of the newsletter, making sure that every news article is included in the final document, along with its publication date.
    You do NOT modify the content and only update the design when necessary. You use the HTML template provided to you to create the newsletter, always including the date information for each article.

analyst:
  role: >
    {topic} Reporting Analyst
  goal: >
    Create detailed reports based on {topic} data analysis and research findings, focusing on the specified date range.
  backstory: >
    You're a meticulous analyst with a keen eye for detail and a strong understanding of time-sensitive information. You're known for your ability to turn complex data into clear and concise reports, making it easy for others to understand and act on the information you provide. You always ensure that your analysis is based on the most recent and relevant data within the specified timeframe.

================================================
File: src/newsletter_gen/config/newsletter_template.html
================================================
<!DOCTYPE html>
<html lang='en'>
<head>
    <meta charset='UTF-8'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0'>
    <title>Weekly Newsletter</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #e8f4f8;
            color: #333;
        }
        .container {
            width: 90%;
            max-width: 1200px;
            margin: auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        h1 {
            text-align: center;
            margin-bottom: 20px;
            color: #004d66;
        }
        h2 {
            margin: 0 0 10px 0;
            color: #006680;
        }
        p {
            line-height: 1.6;
        }
        .grid-container {
            display: grid;
            grid-template-columns: 1fr;
            gap: 20px;
        }
        .grid-item {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            display: flex;
            flex-direction: column;
        }
        .grid-item img {
            width: 100%;
            border-radius: 8px;
            margin-bottom: 15px;
        }
        .grid-item:nth-child(odd) {
            background-color: #eef6f9;
        }
        .grid-item:nth-child(even) {
            background-color: #e8f4f8;
        }
        .grid-item:hover {
            background-color: #d1ecf3;
            transition: background-color 0.3s ease;
        }
        .grid-item h2 {
            color: #004d66;
        }
        .grid-item p {
            margin-bottom: 10px;
        }
        .read-more {
            margin-top: 10px;
            font-size: 14px;
        }
        .read-more a {
            color: #0066cc;
            text-decoration: none;
        }
        .read-more a:hover {
            text-decoration: underline;
        }
        .goodbye {
            text-align: center;
            margin-top: 30px;
            color: #666;
        }
        button {
            background-color: #0066cc;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s ease;
            align-self: flex-start;
        }
        button:hover {
            background-color: #004c99;
        }

        @media (min-width: 768px) {
            .grid-container {
                grid-template-columns: 1fr 1fr;
            }
        }

        @media (min-width: 1200px) {
            .grid-container {
                grid-template-columns: 1fr 1fr 1fr;
            }
        }
    </style>
</head>
<body>
    <div class='container'>
        <h1><!-- TITLE OF THE DAY --></h1>
        <p><!-- PERSONAL MESSAGE --></p>
        <div class='grid-container'>
            <!-- Example Grid Item -->
            <div class='grid-item'>
                <h2>1. <!-- TITLE OF THE STORY --></h2>
                <p><!-- SUMMARY OF THE STORY --></p>
                <p><!-- WHY THIS IS IMPORTANT --></p>
                <button onclick='window.open('https://www.example.com', '_blank')'>Read Full Story</button>
            </div>
            <!-- ADD MORE GRID ITEMS HERE FOLLOWING THE SAME FORMAT -->
        </div>
        <p class='goodbye'>Goodbye and see you until we meet!</p>
    </div>
</body>
</html>


================================================
File: src/newsletter_gen/config/tasks.yaml
================================================
research_task:
  description: >
    Conduct a thorough research about the latest news on {topic}, focusing on articles published between {start_date} and the current date. Be sure to look for sources that are reliable and publish recent news. Do not include articles that are not news material or that are not directly related to {topic}.
    With this research, compile a list of the most relevant news stories that you found. 

    Follow these rules:
    - Only include articles that are especially relevant to {topic} and published within the specified date range.
    - Do not include any news that are not directly related to {topic} or outside the date range.
    - Do not include sources that are not a news article. If the content of the page includes a list of articles or looks like the front page of a website, do not include it in the list!
    - Summarize the news in a few sentences. Make the summary as long as necessary to include all the relevant information, but not too long for a newsletter.
    - Include the URL of the article where you found the news.
    - Include a minimum of 5 news articles and a maximum of 10 news articles in the list.
    - When using the Search Tool, your search query should be concise and include the date range (for example, 'latest news on {topic} from {start_date} to {end_date}').

    IMPORTANT INSTRUCTIONS ABOUT USING TOOLS: When using tools, DO NOT ESCAPE the underscore character '_', EVER. If you need to use a tool and pass in a parameter called 'search_query', you should write 'search_query', not 'search\_query'. THIS IS VERY IMPORTANT, else the tool will not work.

  expected_output: >
    A markdown document with the most relevant news stories. Each news story should contain the following:
    - Title of the news
    - Summary of the news
    - URL of the article where the news was found
    - Publication date of the article

    Here is an example of the format of a news article that you could include in the document:
    
    <EXAMPLE>
      Story 1:
      - Title: **Daily briefing: AI now beats humans at basic reading and maths**
      - **Summary:** AI systems can now nearly match and sometimes exceed human performance in basic tasks. The report discusses the need for new benchmarks to assess AI capabilities and highlights the ethical considerations for AI models.
      - **URL:** [Nature Article](https://www.nature.com/articles/d41586-024-01125-1)
      - **Published:** 2024-04-18
    </EXAMPLE>

edit_task:
  description: >
    Given the list of news articles that will be used in the newsletter, do the following things:
    
    - Rewrite the title of each news article to make it more engaging and interesting for the readers of the newsletter.
    - Add a paragraph to each news article that explains why this news is important and how it can impact the readers of the newsletter.
    - Reorder the bullet points in a way that the most relevant news and topics are at the top of the list based on the importance of the news and topics.
    - Verify that the news articles are directly related to {topic} and that they are not off-topic. If they are off-topic, remove them from the list.
    - Verify that the URLs are correct and that they lead to the correct news article. They should lead to a news article and not to a list of articles or the front page of a website. If the URL is incorrect, ask the researcher to provide the correct URL.
    - Ensure that all articles are within the specified date range of {start_date} to the current date.
    - Do not search for additional news articles or change the content of the news articles. Only edit the existing news articles.

    IMPORTANT INSTRUCTIONS ABOUT USING TOOLS: When using tools, DO NOT ESCAPE the underscore character '_', EVER. If you need to use a tool and pass in a parameter called 'search_query', you should write 'search_query', not 'search\_query'. THIS IS VERY IMPORTANT, else the tool will not work.

  expected_output: >
    A markdown document with all the news to be included in the newsletter of the week. The document should have a title related to the curated stories of the week and a list of news articles.
    
    Each one should contain:
    - Title:
    - Summary: 
    - Why this is important:
    - Source: [URL of the article]
    - Published: [Date of publication]

    Here is an example of a document that you are expected to produce:
    <EXAMPLE>
    Title of the day: AI is taking over the world

      - **Title:** AI Surpasses Human Capabilities in Basic Reading and Maths
        **Summary:** Recent advancements in AI technology have enabled systems to match and sometimes exceed human performance in fundamental tasks such as reading and arithmetic. This shift necessitates the creation of new benchmarks to appropriately evaluate AI capabilities.
        **Why this is important:** This development is crucial as it not only showcases the rapid progress in AI but also prompts a reevaluation of how AI is integrated into educational and professional settings. It raises significant ethical questions about the future role of AI in society.
        **Source:** 'Title of the article' by ['Source'](https://www.nature.com/articles/d41586-024-01125-1)
        **Published:** 2024-04-18

        [... more news articles ...]

    </EXAMPLE>

newsletter_task:
  description: >
    Fill the following HTML template with exactly the same information that is given to you. Also, include the following personal message at the beginning of the newsletter in the space provided: {personal_message}

    (If there is no personal message, leave the space empty.)
    
    Follow these rules:
    - Do not truncate the information or change the order of the elements. 
    - Do not modify the content of the news articles.
    - Do not add any additional information to the newsletter.
    - Add ALL the news stories provided to the newsletter. Even if the template only shows one news story, you should include all the news stories provided as context using the format provided in the template.
    - Include the publication date for each article in the newsletter.

    {html_template}
  expected_output: >
    Return ONLY the contents of the HTML template, without any tripple quotes like '```', '```html' or any other text. 
    This output will be saved as a HTML file and sent to the subscribers.

================================================
File: src/newsletter_gen/tools/research.py
================================================
from langchain.tools import BaseTool
from exa_py import Exa
import os
from datetime import datetime

class SearchAndContents(BaseTool):
    name: str = 'Search and Contents Tool'
    description: str = (
        'Searches the web based on a search query for the latest results. '
        'Results are only from the custom date range entered by the user. Uses the Exa API. This also returns the contents of the search results.'
    )

    def _run(self, search_query: str, start_date: str) -> str:
        exa = Exa(api_key=os.getenv('EXA_API_KEY'))

        # Validate and format the start_date
        try:
            start_published_date = datetime.strptime(start_date, '%Y-%m-%d').strftime('%Y-%m-%d')
        except ValueError:
            return 'Invalid date format. Please use YYYY-MM-DD.'

        end_published_date = datetime.now().strftime('%Y-%m-%d')

        # Log the dates for debugging
        print(f'Searching from {start_published_date} to {end_published_date}')

        search_results = exa.search_and_contents(
            query=search_query,
            use_autoprompt=True,
            start_published_date=start_published_date,
            text={'include_html_tags': False, 'max_characters': 8000},
        )

        return search_results


class FindSimilar(BaseTool):
    name: str = 'Find Similar Tool'
    description: str = (
        'Searches for similar articles to a given article using the Exa API. Takes in a URL of the article.'
    )

    def _run(self, article_url: str, start_date: str) -> str:
        # Validate and format the start_date
        try:
            start_published_date = datetime.strptime(start_date, '%Y-%m-%d').strftime('%Y-%m-%d')
        except ValueError:
            return 'Invalid date format. Please use YYYY-MM-DD.'

        end_published_date = datetime.now().strftime('%Y-%m-%d')

        # Log the dates for debugging
        print(f'Searching from {start_published_date} to {end_published_date}')

        exa = Exa(api_key=os.getenv('EXA_API_KEY'))

        search_results = exa.find_similar_and_contents(
            url=article_url,
            start_published_date=start_published_date,
            text={'include_html_tags': False, 'max_characters': 8000},
        )

        return search_results


class GetContents(BaseTool):
    name: str = 'Get Contents Tool'
    description: str = (
        'Gets the contents of a specific article using the Exa API. '
        'Takes in the ID of the article in a list, like this: ['https://www.cnbc.com/2024/04/18/my-news-story'].'
    )

    def _run(self, article_ids: list) -> str:
        exa = Exa(api_key=os.getenv('EXA_API_KEY'))

        contents = exa.contents(ids=article_ids)
        return contents



        """,
        "repo_created_at": "2025-02-12T13:09:51Z",
        "last_updated": "2025-02-13T01:52:13Z",
        "stars": 0
    },
    "SensibleAutonomousMachine": {
        "github_url": "https://github.com/Parthiban-3997/SensibleAutonomousMachine",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/SensibleAutonomousMachine",
        "description": """
        ================================================
File: README.md
================================================
# Sensible Autonomous Machine #

## INTRODUCTION ##
In recent times, automation has achieved improvements by quality, accuracy and precision. At the same time, the technology itself continues to evolve, bringing new waves of advances in robotics, analytics, and artificial intelligence (AI), and especially machine learning. Together they amount to a step change in technical capabilities that could have profound implications for business, for the economy, and more broadly, for society.  

The modules used in this program include

### 1.1	 Robotics ### 
Robotics is a branch of engineering which incorporates multiple disciplines to design, build, program and use robotic machines.  Robots are used in industries for speeding up the manufacturing process. AI is a highly useful tool in robotic assembly applications. When combined with advanced vision systems, AI can help with real-time course correction, which is particularly useful in complex manufacturing sectors. A handful of robotic systems are now being sold as open source systems with AI capability. Users train robots to do custom tasks based on their specific application, such as small-scale agriculture. The convergence of open source robotics and AI could be a huge trend in the future of AI robots.

### 1.2 Automation in Machines ###

Automation is any individual involved in the creation and application of technology to monitor and control the production and delivery of products and services. Automakers are moving at a frenzied pace to add more and more intelligence to vehicles developed a scale to describe the six different levels of automation for self-driving cars. Level 0 is the driver actually steps on the gas to go faster, steps on the brake to slow down and uses the steering wheel to turn. Level 1 is the driver is still in control of the overall operation and safety of the vehicle. Level 2 is the driver is still responsible for the safe operation of the vehicle. Level 3 states that the car can drive itself, but the human driver must still pay attention and be prepared to take over at any time. Level 4 explains that the car can be driven by a person, but it doesn’t always need to be. It can drive itself full-time under the right circumstances. Level 5 proposes that the car controls itself under all circumstances with no expectation of human intervention. 

### 1.3 Deep Learning ###

Deep Learning is a new area of Machine Learning research, which has been introduced with the objective of moving Machine Learning closer to one of its original goals, Artificial Intelligence. Deep Learning provides computers with the ability to learn without being explicitly programmed. Deep learning focuses on the development of computer programs that can change when exposed to new data.  The process of Deep Learning is similar to that of data mining. Both systems search through data to look for patterns.

### 1.4	 RCNN ###
 
Region-CNN (R-CNN) is one of the state-of-the-art CNN-based deep learning object detection approaches. Based on this, there are fast R-CNN and faster RCNN for faster speed object detection as well as mask R-CNN for object instance segmentation. On the other hand, there are also other object detection approaches, such as YOLO and SSD. 

### 1.5 Python-3.5.2 ###

Python’s standard library is very extensive, offering a wide range of facilities as indicated by the long table of contents listed below. The library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.

### 1.6	 TensorFlow 1.12.0 ###

TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.

### 1.7	 Pip ###

pip is a package management system used to install and manage software packages written in Python. Many packages can be found in the default source for packages and their dependencies —Python Package Index (PyPI). 
Python 2.7.9 and later (on the python2 series), and Python 3.4 and later include pip (pip3 for Python 3) by default. pip is a recursive acronym that can stand for either 'Pip Installs Packages' or 'Pip Installs Python'. Alternatively, pip stands for 'preferred installer program'.

### 1.8	 NumPy ###

NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. The ancestor of NumPy, Numeric, was originally created by Jim Hugunin with contributions from several other developers. In 2005, Travis Oliphant created NumPy by incorporating features of the competing num array into Numeric, with extensive modifications. NumPy is open-source software and has many contributors.

## OVERVIEW ##

The main idea of the project is to initiate a self-driving machine, observing the surroundings across a transport region and act accordingly by the provider’s instructions. This initiative brings many real-world things to autonomous creature and the main purpose is to save time from user’s point of view. 

### 2.1 Primary goal ###

Interfacing SAM to follow back an object by crossing obstacles and to reach destination by sensing the response signal from Smartphone. Monitoring entire surrounding to identify the color patterns of traffic signal and performing right/left turn by getting trained in predicting static/dynamic models. Maintain ratio of frames captured and running status of machine.
 
### 2.2 Secondary Goal ###

![2](https://user-images.githubusercontent.com/24918359/56951544-bedc1f80-6b55-11e9-9130-c8da5accd3aa.JPG) 

## ARCHITECTURE DIAGRAM ##

![21](https://user-images.githubusercontent.com/24918359/56954402-52185380-6b5c-11e9-8096-6df731268d7d.jpg)

## MODULE DESCRIPTION ##

1) Processing Real Time Data
2)	Calibration of Machine by embedding sensors
3)	Manipulation of the Machine
4) Integration of the Machine with Mobile Application

### 3.1 Processing Real Time Data ###

*	Pi camera is integrated with Raspberry Pi.
*	Image processing is done in real time scenario by recognizing object patterns and detecting traffic light using R-CNN.
*	Deep Learning implements feed-forward artificial neural networks or, more particularly, multi-layer perceptron (MLP), the most commonly used type of neural networks. MLP consists of the input layer, output layer, and one or more hidden layers. 
*	Each layer of MLP includes one or more neurons directionally linked with the neurons from the previous and the next layer.

![4](https://user-images.githubusercontent.com/24918359/56951984-cd770680-6b56-11e9-9087-ae2db5096b56.jpg)

### 3.2 Calibration of the Machine by embedding sensors###

### 3.2.1 Detection of Obstacles	###

* Absorption and Reflection of black and white signals to make the machine sense the road patterns for moving forward and turn operations (left and right).

![5](https://user-images.githubusercontent.com/24918359/56951990-d1a32400-6b56-11e9-8447-28f125895c5d.jpg)

### 3.2.2	Measurement of distance between objects ###

* Measuring the distance between the machine and other objects on-road by sensing frequency through Ultrasonic Sensor.

![6](https://user-images.githubusercontent.com/24918359/56952001-d5cf4180-6b56-11e9-87b6-1876523f667b.jpg)

### 3.3	Manipulation of the Machine ###

* L293D Motor IC is interfaced with Raspberry PI to perform start and stop operations.

![7](https://user-images.githubusercontent.com/24918359/56952009-d8ca3200-6b56-11e9-9e6c-6797e0520cb4.jpg)

### 3.4	Integration of the Machine with Mobile Application ###

* User provides the Geo Location via Android Application to the Machine where Latitude and Longitude data is fed to the machine to reach destination.

![8](https://user-images.githubusercontent.com/24918359/56952013-db2c8c00-6b56-11e9-9878-e0b2318743a2.jpg)

## PROCESSING AND TRAINING ##

### Installing TensorFlow ###

### 1. Update the Raspberry Pi ###

First, the Raspberry Pi needs to be fully updated. Open a terminal and issue:

    sudo apt-get update
    sudo apt-get dist-upgrade

![1](https://user-images.githubusercontent.com/24918359/56953172-8dfde980-6b59-11e9-90bd-ad1ec2777631.png)

### 2. Install TensorFlow ###

Next, we’ll install TensorFlow. In the /home/pi directory, create a folder called ‘tf’, which will be used to hold all the installation files for TensorFlow and Protobuf, and cd into it:

mkdir tf
cd tf
wget https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.8.0/tensorflow-1.12.0-cp35-none-linux_armv7l.whl

    sudo pip3 install /home/pi/tf/tensorflow-1.12.0-cp35-none-linux_armv7l.whl

TensorFlow also needs the LibAtlas package. Install it by issuing 

    sudo apt-get install libatlas-base-dev

While we’re at it, let’s install other dependencies that will be used by the TensorFlow Object Detection API. These are listed on the installation instructions in TensorFlow’s Object Detection GitHub repository. Issue:

    sudo pip3 install pillow lxml jupyter matplotlib cython
    sudo apt-get install python-tk

### 3. Install OpenCV ###

TensorFlow’s object detection examples typically use matplotlib to display images, but I prefer to use OpenCV because it’s easier to work with and less error prone. The object detection scripts in this guide’s GitHub repository use OpenCV. So, we need to install OpenCV.

To get OpenCV working on the Raspberry Pi, there’s quite a few dependencies that need to be installed through apt-get. If any of the following commands don’t work, issue “sudo apt-get update” and then try again. Issue:

    sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev
    sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev
    sudo apt-get install libxvidcore-dev libx264-dev
    sudo apt-get install qt4-dev-tools

Now that we’ve got all those installed, we can install OpenCV. Issue:

    pip3 install opencv-python

### 4. Compile and Install Protobuf ###

The TensorFlow object detection API uses Protobuf, a package that implements Google’s Protocol Buffer data format. Unfortunately, there’s currently no easy way to install Protobuf on the Raspberry Pi. We have to compile it from source ourselves and then install it.

First, get the packages needed to compile Protobuf from source. Issue:

    sudo apt-get install autoconf automake libtool curl

Then download the protobuf release from its GitHub repository by issuing:

wget https://github.com/google/protobuf/releases/download/v3.5.1/protobuf-all-3.5.1.tar.gz

If a more recent version of protobuf is available, download that instead. Unpack the file and cd into the folder:

    tar -zxvf protobuf-all-3.5.1.tar.gz
    cd protobuf-3.5.1

Configure the build by issuing the following command (it takes about 2 minutes):

    ./configure

Build the package by issuing:

    make

When it’s finished, issue:

    make check 

This process takes even longer, clocking in at 107 minutes on Pi. According to other guides I’ve seen, this command may exit out with errors, but Protobuf will still work. Now that it’s built, install it by issuing:

    sudo make install

Then move into the python directory and export the library path: 
    
    cd python
    export LD_LIBRARY_PATH=../src/.libs

Next, issue:

    python3 setup.py build --cpp_implementation 
    python3 setup.py test --cpp_implementation
    sudo python3 setup.py install --cpp_implementation

Then issue the following path commands:

    export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp
    export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION=3

Finally, issue:

    sudo ldconfig

Now Protobuf is installed on the Pi. Verify it’s installed correctly by issuing the command below and making sure it puts out the default help text.

Protoc

For some reason, the Raspberry Pi needs to be restarted after this process, or TensorFlow will not work. Go ahead and reboot the Pi by issuing:

    sudo reboot now

### 5. Set up TensorFlow Directory Structure and PYTHONPATH Variable ###

Now that we’ve installed all the packages, we need to set up the TensorFlow directory. Move back to your home directory, then make a directory called “tensorflow1”, and cd into it.

    mkdir tensorflow1
    cd tensorflow1

Download the tensorflow repository from GitHub by issuing:

    git clone --recurse-submodules https://github.com/tensorflow/models.git

Next, we need to modify the PYTHONPATH environment variable to point at some directories inside the TensorFlow repository we just downloaded. We want PYTHONPATH to be set every time we open a terminal, so we have to modify the .bashrc file. Open it by issuing:

    sudo nano ~/.bashrc

    export PYTHONPATH=$PYTHONPATH:/home/pi/tensorflow1/models/research:/home/pi/tensorflow1/models/research/slim

![2](https://user-images.githubusercontent.com/24918359/56953173-8dfde980-6b59-11e9-99d4-6fcecf96bf92.png)

Then, save and exit the file. This makes it so the “export PYTHONPATH” command is called every time you open a new terminal, so the PYTHONPATH variable will always be set appropriately. Close and then re-open the terminal.

Now, we need to use Protoc to compile the Protocol Buffer (.proto) files used by the Object Detection API. The .proto files are located in /research/object_detection/protos, but we need to execute the command from the /research directory. Issue:

    cd /home/pi/tensorflow1/models/research
    protoc object_detection/protos/*.proto --python_out=.

This command converts all the 'name'.proto files to 'name_pb2'.py files. Next, move into the object_detection directory:

    cd /home/pi/tensorflow1/models/research/object_detection

Now, we’ll download the SSD_Mobilenet model from the TensorFlow detection model zoo. The model zoo is Google’s collection of pre-trained object detection models that have various levels of speed and accuracy. The Raspberry Pi has a weak processor, so we need to use a model that takes less processing power. Though the model will run faster, it comes at a tradeoff of having lower accuracy. For this tutorial, we’ll use SSD-MobileNet, which is the fastest model available.

Google is continuously releasing models with improved speed and performance, so check back at the model zoo often to see if there are any better models.

Download the SSD-MobileNet model and unpack it by issuing:

    wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_05_09.tar.gz
    tar -xzvf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz

Now the model is in the object_detection directory and ready to be used.

### 6. Detect Objects ###

Okay, now everything is set up for performing object detection on the Pi! The Python script in this repository, Object_detection_picamera.py, detects objects in live feeds from a Picamera or USB webcam. Basically, the script sets paths to the model and label map, loads the model into memory, initializes the Picamera, and then begins performing object detection on each video frame from the Picamera.

If you’re using a Picamera, make sure it is enabled in the Raspberry Pi configuration menu.

![3](https://user-images.githubusercontent.com/24918359/56953174-8dfde980-6b59-11e9-9563-c660969fbe23.png)

Download the Object_detection_picamera.py file into the object_detection directory by issuing:

wget https://raw.githubusercontent.com/Parthi_Koushik/TensorFlow-Object-Detection-on-the-Raspberry-Pi/master/Object_detection_picamera.py

Run the script by issuing:

    python3 Object_detection_picamera.py 

The script defaults to using an attached Picamera. If you have a USB webcam instead, add --usbcam to the end of the command:
python3 Object_detection_picamera.py –usbcam

Once the script initializes (which can take up to 30 seconds), you will see a window showing a live view from your camera. Common objects inside the view will be identified and have a rectangle drawn around them.

![4](https://user-images.githubusercontent.com/24918359/56953175-8e968000-6b59-11e9-812f-8067e6aac3c0.png)

You can also use a model by adding the frozen inference graph into the object_detection directory and changing the model path in the script. You can test this out using my playing card detector model (transferred from ssd_mobilenet_v2 model and trained on TensorFlow v1.12). Once downloaded and extracted the model, or if you have your own model, place the model folder into the object_detection directory. Place the label_map.pbtxt file into the object_detection/data directory.

![5](https://user-images.githubusercontent.com/24918359/56953176-8e968000-6b59-11e9-9fce-152c5938dc39.png)

Then, open the Object_detection_picamera.py script in a text editor. Go to the line where MODEL_NAME is set and change the string to match the name of the new model folder. Then, on the line where PATH_TO_LABELS is set, change the name of the labelmap file to match the new label map. Change the NUM_CLASSES variable to the number of classes your model can identify.   

![6](https://user-images.githubusercontent.com/24918359/56953177-8e968000-6b59-11e9-89c9-b7573f63d4ff.png)

Now, when you run the script, it will use your model rather than the SSDLite_MobileNet model. 

### Training Datasets ###

### 1. Set up TensorFlow Directory and Anaconda Virtual Environment ###

The TensorFlow Object Detection API requires using the specific directory structure provided in its GitHub repository. It also requires several additional Python packages, specific additions to the PATH and PYTHONPATH variables, and a few extra setup commands to get everything set up to run or train an object detection model.

### 1a. Download TensorFlow Object Detection API repository from GitHub ###

Create a folder directly in C: and name it “tensorflow1”. This working directory will contain the full TensorFlow object detection framework, as well as your training images, training data, trained classifier, configuration files, and everything else needed for the object detection classifier.

### 1b. Download the SSD_MobileNet-V2-COCO model from TensorFlow's model zoo ###

TensorFlow provides several object detection models (pre-trained classifiers with specific neural network architectures) in its model zoo. Some models (such as the SSD-MobileNet model) have an architecture that allows for faster detection but with less accuracy, while some models (such as the Faster-RCNN model) give slower detection but with more accuracy. Use the SSD-MobileNet model for training.

### 1c. Download repository ###

Download the full repository located on this page (scroll to the top and click Clone or Download) and extract all the contents directly into the C:\tensorflow1\models\research\object_detection directory. (You can overwrite the existing 'README.md' file.) This establishes a specific directory structure that will be used.

![7](https://user-images.githubusercontent.com/24918359/56953179-8e968000-6b59-11e9-9b27-2a64e62dcaf5.jpg)

This contains the images, annotation data, .csv files, and TFRecords needed to train a 'Pinochle Deck' playing card detector. You can use these images and data to practice making your own Pinochle Card Detector. It also contains Python scripts that are used to generate the training data. It has scripts to test out the object detection classifier on images, videos, or a webcam feed. You can ignore the \doc folder and its files; they are just there to hold the images used for this readme.

If you want to practice training your own 'Pinochle Deck' card detector, you can leave all the files as they are. You can follow along with this tutorial to see how each of the files were generated, and then run the training. You will still need to generate the TFRecord files (train.record and test.record) as described in Step 4.

If you want to train your own object detector, delete the following files (do not delete the folders):

•	All files in \object_detection\training
•	All files in \object_detection\inference_graph

Now, you are ready to start from scratch in training your own object detector. This tutorial will assume that all the files listed above were deleted, and will go on to explain how to generate the files for your own training dataset.

### 1d. Set up new Anaconda virtual environment ###

Next, we'll work on setting up a virtual environment in Anaconda for tensorflow-gpu. From the Start menu in Windows, search for the Anaconda Prompt utility, right click on it, and click “Run as Administrator”. If Windows asks you if you would like to allow it to make changes to your computer, click Yes.

In the command terminal that pops up, create a new virtual environment called “tensorflow1” by issuing the following command:
C:\> conda create -n tensorflow1 pip python=3.5

Then, activate the environment by issuing:

C:\> activate tensorflow1

Install tensorflow-gpu in this environment by issuing:

(tensorflow1) C:\> pip install --ignore-installed --upgrade tensorflow-gpu

Install the other necessary packages by issuing the following commands:

(tensorflow1) C:\> conda install -c anaconda protobuf
(tensorflow1) C:\> pip install pillow
(tensorflow1) C:\> pip install lxml
(tensorflow1) C:\> pip install Cython
(tensorflow1) C:\> pip install jupyter
(tensorflow1) C:\> pip install matplotlib
(tensorflow1) C:\> pip install pandas
(tensorflow1) C:\> pip install opencv-python
(Note: The ‘pandas’ and ‘opencv-python’ packages are not needed by TensorFlow, but they are used in the Python scripts to generate TFRecords and to work with images, videos, and webcam feeds.)

### 1e. Configure PYTHONPATH environment variable ###

A PYTHONPATH variable must be created that points to the \models, \models\research, and \models\research\slim directories. Do this by issuing the following commands (from any directory):
(tensorflow1) C:\> set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim

(Note: Every time the 'tensorflow1' virtual environment is exited, the PYTHONPATH variable is reset and needs to be set up again.)

### 1f. Compile Protobufs and run setup.py ###

Next, compile the Protobuf files, which are used by TensorFlow to configure model and training parameters. Unfortunately, the short protoc compilation command posted on TensorFlow’s Object Detection API installation page does not work on Windows. Every .proto file in the \object_detection\protos directory must be called out individually by the command.

In the Anaconda Command Prompt, change directories to the \models\research directory and copy and paste the following command into the command line and press Enter:

protoc --python_out=. .\object_detection\protos\anchor_generator.proto .\object_detection\protos\argmax_matcher.proto .\object_detection\protos\bipartite_matcher.proto .\object_detection\protos\box_coder.proto .\object_detection\protos\box_predictor.proto .\object_detection\protos\eval.proto .\object_detection\protos\faster_rcnn.proto .\object_detection\protos\faster_rcnn_box_coder.proto .\object_detection\protos\grid_anchor_generator.proto .\object_detection\protos\hyperparams.proto .\object_detection\protos\image_resizer.proto .\object_detection\protos\input_reader.proto .\object_detection\protos\losses.proto .\object_detection\protos\matcher.proto .\object_detection\protos\mean_stddev_box_coder.proto .\object_detection\protos\model.proto .\object_detection\protos\optimizer.proto .\object_detection\protos\pipeline.proto .\object_detection\protos\post_processing.proto .\object_detection\protos\preprocessor.proto .\object_detection\protos\region_similarity_calculator.proto .\object_detection\protos\square_box_coder.proto .\object_detection\protos\ssd.proto .\object_detection\protos\ssd_anchor_generator.proto .\object_detection\protos\string_int_label_map.proto .\object_detection\protos\train.proto .\object_detection\protos\keypoint_box_coder.proto .\object_detection\protos\multiscale_anchor_generator.proto .\object_detection\protos\graph_rewriter.proto

This creates a name_pb2.py file from every name.proto file in the \object_detection\protos folder.

Finally, run the following commands from the C:\tensorflow1\models\research directory:

(tensorflow1) C:\tensorflow1\models\research> python setup.py build
(tensorflow1) C:\tensorflow1\models\research> python setup.py install

### 1g. Test TensorFlow setup to verify it works ###

The TensorFlow Object Detection API is now all set up to use pre-trained models for object detection, or to train a new one. You can test it out and verify your installation is working by launching the object_detection_tutorial.ipynb script with Jupyter. From the \object_detection directory, issue this command:

(tensorflow1) C:\tensorflow1\models\research\object_detection> jupyter notebook object_detection_tutorial.ipynb
This opens the script in your default web browser and allows you to step through the code one section at a time. You can step through each section by clicking the “Run” button in the upper toolbar. The section is done running when the “In [ * ]” text next to the section populates with a number (e.g. “In [1]”).
Once you have stepped all the way through the script, you should see two labelled images at the bottom section the page. If you see this, then everything is working properly! If not, the bottom section will report any errors encountered.

### 2. Gather and Label Pictures ###

### 2a. Gather Pictures ###

TensorFlow needs hundreds of images of an object to train a good detection classifier. To train a robust classifier, the training images should have random objects in the image along with the desired objects, and should have a variety of backgrounds and lighting conditions. There should be some images where the desired object is partially obscured, overlapped with something else, or only halfway in the picture.

We have two different objects I want to detect (red, green). Then, it took about another 169 pictures with multiple images in the picture. I know I want to be able to detect the cards when they’re overlapping, so I made sure to have the cards be overlapped in many images.

![8](https://user-images.githubusercontent.com/24918359/56953180-8f2f1680-6b59-11e9-8df3-c8369b8fac26.png)

Make sure the images aren’t too large. They should be less than 200KB each, and their resolution shouldn’t be more than 720x1280. The larger the images are, the longer it will take to train the classifier. You can use the resizer.py script in this repository to reduce the size of the images.

![9](https://user-images.githubusercontent.com/24918359/56953182-8f2f1680-6b59-11e9-8446-305b2d82e0d2.png)

After you have all the pictures you need, move 20% of them to the \object_detection\images\test directory, and 80% of them to the \object_detection\images\train directory. Make sure there are a variety of pictures in both the \test and \train directories.

### 2b. Label Pictures ###

With all the pictures gathered, it’s time to label the desired objects in every picture. LabelImg is a great tool for labeling images, and its GitHub page has very clear instructions on how to install and use it.

Download and install LabelImg, point it to your \images\train directory, and then draw a box around each object in each image. Repeat the process for all the images in the \images\test directory. 

![10](https://user-images.githubusercontent.com/24918359/56953184-8f2f1680-6b59-11e9-8184-7ec582bca87e.png)

LabelImg saves a .xml file containing the label data for each image. These .xml files will be used to generate TFRecords, which are one of the inputs to the TensorFlow trainer. Once you have labeled and saved each image, there will be one .xml file for each image in the \test and \train directories.

![11](https://user-images.githubusercontent.com/24918359/56953163-8c342600-6b59-11e9-9ab1-8d7a5ac1bbc0.png)

Also, can check if the size of each bounding box is correct by running sizeChecker.py

(tensorflow1) C:\tensorflow1\models\research\object_detection> python sizeChecker.py --move

![12](https://user-images.githubusercontent.com/24918359/56953164-8cccbc80-6b59-11e9-9080-308976164e9c.png)

### 3. Generate Training Data ###

With the images labeled, it’s time to generate the TFRecords that serve as input data to the TensorFlow training model. This tutorial uses the xml_to_csv.py and generate_tfrecord.py scripts from Dat Tran’s Raccoon Detector dataset, with some slight modifications to work with our directory structure.
First, the image .xml data will be used to create .csv files containing all the data for the train and test images. From the \object_detection folder, issue the following command in the Anaconda command prompt:
(tensorflow1) C:\tensorflow1\models\research\object_detection> python xml_to_csv.py
This creates a train_labels.csv and test_labels.csv file in the \object_detection\images folder.
Next, open the generate_tfrecord.py file in a text editor. Replace the label map starting at line 31 with your own label map, where each object is assigned an ID number. This same number assignment will be used when configuring the labelmap.pbtxt file.

def class_text_to_int(row_label):
    if row_label == 'red':
        return 1
    elif row_label == 'green':
        return 2
    else:
        return None

For example:

def class_text_to_int(row_label):
    if row_label == 'basketball':
        return 1
    elif row_label == 'shirt':
        return 2
    elif row_label == 'shoe':
        return 3
    else:
        return None

Then, generate the TFRecord files by issuing these commands from the \object_detection folder:
python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record
python generate_tfrecord.py --csv_input=images\test_labels.csv --image_dir=images\test --output_path=test.record

These generate a train.record and a test.record file in \object_detection. These will be used to train the new object detection classifier.

### 4. Create Label Map and Configure Training ###

The last thing to do before training is to create a label map and edit the training configuration file.

### 4a. Label map ###

The label map tells the trainer what each object is by defining a mapping of class names to class ID numbers. Use a text editor to create a new file and save it as labelmap.pbtxt in the C:\tensorflow1\models\research\object_detection\training folder. 
item {
  id: 1
  name: 'red'
}

item {
  id: 2
  name: 'green'
}
}

The label map ID numbers should be the same as what is defined in the generate_tfrecord.py file. For the basketball, shirt, and shoe detector example mentioned, the labelmap.pbtxt file will look like:

item {
  id: 1
  name: 'basketball'
}

item {
  id: 2
  name: 'shirt'
}

item {
  id: 3
  name: 'shoe'
}

### 4b. Configure training ###

Finally, the object detection training pipeline must be configured. It defines which model and what parameters will be used for training. This is the last step before running training!
 
Navigate to C:\tensorflow1\models\research\object_detection\samples\configs and copy the faster_rcnn_inception_v2_pets.config file into the \object_detection\training directory

![13](https://user-images.githubusercontent.com/24918359/56953165-8cccbc80-6b59-11e9-82c8-056788bb04d2.png)

![14](https://user-images.githubusercontent.com/24918359/56953166-8cccbc80-6b59-11e9-8d61-bc1a36e04202.jpg)

Save the file after the changes have been made. That’s it! The training job is all configured and ready to go!

### 5. Run the Training ###

Here we go! From the \object_detection directory, issue the following command to begin training:
python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_signals.config

Each step of training reports the loss. It will start high and get lower and lower as training progresses. For my training on the Faster-RCNN-Inception-V2 model, it started at about 3.0 and quickly dropped below 0.8.

![15](https://user-images.githubusercontent.com/24918359/56953168-8d655300-6b59-11e9-8ee0-d3f22e9ddf33.png)

![16](https://user-images.githubusercontent.com/24918359/56953169-8d655300-6b59-11e9-8c76-9e9da0d354dc.png)


You can view the progress of the training job by using TensorBoard. To do this, open a new instance of Anaconda Prompt, activate the tensorflow1 virtual environment, change to the C:\tensorflow1\models\research\object_detection directory, and issue the following command:
(tensorflow1) C:\tensorflow1\models\research\object_detection>tensorboard --logdir=training

![17](https://user-images.githubusercontent.com/24918359/56953170-8d655300-6b59-11e9-97f0-b71c18cbf2db.png)

The training routine periodically saves checkpoints about every five minutes. You can terminate the training by pressing Ctrl+C while in the command prompt window. I typically wait until just after a checkpoint has been saved to terminate the training. You can terminate training and start it later, and it will restart from the last saved checkpoint. The checkpoint at the highest number of steps will be used to generate the frozen inference graph.

### 6. Export Inference Graph ###

Now that training is complete, the last step is to generate the frozen inference graph (.pb file). From the \object_detection folder, issue the following command, where “XXXX” in “model.ckpt-1962” should be replaced with the highest-numbered .ckpt file in the training folder:

python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v2_signal.config --trained_checkpoint_prefix training/model.ckpt-1962 --output_directory inference_graph

![18](https://user-images.githubusercontent.com/24918359/56953171-8d655300-6b59-11e9-95d1-5ae14ae023c6.png)

This creates a frozen_inference_graph.pb file in the \object_detection\inference_graph folder. The .pb file contains the object detection classifier.

### 7. Use Newly Trained Object Detection Classifier ###

Before running the Python scripts, you need to modify the NUM_CLASSES variable in the script to equal the number of classes you want to detect. So NUM_CLASSES = 2. To test your object detector, move a picture of the object or objects into the \object_detection folder, and change the IMAGE_NAME variable in the Object_detection_image.py to match the file name of the picture. Alternatively, you can use a video of the objects (using Object_detection_video.py), or just plug in a USB webcam and point it at the objects (using Object_detection_webcam.py).

## Working with Machine ##
Producing control systems capable of analyzing sensory data in order to provide accurate detection of other vehicles and the road ahead fusing data from multiple sensors. Data Replication between sensory data and real time camera [8]. Monitoring entire surrounding to identify the colour patterns of traffic signal and performing right/left turn by getting trained in predicting static/dynamic models. Maintain ratio of frames captured and running status of machine.

## Sensor Testing ##

IR sensor sends binary values by receiving and decoding signals. The led sends and receives live data of road patterns and communicate the output value with mo-tor IC. Absorption and Reflection of black and white signals to make the machine sense the road patterns for moving forward and turn operations (left and right).

### Motor Testing ###

L293D Motor Driver IC consists of 4 input pins, 4 output pins and 2 enable pins. When enable A is high, left motor (input 1 and input 2) works. When enable B is high, right motor (input 3 and input 4) works. 

Input 1	Input 2	 Result
   0      	0	     Stop
   0	      1	     Anti-Clockwise
   1	      0	     Clockwise
   1	      1	     Stop
   

A GPS aerial is mounted at the rear for location information featuring gyroscopes, altimeters and a tachometer to avoid a minute miscalculation of the machine’s positions.

The machine can safely drive itself under specific conditions such as expressway merging, high speed cruising, low speed traffic jam, closed-campus operations by classifying object patterns and traffic signals. Infrared Sensor performs identification of road patterns during self-parking scenarios by absorbing and reflecting Infrared signals [9]. Pi camera is fixed on top of the machine to detect signal patterns by clas-sifying colors from comparing with the trained data at frequent measure of time

## Conclusion and Future Work ##

The proposed work uses different modules for providing automation to the robotic machine. There are certain systems providing security in existence but with less effi-ciency but our project deals with live data in the form of continuous images which gets automatically deleted after some point of time. This improves the efficiency of the system. Although the machine senses traffic signals and proceed moving opera-tions, installation of higher-grade cameras and motor power shall improve efficiency, performance and speed control of the machine respectively. More complicated im-ages are given as input to train the machine act according to emergency situations by predicting and analyzing. 




================================================
File: Object_detection_picamera.py
================================================
# Import packages
import os
import cv2
import numpy as np
from picamera.array import PiRGBArray
from picamera import PiCamera
import tensorflow as tf
import argparse
import sys
#import final_robo.py

# Set up camera constants
IM_WIDTH = 1280
IM_HEIGHT = 720
#IM_WIDTH = 640    
#IM_HEIGHT = 480   

# Select camera type (if user enters --usbcam when calling this script,
# a USB webcam will be used)
camera_type = 'picamera'
parser = argparse.ArgumentParser()
parser.add_argument('--usbcam', help='Use a USB webcam instead of picamera',
                    action='store_true')
args = parser.parse_args()
if args.usbcam:
    camera_type = 'usb'

sys.path.append('..')

# Import utilites
from utils import label_map_util
from utils import visualization_utils as vis_util

# Name of the directory containing the object detection module we're using
MODEL_NAME = 'ssdlite_mobilenet_v2_coco_2018_05_09'

# Grab path to current working directory
CWD_PATH = os.getcwd()

# Path to frozen detection graph .pb file, which contains the model that is used
# for object detection.
PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')

# Path to label map file
PATH_TO_LABELS = os.path.join(CWD_PATH,'data','mscoco_label_map.pbtxt')

# Number of classes the object detector can identify
NUM_CLASSES = 2

## Load the label map.

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

# Load the Tensorflow model into memory.
detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    sess = tf.Session(graph=detection_graph)


# Define input and output tensors (i.e. data) for the object detection classifier

# Input tensor is the image
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

# Output tensors are the detection boxes, scores, and classes
# Each box represents a part of the image where a particular object was detected
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

# Each score represents level of confidence for each of the objects.
# The score is shown on the result image, together with the class label.
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
#print (detection_classes)

      
# Number of objects detected
num_detections = detection_graph.get_tensor_by_name('num_detections:0')

# Initialize frame rate calculation
frame_rate_calc = 1
freq = cv2.getTickFrequency()
font = cv2.FONT_HERSHEY_SIMPLEX

# Initialize camera and perform object detection.
# The camera has to be set up and used differently depending on if it's a
# Picamera or USB webcam.

### Picamera ###
if camera_type == 'picamera':
    # Initialize Picamera and grab reference to the raw capture
    camera = PiCamera()
    camera.resolution = (IM_WIDTH,IM_HEIGHT)
    camera.framerate = 10
    rawCapture = PiRGBArray(camera, size=(IM_WIDTH,IM_HEIGHT))
    rawCapture.truncate(0)

    for frame1 in camera.capture_continuous(rawCapture, format='bgr',use_video_port=True):

        t1 = cv2.getTickCount()
            
        frame = np.copy(frame1.array)
        frame.setflags(write=1)
        frame_expanded = np.expand_dims(frame, axis=0)

        # Perform the actual detection by running the model with the image as input
        (boxes, scores, classes, num) = sess.run(
            [detection_boxes, detection_scores, detection_classes, num_detections],
            feed_dict={image_tensor: frame_expanded})

        # Draw the results of the detection 
        vis_util.visualize_boxes_and_labels_on_image_array(
            frame,
            np.squeeze(boxes),
            np.squeeze(classes).astype(np.int32),
            np.squeeze(scores),
            category_index,
            use_normalized_coordinates=True,
            line_thickness=8,
            min_score_thresh=0.5)
            

        cv2.putText(frame,'FPS: {0:.2f}'.format(frame_rate_calc),(30,50),font,1,(255,255,0),2,cv2.LINE_AA)
                # All the results have been drawn on the frame, so it's time to display it.
        cv2.imshow('Object detector', frame)
        
       #if(int(categories) == NUM_CLASSES[1])
           # print('Red')
       #elif(int(categories) == NUM_CLASSES[2]):
           #print('Green')
        #else:
         #   print('none')

       # min_score_thresh = 0.5

       # print([category.get(1)] for i in classes[0] if scores[0, i] > min_score_thresh)
             # if detection_classes == 'green':
        #    print('green')
        #else:
        
         #  print('red')
        #print(category_index)
        #print([category_index.get(i) for i in classes[0]])
        #print(scores)
        #print(detection_classes)
        
        #print(num)

        t2 = cv2.getTickCount()
        time1 = (t2-t1)/freq
        frame_rate_calc = 1/time1
        
        # Press 'q' to quit
        if cv2.waitKey(1) == ord('q'):
            break

        rawCapture.truncate(0)

    camera.close()

### USB webcam ###
elif camera_type == 'usb':
    # Initialize USB webcam feed
    camera = cv2.VideoCapture(0)
    ret = camera.set(3,IM_WIDTH)
    ret = camera.set(4,IM_HEIGHT)

    while(True):

        t1 = cv2.getTickCount()

        # Acquire frame and expand frame dimensions to have shape: [1, None, None, 3]
        ret, frame = camera.read()
        frame_expanded = np.expand_dims(frame, axis=0)

        # Perform the actual detection by running the model with the image as input
        (boxes, scores, classes, num) = sess.run(
            [detection_boxes, detection_scores, detection_classes, num_detections],
            feed_dict={image_tensor: frame_expanded})

        # Draw the results of the detection (aka 'visulaize the results')
        vis_util.visualize_boxes_and_labels_on_image_array(
            frame,
            np.squeeze(boxes),
            np.squeeze(classes).astype(np.int32),
            np.squeeze(scores),
            category_index,
            use_normalized_coordinates=True,
            line_thickness=8,
            min_score_thresh=0.85)

        cv2.putText(frame,'FPS: {0:.2f}'.format(frame_rate_calc),(30,50),font,1,(255,255,0),2,cv2.LINE_AA)
        
        # All the results have been drawn on the frame, so it's time to display it.
        cv2.imshow('Object detector', frame)

        t2 = cv2.getTickCount()
        time1 = (t2-t1)/freq
        frame_rate_calc = 1/time1

        # Press 'q' to quit
        if cv2.waitKey(1) == ord('q'):
            break

    camera.release()

cv2.destroyAllWindows()




        """,
        "repo_created_at": "2025-02-12T13:09:55Z",
        "last_updated": "2025-02-12T13:10:02Z",
        "stars": 0
    },
    "Smart-ATS-System-Using-Google-Gemini": {
        "github_url": "https://github.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini",
        "gitingest_url": "https://gitingest.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini",
        "description": """
        ================================================
File: README.md
================================================
# Smart ATS System 💼🔍

This Streamlit web application serves as a Smart ATS (Applicant Tracking System), utilizing Google's Gemini Pro AI model to evaluate resumes against job descriptions. By harnessing the power of AI, it streamlines the hiring process and enhances HR efficiency.

## Deployed Link
The Smart ATS System is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/ATS_Smart_System)

## Features & Impact

- **Resume Evaluation**: Upload resumes in PDF or DOCX format and paste job descriptions to assess candidate suitability for roles efficiently.

- **Skill Enhancement Recommendations**: Get insights into improving candidates' skills tailored to the job requirements, aiding HR professionals in talent development.

- **Keyword Identification**: Identify missing keywords in resumes compared to job descriptions, ensuring thorough evaluation and highlighting areas for improvement.

- **Match Percentage Calculation**: Calculate the percentage match between resumes and job descriptions, providing a quantitative measure of alignment.

- **Query Resolution**: Answer queries based on provided resumes and job descriptions, facilitating clearer understanding and decision-making.

## Screenshots

![ats_1](https://github.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/eed50921-ef63-4158-9905-03bbf202d72e)





================================================
File: launch.py
================================================
## Importing Libraries
import streamlit as st
import os
from PyPDF2 import PdfReader
import google.generativeai as genai
from dotenv import load_dotenv
import docx

# Loading the .env keys
load_dotenv()

# Define functions
def get_gemini_response(model_id, prompt, pdf_content, input_text):
    model = genai.GenerativeModel(model_id)
    response = model.generate_content([prompt, pdf_content, input_text])
    return response.text

def get_pdf_text(pdf_docs):
    text = ''
    for doc in pdf_docs:
        if doc.name.endswith('.pdf'):
            pdf_reader = PdfReader(doc)
            for page in pdf_reader.pages:
                text += page.extract_text()
        elif doc.name.endswith('.docx'):
            doc_reader = docx.Document(doc)
            for para in doc_reader.paragraphs:
                text += para.text + '\n'        
    return text

# Define input prompts
input_prompts = {
    'evaluate_resume': '''
        You are an experienced Technical Human Resource Manager. Your task is to review the provided resume against the job description. 
        Please share your professional evaluation on whether the candidate's profile aligns with the role. 
        Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements.

    ''',
    'improve_skills': '''
        You are a Technical Human Resource Manager with expertise in all domains. 
        Your role is to scrutinize the resume in light of the job description provided. 
        Share your insights on the candidate's suitability for the role from an HR perspective. 
        Additionally, offer advice on enhancing the candidate's skills and identify areas where improvement is needed.

    ''',
    'missing_keywords': '''
        You are a skilled ATS (Applicant Tracking System) scanner with a deep understanding of ATS functionality across various domains. 
        Your task is to evaluate the resume against the provided job description. As a Human Resource Manager,
        assess the compatibility of the resume with the role. Identify the keywords that are missing.
        Also, provide recommendations for enhancing the candidate's skills and indicate which areas require further development.

    ''',
    'percentage_match': '''
        You are a skilled ATS (Applicant Tracking System) scanner with a deep understanding of ATS functionality across various domains. 
        Your task is to evaluate the resume against the provided job description. Give the percentage of match if the resume matches
        the job description. First, present the percentage match, then list the missing keywords, and finally provide your overall thoughts.

    ''',
    'answer_query': '''
        You are an experienced Technical Human Resource Manager. Please answer the following query based on the resume and job description provided.
    '''
}

# Define model options
model_options = [
    'gemini-1.5-flash',
    'gemini-1.5-pro',
    'gemini-1.0-pro',
    'gemini-pro'
]


# Streamlit App
st.set_page_config(page_title='Resume Expert System', page_icon=':chart_with_upwards_trend:')
st.title('Smart ATS System 💼🔍')

# Sidebar for API key, model selection, and resume uploader
with st.sidebar:
    st.markdown('[Get your Google API Key](https://aistudio.google.com/app/apikey)')
    api_key = st.text_input('Enter your Google API Key', type='password')
    selected_model = st.selectbox('Select Gemini Model', model_options)
    uploaded_files = st.file_uploader('Upload Your Resume in .PDF or .DOCX format 📂', type=['pdf', 'docx'], accept_multiple_files=True)
    if uploaded_files:
        st.success('Files Uploaded Successfully.')

# Set the API key for genai
if api_key:
    genai.configure(api_key=api_key)

input_text = st.text_area('Paste the Job Description 📄')

# Align buttons in one row
col1, col2, col3, col4, col5 = st.columns(5)

with col1:
    evaluate_resume_btn = st.button('Evaluate Resume')
with col2:
    improve_skills_btn = st.button('Improve Skills')
with col3:
    missing_keywords_btn = st.button('Identify Missing Keywords')
with col4:
    percentage_match_btn = st.button('Calculate Match Percentage')
with col5:
    answer_query_btn = st.button('Answer My Query')

show_error_api_key = False
show_error_uploaded_files = False
show_error_input_text = False

if evaluate_resume_btn or improve_skills_btn or missing_keywords_btn or percentage_match_btn or answer_query_btn:
    if not api_key:
        show_error_api_key = True
    if not uploaded_files:
        show_error_uploaded_files = True
    if not input_text:
        show_error_input_text = True

    if not show_error_api_key and not show_error_uploaded_files and not show_error_input_text:
        pdf_content = get_pdf_text(uploaded_files)

        if evaluate_resume_btn:
            response = get_gemini_response(selected_model, input_prompts['evaluate_resume'], pdf_content, input_text)
            st.subheader('The Response is')
            st.write(response)
        elif improve_skills_btn:
            response = get_gemini_response(selected_model, input_prompts['improve_skills'], pdf_content, input_text)
            st.subheader('The Response is')
            st.write(response)
        elif missing_keywords_btn:
            response = get_gemini_response(selected_model, input_prompts['missing_keywords'], pdf_content, input_text)
            st.subheader('The Response is')
            st.write(response)
        elif percentage_match_btn:
            response = get_gemini_response(selected_model, input_prompts['percentage_match'], pdf_content, input_text)
            st.subheader('The Response is')
            st.write(response)
        elif answer_query_btn:
            response = get_gemini_response(selected_model, input_prompts['answer_query'], pdf_content, input_text)
            st.subheader('The Response is')
            st.write(response)

# Display error messages near respective fields
if show_error_api_key:
    with st.sidebar:
        st.error('Please enter your Google API Key.')
if show_error_input_text:
    st.error('Please paste the job description to proceed.', icon='📄')


================================================
File: requirements.txt
================================================
streamlit
PyPDF2
google.generativeai
python-dotenv
langchain-groq
python-docx
docx



        """,
        "repo_created_at": "2025-02-12T13:09:58Z",
        "last_updated": "2025-02-12T13:10:06Z",
        "stars": 0
    }
}