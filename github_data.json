{
    "6-Months-Data-Science-Roadmap-": {
        "description": null,
        "readme": "# 6 Months Data Science Roadmap in 2023\n## Work Of Data Scientist?\n![Capture](https://user-images.githubusercontent.com/20041231/211718743-d6604ff7-8828-422b-9b60-ec156cdaf054.JPG)\n\n![Capture](https://user-images.githubusercontent.com/20041231/211718788-8d71ca8c-430a-4dbe-98f2-c66da015ac6e.JPG)\n\n## 1. Python Programming Language -1 Month\nPython:\n![python-logo-master-v3-TM-flattened](https://user-images.githubusercontent.com/20041231/211717885-0b1e049b-f5b3-457d-ba7a-9345ec3aa39c.png)\n\nPython In English:https://www.youtube.com/watch?v=bPrmA1SEN2k&list=PLZoTAELRMXVNUL99R4bDlVYsncUNvwUBB\n\nPython In Hindi: https://www.youtube.com/watch?v=MJd9d9Mpxg0&list=PLTDARY42LDV4qqiJd1Z1tShm3mp9-rP4v\n\nFlask Playlist: https://www.youtube.com/watch?v=4L_xAWDRs7w&list=PLZoTAELRMXVPBaLN3e-uoVRR9hlRFRfUc\n\n### Final Goal Outcome:\n1. Basic To Intermediate Python With various knowledge of various Data structures like numpy,pandas,matplotlib and many more.\n2. Knowledge of performing EDA,Feature Engineering and creating visualization charts using python \n3. Atleast Make some python projects using Frameworks such as Flask with deployment Eg: Web Scrapping Projects  \n\n### What's New In Python 3.10\nhttps://docs.python.org/3/whatsnew/3.10.html\n\n## 2. Statistics\n![1_jidbi1--elimaTb3B2HT5w](https://user-images.githubusercontent.com/20041231/211717931-134aaac2-a8fc-445b-93a6-ae241c66ba5b.png)\n\nEnglish: 7 Days Statistics Live Session :https://www.youtube.com/watch?v=11unm2hmvOQ&list=PLZoTAELRMXVMgtxAboeAx-D9qbnY94Yay\n\nStatistics in ML(43 videos): https://www.youtube.com/watch?v=zRUliXuwJCQ&list=PLZoTAELRMXVMhVyr3Ri9IQ-t5QPBtxzJO\n\nHindi: Stats Playlist: https://www.youtube.com/watch?v=7y3XckjaVOw&list=PLTDARY42LDV6YHSRo669_uDDGmUEmQnDJ\n\n### EDA\nEDA Live :https://www.youtube.com/playlist?list=PLZoTAELRMXVPzj1D0i_6ajJ6gyD22b3jh\n\nEDA Detailed Playlist: https://www.youtube.com/watch?v=ioN1jcWxbv8&list=PLZoTAELRMXVPQyArDHyQVjQxjj_YmEuO9\n\n### Feature Engineering\n\nComplete Detailed Feature Engineering: https://www.youtube.com/watch?v=6WDFfaYtN6s&list=PLZoTAELRMXVPwYGE2PXD3x0bfKnR0cJjN\n\nLive EDA Feature Engineering Playlist: https://www.youtube.com/watch?v=bTN-6VPe8c0&list=PLZoTAELRMXVPzj1D0i_6ajJ6gyD22b3jh\n\n### Final Goal Outcome: \n1. Techniques to Perform Statistical Analysis\n2. Familiar with all concepts which will be important for Machine Learning\n\n## 3. Databases\n1. Mongodb: https://www.youtube.com/watch?v=magzEfYqIos&list=PLZoTAELRMXVN_8zzsevm1bm6G-plsiO1I\n![download (3)](https://user-images.githubusercontent.com/20041231/211718010-30412d4c-9cea-4ae7-858e-a761d0240812.png)\n\n2. Mysql:https://www.youtube.com/watch?v=us1XyayQ6fU&list=PLZoTAELRMXVNMRWlVf0bDDSxNEn38u9Cl\n\n![download (4)](https://user-images.githubusercontent.com/20041231/211718051-2af3c5ba-cb6d-451b-85ea-5bb01abd9869.png)\n\n## 4. Machine Learning\nComplete Around 20-25 Algorithms Both ssupervised And Unsupervised ML\nLive ML Playlist: https://www.youtube.com/watch?v=z8sxaUw_f-M&list=PLZoTAELRMXVPjaAzURB77Kz0YXxj65tYz\n\nComplete ML Playlist: https://www.youtube.com/watch?v=bPrmA1SEN2k&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe\n\nHindi: ML Playlist: https://www.youtube.com/watch?v=7uwa9aPbBRU&list=PLTDARY42LDV7WGmlzZtY-w9pemyPrKNUZ\n\n## Deep Learning Playlist:\n5 DaysLive Deep Learning Playlist: https://www.youtube.com/watch?v=8arGWdq_KL0&list=PLZoTAELRMXVPiyueAqA_eQnsycC_DSBns\n\nComplete Deep Learning Playlist: https://www.youtube.com/watch?v=YFNKnUhm_-s&list=PLZoTAELRMXVPGU70ZGsckrMdr0FteeRUi\n\n## NLP Playlist:\nEnglish: Live NLP Playlist: https://www.youtube.com/watch?v=w3coRFpyddQ&list=PLZoTAELRMXVNNrHSKv36Lr3_156yCo6Nn\n\nComplete NLP Playlist:https://www.youtube.com/watch?v=fM4qTMfCoak&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm\n\n## AIOPS: End To End ML Projects\n1. Github Action CI/CD pipelines\n2. Circle CI\n3. Kubeflow\n4. MLflow\n5. Deployment Technqiues In AWS,AZURE, GCP,Dockers And Kubernetes\n6. Evidently AI\n7. Grafana(Monitoring)\n8. AirFlow\n\n## Internships:\n![Red White Modern Youtube Thumbnail (2)](https://user-images.githubusercontent.com/20041231/211718275-abc43b47-b53d-40d7-a93c-24aceaa6bec8.jpg)\n\niNeuron Internships: https://internship.ineuron.ai/\n\n## Data Science Tracker Sheet For Learning\nhttps://drive.google.com/file/d/18doA_wMja2nAawcE6imIcfnEMf-Pir2n/view\n\n\n## Best Affordable Data Science Course From Pwskills(6-7 Months)\n\nImpact Batch:- Data-Science-Masters (Full Stack Data Science)\n1. Data Science Masters Hindi: https://bit.ly/3CKX1od (Hindi)\n2. Data Science Masters English: https://bit.ly/3iEjWuH (English)\n"
    },
    "Advanced-RAG-Q-A-Chatbot-With-Chain-And-Retrievers-Using-Langchain": {
        "description": null,
        "readme": "# Advanced RAG Q&A Chatbot with LangChain\n\nThis project implements a powerful Question-Answering chatbot using LangChain's Retrieval-Augmented Generation (RAG) concepts. The chatbot can process uploaded PDF documents, answer complex questions based on their content, and leverage advanced techniques like chains and retrievers.\n\n**Key Features**\n\n*   **PDF Document Processing:** Ability to upload and extract knowledge from PDF files.\n*   **Retrieval-Augmented Generation (RAG):** Employs retrievers to access relevant information and chains to structure the question answering process.\n*   **Streamlit Integration:** User-friendly web interface powered by Streamlit.\n*   **LangChain:** Built on the flexible and powerful LangChain framework.\n*   **OpenAI Integration (Optional):** Support for integrating OpenAI's language models like GPT-3.5.\n\n**Prerequisites**\n\n*   Python 3.x\n*   A LangChain API Key (if specified in the code)\n*   An OpenAI API Key (if you're using OpenAI models)\n\n**Installation**\n\n1.  Clone this repository:\n    ```bash\n    git clone [https://github.com/](https://github.com/)<your-username>/<your-repo-name>\n    ```\n2.  Navigate to the project directory:\n    ```bash\n    cd <your-repo-name>\n    ```\n3.  Create a virtual environment (recommended):\n    ```bash\n    python -m venv env\n    env\\Scripts\\activate  # For Windows\n    source env/bin/activate  # For Linux/macOS\n    ```\n4.  Install dependencies:\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n**Usage**\n\n1.  **Environment Variables:**\n    *   Create a `.env` file in the project root.\n    *   Add the following, replacing placeholders with your actual keys:\n        ```\n        OPENAI_API_KEY=<your_openai_api_key>\n        LANGCHAIN_API_KEY=<your_langchain_api_key>\n        ```\n    *   Load the environment variables using the `dotenv` library (already included in the code).\n\n2.  **Start the Application:**\n    ```bash\n    streamlit run app.py \n    ```\n\n3.  **Using the Web Interface**\n    *   Open `http://localhost:8501` (or the provided Streamlit URL) in your web browser.\n    *   Upload a PDF file.\n    *   Type your question in the text box.\n    *   Click \"Submit\" to get the answer.\n\n**Customization**\n\n*   **LLM Choice:** Modify the code to switch between OpenAI and Ollama language models.\n*   **Embedding Techniques:** Experiment with other embedding providers.\n*   **Streamlit Enhancements:** Add more interactive elements or styling to the web interface.\n\n**Contributions**\n\nThis project welcomes contributions! Feel free to add features, fix bugs, or suggest improvements.\n\n**License**\n\n[Specify the license of your project, e.g., MIT, Apache 2.0, etc.]\n"
    },
    "BostonHousePricing": {
        "description": null,
        "readme": "### BostonHousePricing\n\n\n### Softwares and tools required\n[Github account](https://hithub.com)\n[VS Code](https://code.visualstudio.com/)"
    },
    "Chat_Groq_Document_Q_A": {
        "description": null,
        "readme": "## Deployed Link\nDocument Q&A app is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/Chat_Groq_Document_Q_A)\n\n## Screenshots\n![chat_groq_1](https://github.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/6125958f-93ac-4399-aa86-55a9f2613801)\n![chat_groq_5](https://github.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/c0fc5328-cad2-4e13-9b61-5323fb653305)\n![chat_groq_6](https://github.com/Parthiban-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/574bd55a-0726-48b3-aff7-e6d9ddfd874c)\n\n# Chat Groq Document Q&A\n\nThis Streamlit web application allows you to upload PDF documents, embed them using Google's Generative AI Embeddings, and perform question-answering based on the uploaded documents. The app utilizes the powerful Groq language model and its efficient inference engine to provide accurate and relevant answers to your queries. The application follows the RAG (Retrieval-Augmented Generation) approach, which combines the strengths of retrieval systems and generative language models.\n\n## Features\n\n- **PDF Document Upload**: Upload multiple PDF files to create a knowledge base.\n- **Document Embedding**: Leverage Google's Generative AI Embeddings to embed the uploaded PDF documents.\n- **Question Answering**: Ask questions related to the uploaded documents, and receive precise answers powered by the Groq's LPU inference engine.\n- **Custom Prompt Templates**: Customize the prompt template for question-answering to suit your specific needs.\n- **Model Selection**: Choose from a variety of available Groq open source models, including `llama3-8b-8192`, `llama3-70b-8192`, `mixtral-8x7b-32768`, and `gemma-7b-it`.\n- **Document Similarity Search**: Explore relevant document chunks that match the provided question.\n- **Response Time Tracking**: Monitor the response time for each query.\n\n## RAG (Retrieval-Augmented Generation) Approach\n\nThe application follows the RAG approach, which combines the strengths of retrieval systems and generative language models. Here's how it works:\n\n- **Retrieval**: The application embeds the uploaded PDF documents using Google's Generative AI Embeddings and stores them in a vector store (FAISS). When a user asks a question, the relevant document chunks are retrieved from the vector store based on their similarity to the question.\n\n- **Augmentation**: The retrieved document chunks are combined and provided as context to the Groq language model. This augments the language model's knowledge with relevant information from the uploaded documents.\n\n- **Generation**: The Groq language model uses the provided context and the question to generate a precise and relevant answer. The language model's generative capabilities allow it to synthesize information from the retrieved document chunks and produce a coherent response.\n\nThe RAG approach offers several advantages:\n\n- **Scalability**: By leveraging a retrieval system, the application can handle large knowledge bases efficiently, allowing users to query information from extensive document collections.\n- **Contextual Understanding**: The language model can better understand the context of the question and provide more accurate and nuanced answers by using the relevant document chunks as context.\n- **Knowledge Grounding**: The generated answers are grounded in the factual information present in the uploaded documents, ensuring the reliability and trustworthiness of the responses.\n\n## Advantages of Groq LPU Inference Engine\n\n- **High Performance**: Groq's LP (Learning Processor) Inference Engine is designed to deliver exceptional performance for large language models, enabling fast and efficient question answering.\n- **Energy Efficiency**: The LP Inference Engine is optimized for energy efficiency, making it suitable for deployment on various devices and environments.\n- **Scalability**: Groq's architecture allows for seamless scaling of language models, ensuring that the application can handle increasing demands and larger knowledge bases.\n- **Low Latency**: The LP Inference Engine provides low-latency inference, ensuring quick response times for user queries.\n\n## Langsmith Integration\n\nThis application integrates with Langsmith, a platform for monitoring and managing large language model deployments. Langsmith provides the following benefits:\n\n- **Log Monitoring**: Monitor and analyze logs generated by the application, making it easier to debug and troubleshoot issues.\n- **Cost Optimization**: Langsmith helps optimize the cost of running language models by providing insights into usage patterns and suggesting ways to reduce costs.\n- **Performance Monitoring**: Track the performance of the application and identify potential bottlenecks or areas for improvement.\n\n"
    },
    "Chat_With_IPYNB_Files": {
        "description": null,
        "readme": "# Chat Jupyter Notebooks files using Google Gemini\n\nThis Streamlit web application allows you to upload Jupyter Notebook files (.ipynb), embed them using Google's Generative AI Embeddings, and perform question-answering based on the uploaded notebooks. The app utilizes the powerful Gemini language model and its efficient inference engine to provide accurate and relevant answers to your queries. The application follows the RAG (Retrieval-Augmented Generation) approach, which combines the strengths of retrieval systems and generative language models.\n\n## Deployed Link\n\nChat with .IPYNB files app is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/Chat_With_IPYNB_Files)\n\n## Screenshots\n\n![Capture](https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A/assets/26496805/e7ccdc0a-82e5-4ef2-92b5-b03dc24d519c)\n\n\n## Features\n\n- **Notebook File Upload**: Upload multiple Jupyter Notebook files (.ipynb) to create a knowledge base.\n- **Document Embedding**: Leverage Google's Generative AI Embeddings to embed the uploaded notebook documents.\n- **Question Answering**: Ask questions related to the uploaded notebooks, and receive precise answers powered by the Gemini model.\n- **Custom Prompt Templates**: Customize the prompt template for question-answering to suit your specific needs.\n- **Model Selection**: Choose from a variety of available Gemini models, including `models/gemini-1.0-pro`, `models/gemini-1.0-pro-001`, and more.\n- **Document Similarity Search**: Explore relevant document chunks that match the provided question.\n- **Response Time Tracking**: Monitor the response time for each query.\n\n## RAG (Retrieval-Augmented Generation) Approach\n\nThe application follows the RAG approach, which combines the strengths of retrieval systems and generative language models. Here's how it works:\n\n- **Retrieval**: The application embeds the uploaded notebook documents using Google's Generative AI Embeddings and stores them in a vector store (FAISS). When a user asks a question, the relevant document chunks are retrieved from the vector store based on their similarity to the question.\n- **Augmentation**: The retrieved document chunks are combined and provided as context to the Gemini language model. This augments the language model's knowledge with relevant information from the uploaded documents.\n- **Generation**: The Gemini language model uses the provided context and the question to generate a precise and relevant answer. The language model's generative capabilities allow it to synthesize information from the retrieved document chunks and produce a coherent response.\n\nThe RAG approach offers several advantages:\n\n- **Scalability**: By leveraging a retrieval system, the application can handle large knowledge bases efficiently, allowing users to query information from extensive document collections.\n- **Contextual Understanding**: The language model can better understand the context of the question and provide more accurate and nuanced answers by using the relevant document chunks as context.\n- **Knowledge Grounding**: The generated answers are grounded in the factual information present in the uploaded documents, ensuring the reliability and trustworthiness of the responses.\n\n## Advantages of Gemini Model\n\n- **High Performance**: The Gemini model is designed to deliver exceptional performance for large language models, enabling fast and efficient question answering.\n- **Energy Efficiency**: The model is optimized for energy efficiency, making it suitable for deployment on various devices and environments.\n- **Scalability**: Gemini's architecture allows for seamless scaling of language models, ensuring that the application can handle increasing demands and larger knowledge bases.\n- **Low Latency**: The model provides low-latency inference, ensuring quick response times for user queries.\n\n\n\n\n"
    },
    "Chat_With_Multiple_Data_Sources": {
        "description": null,
        "readme": "# Chat_With_Multiple_Data_Sources\n\nThis Streamlit web application leverages LangChain to provide an advanced query assistant that integrates multiple tools and data sources. Users can query information from Wikipedia, arXiv, and custom URLs. The application also supports custom prompts and maintains conversation history.\n\n\n## Deployed Link\nDocument Q&A app is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/Chat_With_Multiple_Data_Sources)\n\n\n## Screenshots\n![data3_2](https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A/assets/26496805/4e7b3d1b-c660-4340-88e0-3cf91c1c8d57)\n![data3_3](https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A/assets/26496805/d5ee0b91-8121-48ea-9fa4-6aff0bc68ae9)\n![data3_1](https://github.com/Parthiban-3997/Chat_Groq_Document_Q_A/assets/26496805/0aba916f-d173-4f9a-900d-a2328992d44c)\n\n\n## Features\n- **Integration with Wikipedia and arXiv**: Retrieve information directly from Wikipedia and arXiv.\n- **Custom URL Support**: Load documents from custom URLs seperated by commas (,) and search within them.\n- **Conversation History**: Maintains a conversation history using LangChain's ConversationBufferMemory.\n- **Custom Prompts**: Users can define their custom prompts to tailor the assistant's responses.\n\n### LangChain Toolkits\nLangChain toolkits provide a set of utilities and pre-built components that simplify the interaction with language models:\n\n- **Document Loaders**: Tools to load documents from various sources. For example, `WebBaseLoader` allows loading web pages and splitting them into manageable chunks.\n- **Text Splitters**: Components like `RecursiveCharacterTextSplitter` help divide documents into chunks of a specified size, optimizing them for processing.\n- **Embeddings**: `OpenAIEmbeddings` generate vector representations of text, useful for similarity searches and information retrieval.\n- **Vector Stores**: `FAISS` is used to store and query vector embeddings efficiently.\n\n### LangChain Agents\nLangChain agents are orchestrators that manage the interaction between the language model and various tools:\n\n- **create_openai_tools_agent**: This function creates an agent that can interact with multiple tools using an OpenAI language model. The agent is configured with a prompt template and can utilize tools like WikipediaQueryRun, ArxivQueryRun, and custom URL retrievers.\n- **AgentExecutor**: Manages the execution of the agent, handling user inputs and generating responses.\n\n## Components\n\n### Main Components\n1. **ChatOpenAI**: The main language model used for generating responses.\n2. **ConversationBufferMemory**: Stores the conversation history, allowing the agent to maintain context across multiple interactions.\n3. **WikipediaQueryRun**: A tool for retrieving information from Wikipedia.\n4. **ArxivQueryRun**: A tool for retrieving information from arXiv.\n5. **WebBaseLoader**: Loads documents from specified URLs.\n6. **FAISS**: Vector store for managing document embeddings.\n7. **ChatPromptTemplate**: Defines the structure of prompts used by the language model.\n\n### Configuration\nThe application allows users to configure the following settings via the sidebar:\n\n- **OpenAI API Key**: For accessing OpenAI's language models.\n- **Custom URLs**: Users can provide URLs to load and search documents.\n- **Custom Prompts**: Define custom prompt templates for tailored responses.\n\n### Usage\nUsers can enter their queries in the main input area. The agent processes the query using the configured tools and returns the response. The conversation history is displayed, showing interactions between the user and the assistant.\n\n### Memory Management\nThe application uses `ConversationBufferMemory` to retain the conversation history, ensuring context is preserved across multiple user queries. This memory is updated with each interaction, providing a coherent and continuous conversation experience.\n\n## Example Usage\n1. Enter your OpenAI API Key in the sidebar.\n2. Optionally, provide URLs to load custom documents.\n3. Define custom prompts if needed.\n4. Click \"Load Tools\" to initialize the agent.\n5. Enter your query in the main input area and receive a response based on the integrated tools and data sources.\n"
    },
    "Chat_With_Multiple_SQL_Databases": {
        "description": null,
        "readme": "# SQL Chat Assistant with Multi-Database Support\n\nThis project is a Streamlit application that allows users to interact with multiple SQL databases through a natural language interface. The application leverages the power of language models, such as GPT-4 and GROQ models, to understand user queries and generate appropriate SQL queries to retrieve the desired information from the connected databases.\n\n\n## Deployed Link\n\nDocument Q&A app is Deployed And Available [Here](https://chatwithmultiplesqldatabases.streamlit.app/)\n\n\n## Screenshots\n\n![db_1](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/a2f233a3-8100-42a9-9a26-f42047a4d36e)\n![db_2](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/72417ea9-baa6-41fe-8d3c-07a8b9bca5a3)\n![db_3](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/d1e18b55-ffa7-4cf8-a60b-c000ef816f0a)\n![db_4](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/d8fdecef-0d0b-424f-936f-8e7ad3a80743)\n![db_5](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/108f2f5b-b2ed-48b9-b892-74bbc8cef64d)\n![db_6](https://github.com/Parthiban-3997/Chat_With_Multiple_Data_Sources/assets/26496805/741b27f6-5ef6-49c0-866b-97831e1b3824)\n\n## Key Features\n\n- **Multi-Database Support**: The application can connect to multiple databases simultaneously, allowing users to query data across different data sources.\n- **Natural Language Interface**: Users can ask questions about the databases in natural language, and the application will translate the queries into SQL queries and execute them against the relevant databases.\n- **Chain of Thought Prompting**: This project utilizes the Chain of Thought prompting technique from LangChain, which encourages the language model to break down complex problems into smaller steps, leading to more accurate and interpretable results.\n- **Few-Shot Learning**: The application provides a set of examples to the language model, allowing it to learn the desired behavior and generate more accurate SQL queries based on the provided context.\n\n## Project Uniqueness\n\nThis project stands out due to its innovative approach to handling multi-database queries using natural language. By combining the power of language models with LangChain's Chain of Thought prompting and Few-Shot Learning techniques, the application can effectively understand and process complex queries spanning multiple databases.\n\nThe use of Chain of Thought prompting encourages the language model to break down the problem into smaller steps, making it easier to understand the user's intent and generate appropriate SQL queries. Additionally, Few-Shot Learning allows the model to learn from a set of examples, enhancing its ability to generate accurate SQL queries for a wide range of scenarios.\n\nFurthermore, the application's ability to connect to multiple databases simultaneously makes it a powerful tool for data analysts and business professionals who need to work with data from different sources. This feature eliminates the need for manual data consolidation and enables seamless querying across multiple data sources.\n\nConsider the following diagram to understand how the different chains and components are built:\n\n![Chatbot Architecture](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/d833ca1e-a51d-47b6-a074-6703dd957525)\n\n\n## Getting Started\n\nTo run the SQL Chat Assistant with Multi-Database Support locally, follow these steps:\n\n1. Clone the repository: `git clone https://github.com/your-repo/sql-chat-assistant.git`\n2. Install the required dependencies: `pip install -r requirements.txt`\n3. Set up your database connections.\n4. If you're connecting to a local database, you'll need to expose it using ngrok. Install the ngrok .exe file from [here](https://ngrok.com/download) and generate the authtoken from [here](https://dashboard.ngrok.com/get-started/your-authtoken).\n5. Finally run the following command to start ngrok and expose your local database on port 3306: `ngrok tcp 3306`\n6. Copy the ngrok URL (e.g., `tcp://x.tcp.ngrok.io:12345`) and use it as the `Host` and 'Port' value.\n7. Run the Streamlit application: `streamlit run app.py`\n\nFor detailed instructions and additional configuration options, please refer to the project's documentation.\n\n## Contributing\n\nContributions to this project are welcome! If you have any ideas, bug fixes, or improvements, feel free to submit a pull request. Please ensure that your code adheres to the project's coding standards and is well-documented.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n"
    },
    "Comparison-of-Supervised-and-Unsupervised-Image-Recognition": {
        "description": null,
        "readme": "# Comparison of Supervised and Unsupervised Image Recognition\n\n## Image Classification and Denoising with CIFAR-10\n\nThis document summarizes the processes, results, and analysis of applying different machine learning models on the CIFAR-10 dataset for image classification and image denoising tasks.\n\n## 1. Image Classification\n\nTwo different approaches were implemented for image classification:\n\n### 1.1 Support Vector Machine (SVM)\n\n#### Training:\n- The CIFAR-10 dataset was loaded using TensorFlow.\n- Pixel values were preprocessed by normalizing to a range of 0-1 and flattening the 32x32x3 image arrays into 1D vectors.\n- An SVM classifier with an RBF kernel was trained on the training set using all available CPU cores (n_jobs=-1) for faster training. GPU acceleration was attempted but not found to be beneficial for this specific task and model.\n- The trained model was saved to svm_model.pkl using pickle.\n\n#### Testing:\n- The saved SVM model was loaded.\n- Predictions were made on the preprocessed test set.\n- Accuracy: 54%\n- Classification Report: The classification report revealed that the model performs decently for some classes like 'airplane', 'automobile', and 'ship' but struggles with others like 'cat', 'deer', and 'frog'. This suggests that the features learned by the SVM with an RBF kernel might not be discriminative enough for all classes in the CIFAR-10 dataset.\n\n#### Prediction Examples:\n- Predictions were made on a separate set of images stored in the Images/ directory.\n- The predicted class labels were displayed alongside the images.\n- Predictions were also visualized on a subset of the test set images to further illustrate the model's performance.\n\n### 1.2 Convolutional Neural Network (CNN)\n\n#### Training:\n- The CIFAR-10 dataset was loaded and preprocessed similarly to the SVM approach.\n- The training set was further split into training and validation sets (80%-20% split).\n- A CNN model was defined using the Keras Sequential API. The architecture included:\n  - Convolutional layers with ReLU activation\n  - Max-pooling layers\n  - Dropout for regularization\n  - Dense layers with ReLU activation\n  - A final dense layer with softmax activation for multi-class classification\n- The model was compiled using the Adam optimizer with a learning rate of 0.001 and sparse categorical cross-entropy loss.\n- Training was performed for 20 epochs with a batch size of 32.\n- Training and validation accuracy and loss were plotted.\n- Test Accuracy: 76.28%\n\n#### Model Summary:\n- The model.summary() function provided a detailed overview of the CNN architecture, including the output shape and number of parameters for each layer.\n\n#### Saving the Model:\n- The trained CNN model was saved to CNNFinal_Model.h5.\n\n#### Evaluation:\n- The saved CNN model was loaded.\n- Predictions were made on the test set, and a classification report was generated.\n- Classification Report: The CNN model significantly outperformed the SVM, achieving an accuracy of 76.28%. This highlights the superiority of CNNs for image classification tasks due to their ability to learn spatial hierarchies of features.\n\n#### Prediction Examples:\n- Similar to the SVM, predictions were visualized for images from the Images/ directory and a subset of the test set.\n\n## 2. Image Denoising\n\nAn Autoencoder was implemented for image denoising:\n\n### 2.1 Autoencoder\n\n#### Training:\n- The CIFAR-10 dataset was loaded, and Gaussian noise was added to the images to create a noisy dataset.\n- An autoencoder model was defined using the Keras Sequential API. The architecture consisted of:\n  - Encoder: Convolutional and MaxPooling layers to learn a compressed representation of the input.\n  - Decoder: Convolutional and UpSampling layers to reconstruct the denoised image from the compressed representation.\n- The model was compiled using the Adam optimizer and mean squared error loss.\n- Training was performed for a specified number of epochs.\n- The trained autoencoder model was saved to Denoising_Model_Final.h5.\n\n#### Testing:\n- The saved autoencoder model was loaded.\n- The model was evaluated on a noisy test set.\n- Test Loss: 22982.15\n- Test Accuracy: 33.79%\n\n#### Prediction Examples:\n- A visualize_data function was defined to display grids of images.\n- The function was used to visualize:\n  - Noisy images from the test set.\n  - Denoised images predicted by the autoencoder.\n  - Original clean images for comparison.\n- The autoencoder's denoising capability was further demonstrated on:\n  - Images with smaller resolutions.\n  - Images with larger resolutions.\n\n## 3. Image Classification using K-means with PCA\n\n#### Training:\n- The CIFAR-10 dataset was loaded and preprocessed by normalizing pixel values and flattening the images.\n- PCA (Principal Component Analysis) was applied to reduce the dimensionality of the data while retaining 99% of the variance.\n- A K-means clustering model was trained on the PCA-transformed training data with 10 clusters, representing the 10 classes in CIFAR-10.\n- The trained K-means model was saved to kmeans_model.pkl.\n\n#### Evaluation:\n- The saved K-means model was loaded.\n- Silhouette Score: 0.0538\n- Davies-Bouldin Score: 2.7032\n- These scores suggest that the clustering is not very distinct, which is expected given the complexity of image data and the limitations of K-means in high-dimensional spaces.\n\n#### Test Accuracy: 22.11%\n\n#### Prediction Examples:\n- Predictions were visualized on a subset of the test set images.\n- Predictions were also made and displayed for images from the Images/ directory.\n\n## Conclusion\n\nThis project explored different machine learning techniques for image classification and denoising on the CIFAR-10 dataset. The CNN model achieved the highest accuracy (76.28%) for image classification, demonstrating the power of deep learning for this task. The autoencoder showed promising results in denoising images, while the K-means approach, though less accurate, provided insights into clustering image data. Future work could involve exploring more complex CNN architectures, fine-tuning hyperparameters, and experimenting with different autoencoder designs for improved performance."
    },
    "Data-Analyst-Roadmap": {
        "description": null,
        "readme": "# Data Analyst RoadMap\n\n## Work Of Data Analyst\n\n![Capture](https://user-images.githubusercontent.com/20041231/211466839-e0145119-20fd-4efe-bbd7-d2c3b10fdfba.JPG)\n\n![Capture](https://user-images.githubusercontent.com/20041231/211468652-d4316856-0ee5-44ea-9dd9-538beef38180.JPG)\n\n\n## 1. Programming Language\n### Python \n![Python-Symbol](https://user-images.githubusercontent.com/20041231/211466229-df1c12da-ed6e-4bb9-97f7-84871a287580.png)\n\nPython In English:https://www.youtube.com/watch?v=bPrmA1SEN2k&list=PLZoTAELRMXVNUL99R4bDlVYsncUNvwUBB\n\nPython In Hindi: https://www.youtube.com/watch?v=MJd9d9Mpxg0&list=PLTDARY42LDV4qqiJd1Z1tShm3mp9-rP4v\n\n### R Programming Language\n![unnamed (1)](https://user-images.githubusercontent.com/20041231/211466981-43ebae2c-0581-4604-8b01-35a97d350080.png)\n\n## 2. Statistics\n![1_UAGU532MbhR5cm3symwWqg](https://user-images.githubusercontent.com/20041231/211467108-a82c82fa-4366-440b-8294-5bd3e0bbf081.png)\n\nEnglish: 7 Days Statistics Live Session :https://www.youtube.com/watch?v=11unm2hmvOQ&list=PLZoTAELRMXVMgtxAboeAx-D9qbnY94Yay\n\nStatistics in ML(43 videos): https://www.youtube.com/watch?v=zRUliXuwJCQ&list=PLZoTAELRMXVMhVyr3Ri9IQ-t5QPBtxzJO\n\nHindi: Stats Playlist: https://www.youtube.com/watch?v=7y3XckjaVOw&list=PLTDARY42LDV6YHSRo669_uDDGmUEmQnDJ\n\n### EDA\nEDA Live :https://www.youtube.com/playlist?list=PLZoTAELRMXVPzj1D0i_6ajJ6gyD22b3jh\n\nEDA Detailed Playlist: https://www.youtube.com/watch?v=ioN1jcWxbv8&list=PLZoTAELRMXVPQyArDHyQVjQxjj_YmEuO9\n\n### Feature Engineering\n\nComplete Detailed Feature Engineering: https://www.youtube.com/watch?v=6WDFfaYtN6s&list=PLZoTAELRMXVPwYGE2PXD3x0bfKnR0cJjN\n\nLive EDA Feature Engineering Playlist: https://www.youtube.com/watch?v=bTN-6VPe8c0&list=PLZoTAELRMXVPzj1D0i_6ajJ6gyD22b3jh\n\n### Final Goal Outcome: \n1. Techniques to Perform Statistical Analysis\n2. Familiar with all concepts which will be important for Machine Learning\n\n## Databases\n![MySQL-Logo](https://user-images.githubusercontent.com/20041231/211467463-3fe5e606-e11b-49cf-9071-3abdd25584c8.png)\n![MongoDB_Logo svg](https://user-images.githubusercontent.com/20041231/211467512-c1e429b7-1bb9-47c0-b39c-99932b9043a5.png)\n1. Mongodb: https://www.youtube.com/watch?v=magzEfYqIos&list=PLZoTAELRMXVN_8zzsevm1bm6G-plsiO1I\n2. Mysql:https://www.youtube.com/watch?v=us1XyayQ6fU&list=PLZoTAELRMXVNMRWlVf0bDDSxNEn38u9Cl\n\n## ETL Tools\n![infa-social-share-2022](https://user-images.githubusercontent.com/20041231/211472581-3b05b5d7-2d76-4002-835a-172cada11672.png)\n\nAmazing Article on ETL: https://www.informatica.com/resources/articles/what-is-etl.html\n\n![Capture](https://user-images.githubusercontent.com/20041231/211473712-f8737fd4-0622-49de-b751-706322813b31.JPG)\n\nFSDA 2.0: https://ineuron.ai/course/Full-Stack-Data-Analytics-2.0\n\nTop ETL Tools: https://www.integrate.io/blog/top-7-etl-tools/\n\n## Business Intelligence Tools\n\n![maxresdefaultreduced](https://user-images.githubusercontent.com/20041231/211717386-39f75daf-0c1e-4295-8e00-657578f397a1.jpg)\n\n![Tableau-logo](https://user-images.githubusercontent.com/20041231/211717508-ccd3da44-1e33-47c0-a1ec-66583f73c496.jpg)\n\n## Advanced Excel\n![png-clipart-microsoft-excel-app-store-spreadsheet-microsoft-text-logo](https://user-images.githubusercontent.com/20041231/211717601-a7520e3b-bd50-4cbd-957a-8ff2bbce25b3.png)\n\n## Real Time Internships\n![Red White Modern Youtube Thumbnail (2)](https://user-images.githubusercontent.com/20041231/211743026-4f3da97c-897b-4d10-acea-503591ca935d.jpg)\n\nhttps://internship.ineuron.ai/\n\n## Best Affordable Data Science Course From Pwskills(6-7 Months)\n\nImpact Batch:- Data-Science-Masters (Full Stack Data Science)\n1. Data Science Masters Hindi: https://bit.ly/3CKX1od (Hindi)\n2. Data Science Masters English: https://bit.ly/3iEjWuH (English)\n\n"
    },
    "DocMulti-Chat-Assistant-Using-LlamaIndex": {
        "description": null,
        "readme": "# DocMulti Chat Assistant Using LlamaIndex \ud83e\udd99\n\n## Overview\nDocMulti Chat Assistant is a powerful Streamlit-based application that leverages LlamaIndex to provide an interactive chat interface for querying multiple documents. This tool excels at processing and understanding complex data structures, ensuring that no information is lost during the parsing and indexing stages.\n\n\n## Deployed Link\nDocMulti-Chat-Assistant app is Deployed And Available [Here](https://huggingface.co/spaces/Parthiban97/DocMulti_Chat_Assistant_Using_LlamaIndex)\n\n\n## Screenshots\n![llamaindex_1](https://github.com/user-attachments/assets/15b00af3-e8a6-4a09-a0ab-b5bf9fe53f54)\n![llamaindex_2](https://github.com/user-attachments/assets/7d8925f2-6be2-4588-a0ad-d88ec14c8dec)\n![llamaindex_3](https://github.com/user-attachments/assets/595e7cd3-d99a-41c4-8a2a-66efddc9b4aa)\n![llamaindex_4](https://github.com/user-attachments/assets/7224ff3e-e47a-471f-bceb-6a699f1d228c)\n\n### Examples from uploaded document file structures\n![llamaindex_5](https://github.com/user-attachments/assets/c1b1b728-eb23-4ebf-a92b-9712ca8bff9c)\n\n## Features\n- **Multi-Document Support**: Upload and process multiple document types.\n- **Advanced Parsing**: Option to use LlamaParse for complex documents with graphs and tables.\n- **Customizable Models**: Choose from various Groq models for text generation.\n- **Flexible Embedding**: Utilizes Google's Gemini for document embedding.\n- **Interactive Chat Interface**: Engage in a conversation about your documents.\n- **Memory Retention**: Maintains context across multiple queries using a chat memory buffer.\n- **Custom Parsing Instructions**: Tailor the document parsing process with custom instructions.\n- **Custom Prompt Templates**: Define your own prompts for more specific interactions.\n\n## Components\n- **Streamlit**: For the web interface.\n- **LlamaIndex**: Core indexing and querying engine.\n- **Groq**: Large Language Model for text generation.\n- **Google Gemini**: For document embedding.\n- **LlamaParse**: Optional advanced document parser.\n\n## Usage\n1. Launch the Streamlit app.\n2. In the sidebar:\n   - Enter your API keys for Groq, Google, and Llama Cloud.\n   - Select a Groq model.\n   - Upload your documents.\n   - Choose whether to use LlamaParse.\n   - Set any advanced options like custom parsing instructions or prompt templates.\n3. Click \"Start Document Indexing\" to process your documents.\n4. Once indexed, use the chat interface to ask questions about your documents.\n\n## Supported Document Types\nDocMulti Chat Assistant supports an extensive range of document formats, making it incredibly versatile for various use cases. Here's the full list of supported extensions:\n\n- **Text Documents**: .txt, .rtf, .md\n- **Microsoft Office**: .doc, .docx, .docm, .dot, .dotm, .xls, .xlsx, .xlsm, .xlsb, .ppt, .pptx, .pptm, .pot, .potm, .potx\n- **OpenDocument**: .odt, .ods, .odp\n- **PDF**: .pdf\n- **eBooks**: .epub\n- **Images**: .jpg, .jpeg, .png, .gif, .bmp, .svg, .tiff, .webp\n- **Web**: .htm, .html, .xml\n- **Spreadsheets**: .csv, .tsv, .dif, .sylk, .slk, .prn\n- **Database**: .dbf\n- **Other Office Suites**: \n  - Apple iWork: .pages, .numbers, .key\n  - WordPerfect: .wpd\n  - Lotus: .wks, .123\n  - Quattro Pro: .qpw\n- **Specialized Formats**: .602, .abw, .cgm, .cwk, .hwp, .lwp, .mw, .mcw, .pbd, .sda, .sdd, .sdp, .sdw, .sgl, .sti, .sxi, .sxw, .uof, .uop, .uot, .vor, .wps, .zabw, .et, .eth, .wk1, .wk2, .wk3, .wk4, .wq1, .wq2, .wb1, .wb2, .wb3, .xlr\n\n## Advanced Features\n- **Custom Parsing Instructions**: Tailor how LlamaParse extracts information from your documents.\n- **Custom Prompt Templates**: Create specific prompts to guide the AI's responses.\n- **Adjustable Model Parameters**: Select different Groq models to balance between speed and capability.\n\n## Notes\n- Ensure you have valid API keys for Groq, Google, and Llama Cloud before using the application.\n- Processing time may vary depending on the number and complexity of uploaded documents.\n\n## Troubleshooting\n- If you encounter errors during document processing, check your API keys and internet connection.\n- For issues with specific document types, try toggling the LlamaParse option.\n\n## Contributing\nContributions to improve DocMulti Chat Assistant are welcome. Please submit pull requests or open issues on the project repository.\n\n"
    },
    "Excel-Sales-Analytics": {
        "description": null,
        "readme": "## Sales Report :\n\n\n- **Project objective:** \n\n    **1.** Create a _[customer performance report](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/Customer%20Performance%20Report.pdf)_ \n\n    **2.** Conduct a comprehensive comparison between _[market performance and sales targets](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/Market%20Performance%20vs%20Target%20Report.pdf)_\n\n- **Purpose of sales analytics:** Empower businesses to monitor and evaluate their sales activities and performance.\n\n- **Importance of analyzing sales data:** Identify sales patterns and track key performance indicators (KPIs).\n\n- **Role of reports:** Determine effective customer discounts, facilitate negotiations with consumers, and identify potential business expansion opportunities in promising countries.\n\n\n## Finance Report :\n\n- **Project objective:** \n\n    **1.** Create Profit and Loss (P&L) reports by _[Fiscal Year](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/P%26L%20Statement%20by%20Fiscal%20Year.pdf)_ & _[Months](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/P%26L%20Statement%20by%20Months.pdf)_ \n\n   **2.** Create Profit and Loss (P&L) reports by _[Markets](https://github.com/Parthiban-3997/Excel-Sales-Analytics/blob/main/P%26L%20Statement%20by%20Markets.pdf)_\n\n- **Purpose of sales analytics:** Evaluation of financial performance, support decision-making, and facilitate communication with stakeholders.\n\n- **Importance of analyzing Finance data:** Aid in benchmarking against industry peers and previous periods Foundation for budgeting and forecasting.\n\n- **Role of reports:** Align financial planning with strategic goals Instill confidence in the organization's financial outlook.\n\n\n## Technical & Soft Skills:\n- [x]\tProficiency in ETL methodology (Extract, Transform, Load).\n- [x]\tSkills to generate a date table using Power Query.\n- [x]\tAbility to derive fiscal months and quarters.\n- [x]\tEstablishing data model relationships with Power Pivot.\n- [x]\tProficiency in incorporating supplementary data into an existing data model.\n- [x]\tUtilizing DAX to create calculated columns.\n\n## Soft Skills:\n- [x]\tRefined understanding of Sales & Finance Reports\n- [x]\tDesigning user-centric reports with empathy in mind.\n- [x]\tOptimization of report generation through meticulous fine-tuning.\n- [x]\tDeveloping a systematic approach to devising a report building plan.\n"
    },
    "Fake-News-Classifier-using-LSTM": {
        "description": null,
        "readme": "# Fake-News-Classifier-using-LSTM\n"
    },
    "Generative-AI-With-Cloud": {
        "description": null,
        "readme": "# Generative AI On Cloud (Azure And AWS)\n## Prerequisites To Start With\n### 1. Python Programming Language\n1. Complete Python Playlist In English: [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=bPrmA1SEN2k&list=PLZoTAELRMXVNUL99R4bDlVYsncUNvwUBB)\n\n2. Complete Python Playlist In Hindi:   [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=MJd9d9Mpxg0&list=PLTDARY42LDV4qqiJd1Z1tShm3mp9-rP4v)\n\n3. Flask Playlist:    [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=4L_xAWDRs7w&list=PLZoTAELRMXVPBaLN3e-uoVRR9hlRFRfUc)\n\n4. Fast API Tutorials [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=WU65u9d-97c&list=PLZoTAELRMXVPgsojPOHF9i0u2L83-m9P7)\n\n## 2. Basic Machine Learning Natural Language Processing (Day 1 - Day 5) [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=w3coRFpyddQ&list=PLZoTAELRMXVNNrHSKv36Lr3_156yCo6Nn)\n1. Why NLP?\n2. One hot Encoding, Bag Of Words,\n3. TFIDF\n4. Word2vec,AvgWord2vec\n\n## 3. Basic Deep Learning Concepts (Day 1- Day 5) [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=8arGWdq_KL0&list=PLZoTAELRMXVPiyueAqA_eQnsycC_DSBns)\n\n1. ANN - Working Of MultiLayered Neural Network\n2. Forward Propogation, Backward Propogation\n3. Activation Functions, Loss Functions\n4. Optimizers\n\n## 4. Advanced NLP Concepts (Day 6 - Last Video) [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=ZwYtqTaZ2io&list=PLZoTAELRMXVNNrHSKv36Lr3_156yCo6Nn&index=8)\n1. RNN, LSTM RNN\n2. GRU RNN\n3. Bidirection LSTM RNN\n4. Encoder Decoder, Attention is all you need ,Seq to Seq \n5. Transformers\n\n## 5. Generative AI Playlist:\n1. Updated Langchain Playlist [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=KmQOlg5YfU0&list=PLZoTAELRMXVOQPRG7VAuHL--y97opD5GQ&pp=gAQBiAQB)\n2. Updated Langchain Playlist In Hindi [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=tEL833CPhqw&list=PLTDARY42LDV6flFgQLJCcVSXXa58mZ9Ty&pp=iAQB)\n3. Fine Tuning LLm Playlist [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=Vg3dS-NLUT4&list=PLZoTAELRMXVN9VbAx5I2VvloTtYmlApe3&pp=gAQBiAQB)\n4. Google Gemini Playlist [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=it0l6lx3qI0&list=PLZoTAELRMXVNbDmGZlcgCA3a8mRQp5axb&pp=gAQBiAQB)\n5. More Langchain Projects [![YouTube](https://img.shields.io/badge/YouTube-Video-green)](https://www.youtube.com/watch?v=4O1rs7mrNDo&list=PLZoTAELRMXVORE4VF7WQ_fAl0L1Gljtar&pp=gAQBiAQB)\n6. Generative AI On Azure Cloud [![YouTube](https://img.shields.io/badge/YouTube-Video-green)]()\n7. Generative AI On AWS Cloud [![YouTube](https://img.shields.io/badge/YouTube-Video-green)]()\n\n## Youtube Channels For Referring All Videos. Consider Subscribing and pressing the bell icon.\n\n- Krish Naik : https://www.youtube.com/@krishnaik06\n- Krish Naik Hindi: https://www.youtube.com/@krishnaikhindi\n- Success Stories By Krish: https://www.youtube.com/channel/UCNSHtBgZ3dhcpv190JrK_LQ\n- Support Channel and Join this channel to get access to perks: https://www.youtube.com/channel/UCNU_lfiiWBdtULKOw6X0Dig/join\n- Join My whatsapp channel:https://whatsapp.com/channel/0029Va9q4Yh2Jl8NIS1oPX01\n- Join My Telegram Channel:https://t.me/+V0UeLG8ji-F8ThNb\n\n"
    },
    "github-slideshow": {
        "description": "A robot powered training repository :robot:",
        "readme": "# Your GitHub Learning Lab Repository for Introducing GitHub\n\nWelcome to **your** repository for your GitHub Learning Lab course. This repository will be used during the different activities that I will be guiding you through. See a word you don't understand? We've included an emoji \ud83d\udcd6 next to some key terms. Click on it to see its definition.\n\nOh! I haven't introduced myself...\n\nI'm the GitHub Learning Lab bot and I'm here to help guide you in your journey to learn and master the various topics covered in this course. I will be using Issue and Pull Request comments to communicate with you. In fact, I already added an issue for you to check out.\n\n![issue tab](https://lab.github.com/public/images/issue_tab.png)\n\nI'll meet you over there, can't wait to get started!\n\nThis course is using the :sparkles: open source project [reveal.js](https://github.com/hakimel/reveal.js/). In some cases we\u2019ve made changes to the history so it would behave during class, so head to the original project repo to learn more about the cool people behind this project.\n"
    },
    "images": {
        "description": null,
        "readme": "# images"
    },
    "IPL-Data-Analysis-2008-2022": {
        "description": null,
        "readme": "## IPL-Data-Analysis-2008-2022\n\n\nThis project analyzes IPL match data from 2008 to 2022 to gain insights into various aspects of the tournament.It demonstrates a strong understanding of data analysis techniques and the ability to extract valuable insights from real-world data. It showcases the potential of data analysis to enhance our understanding of sports and provide valuable information for decision-making.\n\n\n# Data Sources\n- **Match Data**: A CSV file containing information about each match, including date, teams, venue, toss, result, and margin of victory.\n- **Ball-by-Ball Data**: A CSV file containing detailed information about each ball bowled in every match, including runs scored, wickets taken, and extra runs.\n\n\n# Data Cleaning and Preparation\n- **Handling Missing Data**: Identified and addressed missing values in both datasets.\n- **Data Type Conversion**: Converted relevant columns to appropriate data types (e.g., dates, integers).\n- **Feature Engineering**: Created new features like \"No. of Hundereds\" and \"No. of Fifties\" to analyze batsman performance.\n\n\n# Features Analyzed\n- **Team Performance**: Analyzed team win percentage, winning mode (wickets or runs), and toss preference.\n- **Player Performance**: Identified top run-scorers, wicket-takers, and players with the most centuries and half-centuries.\n- **Venue Impact**: Investigated the influence of different venues on match outcomes.\n- **Toss Decision**: Examined the impact of winning the toss on match results.\n- **Season Trends**: Identified season-wise trends in toss decisions, winning teams, and match outcomes.\n\n\n## Impactful Findings\n1.\tHome Advantage: Teams playing at their home ground tend to have a higher chance of winning.\n2.\tSecond Batting Advantage: Teams winning the toss generally choose to bowl first, indicating a preference for chasing targets.\n3.\tMumbai Indians Dominance: Mumbai Indians have consistently been a dominant team, winning the most matches and having the highest win percentage.\n4.\tToss Decision Trends: While teams generally prefer bowling first, there were specific seasons (2009, 2010, 2013) where teams opted to bat first more often.\n5.\tVenue Impact: Certain venues have been more conducive to high-scoring matches, while others have favored bowlers.\n6.\tTop Performers: The analysis identified top run-scorers and wicket-takers, highlighting the most impactful players in the tournament.\n\n\n## Project Steps\n1.\tData Acquisition: Obtained the IPL match data and ball-by-ball data from reliable sources.\n2.\tData Cleaning: Cleaned and preprocessed the data to handle missing values and inconsistencies.\n3.\tExploratory Data Analysis: Performed exploratory analysis to understand the data distribution and identify key trends.\n4.\tFeature Engineering: Created new features to enhance the analysis and gain deeper insights.\n5.\tStatistical Analysis: Performed statistical analysis to test hypotheses and draw meaningful conclusions.\n6.\tData Visualization: Created insightful visualizations to present the findings in a clear and engaging manner.\n7.\tConclusion and Recommendations: Summarized the key findings and provided recommendations based on the analysis.\n\n\n## Conclusion\nThis project provides a comprehensive analysis of IPL match data, revealing valuable insights into team performance, player statistics, venue impact, and toss decisions. The findings can be used by cricket enthusiasts, analysts, and teams to understand the dynamics of the tournament and make informed decisions.\n\n\n## Future Work\n1. Prediction Models: Develop predictive models to forecast match outcomes based on various factors.\n2. Advanced Feature Engineering: Explore more complex features to improve the accuracy of predictions.\n3. Sentiment Analysis: Analyze social media data to understand fan sentiment and its impact on match outcomes.\n4. Player Performance Prediction: Develop models to predict individual player performance in future matches.\n\n\n\n\n"
    },
    "Langchain_Series_GenAI": {
        "description": null,
        "readme": "# GenAI Chatbot Project\n\n## Overview\nThis project demonstrates the integration of Langchain and OpenAI to create a conversational AI chatbot. The chatbot utilizes the capabilities of Langchain for language processing and OpenAI's GPT-3.5 model for generating responses.\n\n## Requirements\n- Python 3.10\n- langchain_openai\n- langchain_core\n- streamlit\n- python-dotenv\n\n## Installation\n1. Clone the repository:\ngit clone https://github.com/Parthiban-3997/Langchain_Series_GenAI.git\ncd Langchain_Series_GenAI\n\n\n2. Install dependencies:\npip install -r requirements.txt\n\n\n\n## Usage\n1. Set up environment variables:\n- Create a `.env` file in the root directory of your project.\n- Add the following variables to your `.env` file:\n  ```\n  LANGCHAIN_API_KEY=\"your_langchain_api_key_here\"\n  OPENAI_API_KEY=\"your_openai_api_key_here\"\n  ```\n2. Run the Streamlit app:\nstreamlit run app.py\n\n\n3. Enter the topic you want to search for in the text input field and the chatbot will provide responses based on the input.\n\n## Example\n![screenshot](https://github.com/Parthiban-3997/Langchain_Series_GenAI/assets/26496805/ef76bcf3-30cd-4745-b338-842966d75fa2)\n\n## License\nThis project is licensed under the [MIT License](LICENSE).\n\n\n\n"
    },
    "leap-ai-avatars": {
        "description": null,
        "readme": "# Leap AI Avatars \u26a1\ufe0f\n\nWelcome! Bookmark this repo as a starter template for a Headshots or Avatars app built on Leap AI.\n\nIt provides a UI for image upload, trains a custom model on Leap, and then generates images of your subject in various styles.\n\nTry it out [here](https://ai-avatars.vercel.app/)!\n\nLet's get started by forking this repository (button top right), and downloading it to your computer. from there follow the below :)\n\n### Run it locally\n\n1. Open the terminal\n2. Run `npm install` to grab the necessary packages\n3. Hit `npm run dev` to start your server on `http://localhost:3000`\n\n### How to generate images\n\n1. Upload 3-10 photos of yourself\n2. Add your API Key from Leap AI\n3. Add your model ID from Leap AI to use your existing models (Optional)\n\n### Customizations\n\n1. Head to `pages/index.tsx` for editing text, prompts, and colors to match your theme\n2. Adjust prompts and subjectKeyword (ie. @me) in `helpers/prompts.ts`\n3. Adjust the number of images generated w/ the numberOfImages parameter in `/pages/api/generate`\n\n### Deployment\n\n1. Push all your changes to Github (or another git provider)\n2. Head to vercel.app, import your repo, and hit deploy\n3. Note: you will need vercel pro plan or `/pages/api/generate` call will likely timeout after 10 sec. You can also deploy on [Zeet](https://zeet.co/) to avoid this issue.\n\n### Wrapping Up \ud83d\udc4f\n\nThis is huge! You've got an AI Avatars app running on the web, and you can share it with the world.\n\nIf you got value from this -- please give us a star \u2b50\n\nBuilt with [Leap AI](https://tryleap.ai)\n"
    },
    "LLM_Langchain": {
        "description": null,
        "readme": "No README found"
    },
    "Malaria-Prediction": {
        "description": null,
        "readme": "# Malaria-Prediction\n\nThe mail goal is to create a web app which is capable of predicting whether the person got infected by malaria or not.\n"
    },
    "mlflow": {
        "description": null,
        "readme": "# mlflow"
    },
    "MLProject": {
        "description": null,
        "readme": "## End to End ML Projects"
    },
    "Multimodal-Emotion-Recognition-using-Speech-Cues-and-Facial-Expressions": {
        "description": null,
        "readme": "# Multimodal Emotion Recognition System\n\nRecognizing and understanding emotions is crucial in human-computer interaction, as it can greatly enhance decision-making and judgment. This project proposes a comprehensive emotion recognition system that focuses on analyzing candidates' expressions during behavioral interviews.\n\n## Table of Contents\n- [Introduction](#introduction)\n- [Features](#features)\n- [Usage](#usage)\n- [Technologies Used](#technologies-used)\n\n\n## Introduction\nThe Multimodal Emotion Recognition System aims to enhance the hiring process by analyzing candidates' emotions during behavioral interviews. It utilizes both textual data and facial images captured during interviews to analyze underlying sentiments. The system incorporates advanced deep learning techniques such as Bi-directional Long Short-Term Memory (Bi-LSTM) for textual content and Convolutional Neural Networks (CNNs) for facial expressions.\n\n## Features\n- Analyzes facial expressions and textual data from speech cues for emotion recognition.\n- Utilizes Bi-LSTM and CNNs for accurate emotion analysis.\n- Provides real-time analysis to interviewers through a user-friendly web interface.\n\n\n## Usage\n- Access the web interface by opening the provided URL. (multimodal-emotion-detection.azurewebsites.net)\n- Capture candidate facial images and speech cues.\n- The system will analyze emotions and provide real-time insights.\n\n\n\n## Technologies Used\n- Streamlit\n- Docker\n- CI/CD Pipeline\n- Microsoft Azure\n\n\n  ## Application can be accessed via\n- https://multimodal-emotion-detection.azurewebsites.net/\n"
    },
    "ollama": {
        "description": "Get up and running with Llama 2, Mistral, Gemma, and other large language models.",
        "readme": "<div align=\"center\">\n  <img alt=\"ollama\" height=\"200px\" src=\"https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7\">\n</div>\n\n# Ollama\n\n[![Discord](https://dcbadge.vercel.app/api/server/ollama?style=flat&compact=true)](https://discord.gg/ollama)\n\nGet up and running with large language models locally.\n\n### macOS\n\n[Download](https://ollama.com/download/Ollama-darwin.zip)\n\n### Windows preview\n\n[Download](https://ollama.com/download/OllamaSetup.exe)\n\n### Linux\n\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n[Manual install instructions](https://github.com/jmorganca/ollama/blob/main/docs/linux.md)\n\n### Docker\n\nThe official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.\n\n### Libraries\n\n- [ollama-python](https://github.com/ollama/ollama-python)\n- [ollama-js](https://github.com/ollama/ollama-js)\n\n## Quickstart\n\nTo run and chat with [Llama 2](https://ollama.com/library/llama2):\n\n```\nollama run llama2\n```\n\n## Model library\n\nOllama supports a list of models available on [ollama.com/library](https://ollama.com/library 'ollama model library')\n\nHere are some example models that can be downloaded:\n\n| Model              | Parameters | Size  | Download                       |\n| ------------------ | ---------- | ----- | ------------------------------ |\n| Llama 2            | 7B         | 3.8GB | `ollama run llama2`            |\n| Mistral            | 7B         | 4.1GB | `ollama run mistral`           |\n| Dolphin Phi        | 2.7B       | 1.6GB | `ollama run dolphin-phi`       |\n| Phi-2              | 2.7B       | 1.7GB | `ollama run phi`               |\n| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`       |\n| Starling           | 7B         | 4.1GB | `ollama run starling-lm`       |\n| Code Llama         | 7B         | 3.8GB | `ollama run codellama`         |\n| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored` |\n| Llama 2 13B        | 13B        | 7.3GB | `ollama run llama2:13b`        |\n| Llama 2 70B        | 70B        | 39GB  | `ollama run llama2:70b`        |\n| Orca Mini          | 3B         | 1.9GB | `ollama run orca-mini`         |\n| Vicuna             | 7B         | 3.8GB | `ollama run vicuna`            |\n| LLaVA              | 7B         | 4.5GB | `ollama run llava`             |\n| Gemma              | 2B         | 1.4GB | `ollama run gemma:2b`          |\n| Gemma              | 7B         | 4.8GB | `ollama run gemma:7b`          |\n\n> Note: You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n\n## Customize a model\n\n### Import from GGUF\n\nOllama supports importing GGUF models in the Modelfile:\n\n1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import.\n\n   ```\n   FROM ./vicuna-33b.Q4_0.gguf\n   ```\n\n2. Create the model in Ollama\n\n   ```\n   ollama create example -f Modelfile\n   ```\n\n3. Run the model\n\n   ```\n   ollama run example\n   ```\n\n### Import from PyTorch or Safetensors\n\nSee the [guide](docs/import.md) on importing models for more information.\n\n### Customize a prompt\n\nModels from the Ollama library can be customized with a prompt. For example, to customize the `llama2` model:\n\n```\nollama pull llama2\n```\n\nCreate a `Modelfile`:\n\n```\nFROM llama2\n\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n\n# set the system message\nSYSTEM \"\"\"\nYou are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n\"\"\"\n```\n\nNext, create and run the model:\n\n```\nollama create mario -f ./Modelfile\nollama run mario\n>>> hi\nHello! It's your friend Mario.\n```\n\nFor more examples, see the [examples](examples) directory. For more information on working with a Modelfile, see the [Modelfile](docs/modelfile.md) documentation.\n\n## CLI Reference\n\n### Create a model\n\n`ollama create` is used to create a model from a Modelfile.\n\n```\nollama create mymodel -f ./Modelfile\n```\n\n### Pull a model\n\n```\nollama pull llama2\n```\n\n> This command can also be used to update a local model. Only the diff will be pulled.\n\n### Remove a model\n\n```\nollama rm llama2\n```\n\n### Copy a model\n\n```\nollama cp llama2 my-llama2\n```\n\n### Multiline input\n\nFor multiline input, you can wrap text with `\"\"\"`:\n\n```\n>>> \"\"\"Hello,\n... world!\n... \"\"\"\nI'm a basic program that prints the famous \"Hello, world!\" message to the console.\n```\n\n### Multimodal models\n\n```\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\n```\n\n### Pass in prompt as arguments\n\n```\n$ ollama run llama2 \"Summarize this file: $(cat README.md)\"\n Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.\n```\n\n### List models on your computer\n\n```\nollama list\n```\n\n### Start Ollama\n\n`ollama serve` is used when you want to start ollama without running the desktop application.\n\n## Building\n\nInstall `cmake` and `go`:\n\n```\nbrew install cmake go\n```\n\nThen generate dependencies:\n\n```\ngo generate ./...\n```\n\nThen build the binary:\n\n```\ngo build .\n```\n\nMore detailed instructions can be found in the [developer guide](https://github.com/jmorganca/ollama/blob/main/docs/development.md)\n\n### Running local builds\n\nNext, start the server:\n\n```\n./ollama serve\n```\n\nFinally, in a separate shell, run a model:\n\n```\n./ollama run llama2\n```\n\n## REST API\n\nOllama has a REST API for running and managing models.\n\n### Generate a response\n\n```\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"Why is the sky blue?\"\n}'\n```\n\n### Chat with a model\n\n```\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"mistral\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\n```\n\nSee the [API documentation](./docs/api.md) for all endpoints.\n\n## Community Integrations\n\n### Web & Desktop\n\n- [Bionic GPT](https://github.com/bionic-gpt/bionic-gpt)\n- [Enchanted (macOS native)](https://github.com/AugustDev/enchanted)\n- [HTML UI](https://github.com/rtcfirefly/ollama-ui)\n- [Chatbot UI](https://github.com/ivanfioravanti/chatbot-ollama)\n- [Typescript UI](https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file)\n- [Minimalistic React UI for Ollama Models](https://github.com/richawo/minimal-llm-ui)\n- [Open WebUI](https://github.com/open-webui/open-webui)\n- [Ollamac](https://github.com/kevinhermawan/Ollamac)\n- [big-AGI](https://github.com/enricoros/big-AGI/blob/main/docs/config-local-ollama.md)\n- [Cheshire Cat assistant framework](https://github.com/cheshire-cat-ai/core)\n- [Amica](https://github.com/semperai/amica)\n- [chatd](https://github.com/BruceMacD/chatd)\n- [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)\n- [MindMac](https://mindmac.app)\n- [NextJS Web Interface for Ollama](https://github.com/jakobhoeg/nextjs-ollama-llm-ui)\n- [Msty](https://msty.app)\n- [Chatbox](https://github.com/Bin-Huang/Chatbox)\n- [NextChat](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web) with [Get Started Doc](https://docs.nextchat.dev/models/ollama)\n\n### Terminal\n\n- [oterm](https://github.com/ggozad/oterm)\n- [Ellama Emacs client](https://github.com/s-kostyaev/ellama)\n- [Emacs client](https://github.com/zweifisch/ollama)\n- [gen.nvim](https://github.com/David-Kunz/gen.nvim)\n- [ollama.nvim](https://github.com/nomnivore/ollama.nvim)\n- [ollama-chat.nvim](https://github.com/gerazov/ollama-chat.nvim)\n- [ogpt.nvim](https://github.com/huynle/ogpt.nvim)\n- [gptel Emacs client](https://github.com/karthink/gptel)\n- [Oatmeal](https://github.com/dustinblackman/oatmeal)\n- [cmdh](https://github.com/pgibler/cmdh)\n- [tenere](https://github.com/pythops/tenere)\n- [llm-ollama](https://github.com/taketwo/llm-ollama) for [Datasette's LLM CLI](https://llm.datasette.io/en/stable/).\n- [ShellOracle](https://github.com/djcopley/ShellOracle)\n\n### Database\n\n- [MindsDB](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/ollama_handler/README.md)\n\n### Package managers\n\n- [Pacman](https://archlinux.org/packages/extra/x86_64/ollama/)\n- [Helm Chart](https://artifacthub.io/packages/helm/ollama-helm/ollama)\n\n### Libraries\n\n- [LangChain](https://python.langchain.com/docs/integrations/llms/ollama) and [LangChain.js](https://js.langchain.com/docs/modules/model_io/models/llms/integrations/ollama) with [example](https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa)\n- [LangChainGo](https://github.com/tmc/langchaingo/) with [example](https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example)\n- [LangChain4j](https://github.com/langchain4j/langchain4j) with [example](https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java)\n- [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/examples/llm/ollama.html)\n- [LangChain4j](https://github.com/langchain4j/langchain4j/tree/main/langchain4j-ollama)\n- [LiteLLM](https://github.com/BerriAI/litellm)\n- [OllamaSharp for .NET](https://github.com/awaescher/OllamaSharp)\n- [Ollama for Ruby](https://github.com/gbaptista/ollama-ai)\n- [Ollama-rs for Rust](https://github.com/pepperoni21/ollama-rs)\n- [Ollama4j for Java](https://github.com/amithkoujalgi/ollama4j)\n- [ModelFusion Typescript Library](https://modelfusion.dev/integration/model-provider/ollama)\n- [OllamaKit for Swift](https://github.com/kevinhermawan/OllamaKit)\n- [Ollama for Dart](https://github.com/breitburg/dart-ollama)\n- [Ollama for Laravel](https://github.com/cloudstudio/ollama-laravel)\n- [LangChainDart](https://github.com/davidmigloz/langchain_dart)\n- [Semantic Kernel - Python](https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama)\n- [Haystack](https://github.com/deepset-ai/haystack-integrations/blob/main/integrations/ollama.md)\n- [Elixir LangChain](https://github.com/brainlid/langchain)\n- [Ollama for R - rollama](https://github.com/JBGruber/rollama)\n- [Ollama-ex for Elixir](https://github.com/lebrunel/ollama-ex)\n- [Ollama Connector for SAP ABAP](https://github.com/b-tocs/abap_btocs_ollama)\n\n### Mobile\n\n- [Enchanted](https://github.com/AugustDev/enchanted)\n- [Maid](https://github.com/Mobile-Artificial-Intelligence/maid)\n\n### Extensions & Plugins\n\n- [Raycast extension](https://github.com/MassimilianoPasquini97/raycast_ollama)\n- [Discollama](https://github.com/mxyng/discollama) (Discord bot inside the Ollama discord channel)\n- [Continue](https://github.com/continuedev/continue)\n- [Obsidian Ollama plugin](https://github.com/hinterdupfinger/obsidian-ollama)\n- [Logseq Ollama plugin](https://github.com/omagdy7/ollama-logseq)\n- [Dagger Chatbot](https://github.com/samalba/dagger-chatbot)\n- [Discord AI Bot](https://github.com/mekb-turtle/discord-ai-bot)\n- [Ollama Telegram Bot](https://github.com/ruecat/ollama-telegram)\n- [Hass Ollama Conversation](https://github.com/ej52/hass-ollama-conversation)\n- [Rivet plugin](https://github.com/abrenneke/rivet-plugin-ollama)\n- [Llama Coder](https://github.com/ex3ndr/llama-coder) (Copilot alternative using Ollama)\n- [Obsidian BMO Chatbot plugin](https://github.com/longy2k/obsidian-bmo-chatbot)\n- [Copilot for Obsidian plugin](https://github.com/logancyang/obsidian-copilot)\n- [Obsidian Local GPT plugin](https://github.com/pfrankov/obsidian-local-gpt)\n- [Open Interpreter](https://docs.openinterpreter.com/language-model-setup/local-models/ollama)\n- [twinny](https://github.com/rjmacarthy/twinny) (Copilot and Copilot chat alternative using Ollama)\n- [Wingman-AI](https://github.com/RussellCanfield/wingman-ai) (Copilot code and chat alternative using Ollama and HuggingFace)\n- [Page Assist](https://github.com/n4ze3m/page-assist) (Chrome Extension)\n"
    },
    "Parthiban-3997": {
        "description": "Config files for my GitHub profile.",
        "readme": "\ud83d\udc4b Hi there, I\u2019m Parthiban Ravichandran, a passionate Data Scientist with a strong academic background and a thirst for solving real-world problems using data-driven solutions.\n\n\ud83c\udf93 I recently completed my Master's degree in Data Science from the University of Central Lancashire, where I honed my skills in statistics, machine learning, deep learning, and computer vision.\n\n\ud83d\udcac My motive is to give my full potential to build AI-based solutions that solve high-impact problems for people around the globe and simplify everyday living.\n\n\ud83d\udc9e\ufe0f I\u2019m looking to collaborate on Data Science Projects\n\n\ud83d\udceb You can follow me on LinkedIn: https://www.linkedin.com/in/parthiban-ravichandran/\n\n\ud83d\udd2c My technical skills include:\n- Languages: Python\n- Libraries: Numpy, Pandas, Matplotlib, Seaborn, Scikit-learn, Tensorflow, Keras, NLTK, Mediapipe\n- Frameworks: Flask, Streamlit\n- Databases: MySQL, MongoDB\n- Cloud Platforms: Heroku, Microsoft Azure\n- DevOps: Docker, CI/CD Pipeline, GitHub Actions\n- Tech Stack: Machine and Deep Learning Algorithms, RNN, OpenCV, Transfer Learning Techniques, OpenAI, LangChain, Statistics\n\n\ud83c\udf31 I'm actively seeking full-time roles related to Data Science, where I can apply my knowledge and expertise to tackle complex challenges.\n\n\ud83e\udd1d I'm eager to collaborate on Data Science projects and contribute to innovative solutions. If you're looking for a Data Scientist who is well-versed in the latest technologies and dedicated to making an impact, let's connect!\n\n\ud83d\udcac For collaborating don't hesitate to reach me on rparthiban729@gmail.com\n\n<!---\nParthiban-3997/Parthiban-3997 is a \u2728 special \u2728 repository because its `README.md` (this file) appears on your GitHub profile.\nYou can click the Preview link to take a look at your changes.\n--->\n"
    },
    "Potato-Disease-Prediction": {
        "description": null,
        "readme": "# Potato-Disease-Prediction\n\nThe code is geneated using Deep learning techniques to predict the common diseases in potato leaf like \n1.Early Blight \n2. Health \n3.Late Blight\n"
    },
    "ProjectorApp": {
        "description": "Projector- A Full Stack website done with HTML, CSS, Node.js, MongoDB",
        "readme": "# ProjectorApp\nProjector- A Full Stack website done with HTML, CSS, Node.js, MongoDB\n"
    },
    "Real_Time_Based_News_Letter_Generation_Using_CrewAI": {
        "description": null,
        "readme": "## AI-Powered Real-Time Newsletter Generation\n\nWelcome to the NewsletterGen Crew project, powered by [crewAI](https://crewai.com). This template is designed to help you set up a multi-agent AI system with ease, leveraging the powerful and flexible framework provided by crewAI. The goal is to enable agents to collaborate effectively on complex tasks, maximizing their collective intelligence and capabilities.\n\nThis project is a Streamlit application that leverages AI agents to generate customized newsletters based on real-time news data. It uses the CrewAI framework to coordinate a team of AI agents that research, edit, and design newsletters on specified topics.\n\n## Deployed Link\n\nNewsletter Generation App is Deployed And Available [Here](https://realtimebasednewslettergeneration.streamlit.app/)\n\n\n## Screenshots\n\n![crew_1](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/187984bf-a3fc-49cd-9b20-841629baaa12)\n![crew_2](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/5419b9d9-2558-49e0-a4c4-e95eaab78b0f)\n![crew_3](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/05aabb8b-2f64-4caa-a174-0a8dd7f1ab69)\n![crew_4](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/b550c8fe-518a-48ef-9d3b-0cbd998df00a)\n![crew_5](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/fd342956-f0cf-490c-8090-0727f877c77a)\n![crew_6](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/edcde6df-03fe-4928-8ebb-2d7a806aa592)\n![crew_7](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/ea7e67c4-d69c-4075-b2c9-49e1f2d7b69b)\n\n\n## Key Features\n- **Real-Time News Integration**: Utilizes the Exa API to fetch the latest news articles on specified topics based on semantic meaning and not by keyword search.\n- **AI-Driven Content Creation**: Employs a team of AI agents to research, curate, and edit content for the newsletter.\n- **Customizable Topics**: Users can input any topic of interest to generate a relevant newsletter.\n- **Date Range Selection**: Allows users to specify a start date for news searches, ensuring up-to-date content.\n- **Personalized Messaging**: Includes an option to add a custom personal message at the top of the newsletter.\n- **HTML Newsletter Generation**: Produces a fully formatted HTML newsletter ready for distribution.\n- **One-Click Download**: Enables users to download the generated newsletter as an HTML file.\n\n\n## Project Uniqueness\nThis project stands out due to its innovative approach to newsletter creation:\n\n- **AI Agent Collaboration**: Utilizes the CrewAI framework to orchestrate a team of AI agents, each with specialized roles in the newsletter creation process.\n- **Real-Time Data Integration**: Incorporates the latest news by leveraging the Exa API, ensuring newsletters are always current and relevant.\n- **Flexible Content Generation**: Adapts to any topic specified by the user, making it versatile for various industries and interests.\n- **Streamlined User Experience**: Offers a simple, intuitive interface for users to generate complex, AI-crafted newsletters with minimal input.\n- **Scalable Architecture**: Designed to handle multiple simultaneous requests and can be easily extended to include additional features or data sources.\n\n\n## How It Works\nUser Input: Users specify a topic, personal message, and start date for news search.\n\n- **Research Agent**: Searches for and analyzes recent news articles on the specified topic.\n- **Editor Agent**: Curates and refines the content, ensuring relevance and coherence.\n- **Designer Agent**: Formats the content into an attractive HTML newsletter template.\n- **Output**: Users can preview and download the generated newsletter as an HTML file.\n\n\n## Impact\nThis tool revolutionizes the newsletter creation process by:\n\nReducing the time and effort required to create high-quality, topical newsletters.\nEnsuring content is always fresh and relevant, improving reader engagement.\nAllowing for rapid creation of newsletters on any topic, enhancing responsiveness to current events.\nProviding a scalable solution for businesses and individuals who need to produce regular, customized newsletters.\nBy automating the research, writing, and design processes, this application empowers users to create professional-grade newsletters without the need for a dedicated content team or extensive time investment.\n\n\nConsider the following diagram to understand how multi agents are built:\n\n![Agent Architecture](https://github.com/Parthiban-3997/Chat_With_Multiple_SQL_Databases/assets/26496805/21e0998b-4d39-4ac2-b47b-cc332bdbca2d)\n\n\n## Installation\n\nEnsure you have Python >=3.10 <=3.13 installed on your system. This project uses [Poetry](https://python-poetry.org/) for dependency management and package handling, offering a seamless setup and execution experience.\n\nFirst, if you haven't already, install Poetry:\n\n```bash\npip install poetry\n```\n\nNext, navigate to your project directory and install the dependencies:\n\n1. First lock the dependencies and then install them:\n```bash\npoetry lock\n```\n```bash\npoetry install\n```\n### Customizing\n\n**Add your  `ANTHROPIC_API_KEY` into the `.env` file**\n\n- Modify `src/newsletter_gen/config/agents.yaml` to define your agents\n- Modify `src/newsletter_gen/config/tasks.yaml` to define your tasks\n- Modify `src/newsletter_gen/crew.py` to add your own logic, tools and specific args\n- Modify `src/newsletter_gen/main.py` to add custom inputs for your agents and tasks\n\n## Running the Project\n\nTo kickstart your crew of AI agents and begin task execution, run this from the root folder of your project:\n\n```bash\npoetry run newsletter_gen\n```\n\nThis command initializes the newsletter-gen Crew, assembling the agents and assigning them tasks as defined in your configuration.\n\n\n## Understanding Your Crew\n\nThe newsletter-gen Crew is composed of multiple AI agents, each with unique roles, goals, and tools. These agents collaborate on a series of tasks, defined in `config/tasks.yaml`, leveraging their collective skills to achieve complex objectives. The `config/agents.yaml` file outlines the capabilities and configurations of each agent in your crew.\n\n\n## Contributing\n\nContributions to this project are welcome! If you have any ideas, bug fixes, or improvements, feel free to submit a pull request. Please ensure that your code adheres to the project's coding standards and is well-documented.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n"
    },
    "SensibleAutonomousMachine": {
        "description": "Using Deep Learning and Convolutional Neural Networks",
        "readme": "# Sensible Autonomous Machine #\n\n## INTRODUCTION ##\nIn recent times, automation has achieved improvements by quality, accuracy and precision. At the same time, the technology itself continues to evolve, bringing new waves of advances in robotics, analytics, and artificial intelligence (AI), and especially machine learning. Together they amount to a step change in technical capabilities that could have profound implications for business, for the economy, and more broadly, for society.  \n\nThe modules used in this program include\n\n### 1.1\t Robotics ### \nRobotics is a branch of engineering which incorporates multiple disciplines to design, build, program and use robotic machines.  Robots are used in industries for speeding up the manufacturing process. AI is a highly useful tool in robotic assembly applications. When combined with advanced vision systems, AI can help with real-time course correction, which is particularly useful in complex manufacturing sectors. A handful of robotic systems are now being sold as open source systems with AI capability. Users train robots to do custom tasks based on their specific application, such as small-scale agriculture. The convergence of open source robotics and AI could be a huge trend in the future of AI robots.\n\n### 1.2 Automation in Machines ###\n\nAutomation is any individual involved in the creation and application of technology to monitor and control the production and delivery of products and services. Automakers are moving at a frenzied pace to add more and more intelligence to vehicles developed a scale to describe the six different levels of automation for self-driving cars. Level 0 is the driver actually steps on the gas to go faster, steps on the brake to slow down and uses the steering wheel to turn. Level 1 is the driver is still in control of the overall operation and safety of the vehicle. Level 2 is the driver is still responsible for the safe operation of the vehicle. Level 3 states that the car can drive itself, but the human driver must still pay attention and be prepared to take over at any time. Level 4 explains that the car can be driven by a person, but it doesn\u2019t always need to be. It can drive itself full-time under the right circumstances. Level 5 proposes that the car controls itself under all circumstances with no expectation of human intervention. \n\n### 1.3 Deep Learning ###\n\nDeep Learning is a new area of Machine Learning research, which has been introduced with the objective of moving Machine Learning closer to one of its original goals, Artificial Intelligence. Deep Learning provides computers with the ability to learn without being explicitly programmed. Deep learning focuses on the development of computer programs that can change when exposed to new data.  The process of Deep Learning is similar to that of data mining. Both systems search through data to look for patterns.\n\n### 1.4\t RCNN ###\n \nRegion-CNN (R-CNN) is one of the state-of-the-art CNN-based deep learning object detection approaches. Based on this, there are fast R-CNN and faster RCNN for faster speed object detection as well as mask R-CNN for object instance segmentation. On the other hand, there are also other object detection approaches, such as YOLO and SSD. \n\n### 1.5 Python-3.5.2 ###\n\nPython\u2019s standard library is very extensive, offering a wide range of facilities as indicated by the long table of contents listed below. The library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\n### 1.6\t TensorFlow 1.12.0 ###\n\nTensorFlow\u2122 is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\n\n### 1.7\t Pip ###\n\npip is a package management system used to install and manage software packages written in Python. Many packages can be found in the default source for packages and their dependencies \u2014Python Package Index (PyPI). \nPython 2.7.9 and later (on the python2 series), and Python 3.4 and later include pip (pip3 for Python 3) by default. pip is a recursive acronym that can stand for either \"Pip Installs Packages\" or \"Pip Installs Python\". Alternatively, pip stands for \"preferred installer program\".\n\n### 1.8\t NumPy ###\n\nNumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. The ancestor of NumPy, Numeric, was originally created by Jim Hugunin with contributions from several other developers. In 2005, Travis Oliphant created NumPy by incorporating features of the competing num array into Numeric, with extensive modifications. NumPy is open-source software and has many contributors.\n\n## OVERVIEW ##\n\nThe main idea of the project is to initiate a self-driving machine, observing the surroundings across a transport region and act accordingly by the provider\u2019s instructions. This initiative brings many real-world things to autonomous creature and the main purpose is to save time from user\u2019s point of view. \n\n### 2.1 Primary goal ###\n\nInterfacing SAM to follow back an object by crossing obstacles and to reach destination by sensing the response signal from Smartphone. Monitoring entire surrounding to identify the color patterns of traffic signal and performing right/left turn by getting trained in predicting static/dynamic models. Maintain ratio of frames captured and running status of machine.\n \n### 2.2 Secondary Goal ###\n\n![2](https://user-images.githubusercontent.com/24918359/56951544-bedc1f80-6b55-11e9-9130-c8da5accd3aa.JPG) \n\n## ARCHITECTURE DIAGRAM ##\n\n![21](https://user-images.githubusercontent.com/24918359/56954402-52185380-6b5c-11e9-8096-6df731268d7d.jpg)\n\n## MODULE DESCRIPTION ##\n\n1) Processing Real Time Data\n2)\tCalibration of Machine by embedding sensors\n3)\tManipulation of the Machine\n4) Integration of the Machine with Mobile Application\n\n### 3.1 Processing Real Time Data ###\n\n*\tPi camera is integrated with Raspberry Pi.\n*\tImage processing is done in real time scenario by recognizing object patterns and detecting traffic light using R-CNN.\n*\tDeep Learning implements feed-forward artificial neural networks or, more particularly, multi-layer perceptron (MLP), the most commonly used type of neural networks. MLP consists of the input layer, output layer, and one or more hidden layers. \n*\tEach layer of MLP includes one or more neurons directionally linked with the neurons from the previous and the next layer.\n\n![4](https://user-images.githubusercontent.com/24918359/56951984-cd770680-6b56-11e9-9087-ae2db5096b56.jpg)\n\n### 3.2 Calibration of the Machine by embedding sensors###\n\n### 3.2.1 Detection of Obstacles\t###\n\n* Absorption and Reflection of black and white signals to make the machine sense the road patterns for moving forward and turn operations (left and right).\n\n![5](https://user-images.githubusercontent.com/24918359/56951990-d1a32400-6b56-11e9-8447-28f125895c5d.jpg)\n\n### 3.2.2\tMeasurement of distance between objects ###\n\n* Measuring the distance between the machine and other objects on-road by sensing frequency through Ultrasonic Sensor.\n\n![6](https://user-images.githubusercontent.com/24918359/56952001-d5cf4180-6b56-11e9-87b6-1876523f667b.jpg)\n\n### 3.3\tManipulation of the Machine ###\n\n* L293D Motor IC is interfaced with Raspberry PI to perform start and stop operations.\n\n![7](https://user-images.githubusercontent.com/24918359/56952009-d8ca3200-6b56-11e9-9e6c-6797e0520cb4.jpg)\n\n### 3.4\tIntegration of the Machine with Mobile Application ###\n\n* User provides the Geo Location via Android Application to the Machine where Latitude and Longitude data is fed to the machine to reach destination.\n\n![8](https://user-images.githubusercontent.com/24918359/56952013-db2c8c00-6b56-11e9-9878-e0b2318743a2.jpg)\n\n## PROCESSING AND TRAINING ##\n\n### Installing TensorFlow ###\n\n### 1. Update the Raspberry Pi ###\n\nFirst, the Raspberry Pi needs to be fully updated. Open a terminal and issue:\n\n    sudo apt-get update\n    sudo apt-get dist-upgrade\n\n![1](https://user-images.githubusercontent.com/24918359/56953172-8dfde980-6b59-11e9-90bd-ad1ec2777631.png)\n\n### 2. Install TensorFlow ###\n\nNext, we\u2019ll install TensorFlow. In the /home/pi directory, create a folder called \u2018tf\u2019, which will be used to hold all the installation files for TensorFlow and Protobuf, and cd into it:\n\nmkdir tf\ncd tf\nwget https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.8.0/tensorflow-1.12.0-cp35-none-linux_armv7l.whl\n\n    sudo pip3 install /home/pi/tf/tensorflow-1.12.0-cp35-none-linux_armv7l.whl\n\nTensorFlow also needs the LibAtlas package. Install it by issuing \n\n    sudo apt-get install libatlas-base-dev\n\nWhile we\u2019re at it, let\u2019s install other dependencies that will be used by the TensorFlow Object Detection API. These are listed on the installation instructions in TensorFlow\u2019s Object Detection GitHub repository. Issue:\n\n    sudo pip3 install pillow lxml jupyter matplotlib cython\n    sudo apt-get install python-tk\n\n### 3. Install OpenCV ###\n\nTensorFlow\u2019s object detection examples typically use matplotlib to display images, but I prefer to use OpenCV because it\u2019s easier to work with and less error prone. The object detection scripts in this guide\u2019s GitHub repository use OpenCV. So, we need to install OpenCV.\n\nTo get OpenCV working on the Raspberry Pi, there\u2019s quite a few dependencies that need to be installed through apt-get. If any of the following commands don\u2019t work, issue \u201csudo apt-get update\u201d and then try again. Issue:\n\n    sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev\n    sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev\n    sudo apt-get install libxvidcore-dev libx264-dev\n    sudo apt-get install qt4-dev-tools\n\nNow that we\u2019ve got all those installed, we can install OpenCV. Issue:\n\n    pip3 install opencv-python\n\n### 4. Compile and Install Protobuf ###\n\nThe TensorFlow object detection API uses Protobuf, a package that implements Google\u2019s Protocol Buffer data format. Unfortunately, there\u2019s currently no easy way to install Protobuf on the Raspberry Pi. We have to compile it from source ourselves and then install it.\n\nFirst, get the packages needed to compile Protobuf from source. Issue:\n\n    sudo apt-get install autoconf automake libtool curl\n\nThen download the protobuf release from its GitHub repository by issuing:\n\nwget https://github.com/google/protobuf/releases/download/v3.5.1/protobuf-all-3.5.1.tar.gz\n\nIf a more recent version of protobuf is available, download that instead. Unpack the file and cd into the folder:\n\n    tar -zxvf protobuf-all-3.5.1.tar.gz\n    cd protobuf-3.5.1\n\nConfigure the build by issuing the following command (it takes about 2 minutes):\n\n    ./configure\n\nBuild the package by issuing:\n\n    make\n\nWhen it\u2019s finished, issue:\n\n    make check \n\nThis process takes even longer, clocking in at 107 minutes on Pi. According to other guides I\u2019ve seen, this command may exit out with errors, but Protobuf will still work. Now that it\u2019s built, install it by issuing:\n\n    sudo make install\n\nThen move into the python directory and export the library path: \n    \n    cd python\n    export LD_LIBRARY_PATH=../src/.libs\n\nNext, issue:\n\n    python3 setup.py build --cpp_implementation \n    python3 setup.py test --cpp_implementation\n    sudo python3 setup.py install --cpp_implementation\n\nThen issue the following path commands:\n\n    export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp\n    export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION=3\n\nFinally, issue:\n\n    sudo ldconfig\n\nNow Protobuf is installed on the Pi. Verify it\u2019s installed correctly by issuing the command below and making sure it puts out the default help text.\n\nProtoc\n\nFor some reason, the Raspberry Pi needs to be restarted after this process, or TensorFlow will not work. Go ahead and reboot the Pi by issuing:\n\n    sudo reboot now\n\n### 5. Set up TensorFlow Directory Structure and PYTHONPATH Variable ###\n\nNow that we\u2019ve installed all the packages, we need to set up the TensorFlow directory. Move back to your home directory, then make a directory called \u201ctensorflow1\u201d, and cd into it.\n\n    mkdir tensorflow1\n    cd tensorflow1\n\nDownload the tensorflow repository from GitHub by issuing:\n\n    git clone --recurse-submodules https://github.com/tensorflow/models.git\n\nNext, we need to modify the PYTHONPATH environment variable to point at some directories inside the TensorFlow repository we just downloaded. We want PYTHONPATH to be set every time we open a terminal, so we have to modify the .bashrc file. Open it by issuing:\n\n    sudo nano ~/.bashrc\n\n    export PYTHONPATH=$PYTHONPATH:/home/pi/tensorflow1/models/research:/home/pi/tensorflow1/models/research/slim\n\n![2](https://user-images.githubusercontent.com/24918359/56953173-8dfde980-6b59-11e9-99d4-6fcecf96bf92.png)\n\nThen, save and exit the file. This makes it so the \u201cexport PYTHONPATH\u201d command is called every time you open a new terminal, so the PYTHONPATH variable will always be set appropriately. Close and then re-open the terminal.\n\nNow, we need to use Protoc to compile the Protocol Buffer (.proto) files used by the Object Detection API. The .proto files are located in /research/object_detection/protos, but we need to execute the command from the /research directory. Issue:\n\n    cd /home/pi/tensorflow1/models/research\n    protoc object_detection/protos/*.proto --python_out=.\n\nThis command converts all the \"name\".proto files to \"name_pb2\".py files. Next, move into the object_detection directory:\n\n    cd /home/pi/tensorflow1/models/research/object_detection\n\nNow, we\u2019ll download the SSD_Mobilenet model from the TensorFlow detection model zoo. The model zoo is Google\u2019s collection of pre-trained object detection models that have various levels of speed and accuracy. The Raspberry Pi has a weak processor, so we need to use a model that takes less processing power. Though the model will run faster, it comes at a tradeoff of having lower accuracy. For this tutorial, we\u2019ll use SSD-MobileNet, which is the fastest model available.\n\nGoogle is continuously releasing models with improved speed and performance, so check back at the model zoo often to see if there are any better models.\n\nDownload the SSD-MobileNet model and unpack it by issuing:\n\n    wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_05_09.tar.gz\n    tar -xzvf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n\nNow the model is in the object_detection directory and ready to be used.\n\n### 6. Detect Objects ###\n\nOkay, now everything is set up for performing object detection on the Pi! The Python script in this repository, Object_detection_picamera.py, detects objects in live feeds from a Picamera or USB webcam. Basically, the script sets paths to the model and label map, loads the model into memory, initializes the Picamera, and then begins performing object detection on each video frame from the Picamera.\n\nIf you\u2019re using a Picamera, make sure it is enabled in the Raspberry Pi configuration menu.\n\n![3](https://user-images.githubusercontent.com/24918359/56953174-8dfde980-6b59-11e9-9563-c660969fbe23.png)\n\nDownload the Object_detection_picamera.py file into the object_detection directory by issuing:\n\nwget https://raw.githubusercontent.com/Parthi_Koushik/TensorFlow-Object-Detection-on-the-Raspberry-Pi/master/Object_detection_picamera.py\n\nRun the script by issuing:\n\n    python3 Object_detection_picamera.py \n\nThe script defaults to using an attached Picamera. If you have a USB webcam instead, add --usbcam to the end of the command:\npython3 Object_detection_picamera.py \u2013usbcam\n\nOnce the script initializes (which can take up to 30 seconds), you will see a window showing a live view from your camera. Common objects inside the view will be identified and have a rectangle drawn around them.\n\n![4](https://user-images.githubusercontent.com/24918359/56953175-8e968000-6b59-11e9-812f-8067e6aac3c0.png)\n\nYou can also use a model by adding the frozen inference graph into the object_detection directory and changing the model path in the script. You can test this out using my playing card detector model (transferred from ssd_mobilenet_v2 model and trained on TensorFlow v1.12). Once downloaded and extracted the model, or if you have your own model, place the model folder into the object_detection directory. Place the label_map.pbtxt file into the object_detection/data directory.\n\n![5](https://user-images.githubusercontent.com/24918359/56953176-8e968000-6b59-11e9-9fce-152c5938dc39.png)\n\nThen, open the Object_detection_picamera.py script in a text editor. Go to the line where MODEL_NAME is set and change the string to match the name of the new model folder. Then, on the line where PATH_TO_LABELS is set, change the name of the labelmap file to match the new label map. Change the NUM_CLASSES variable to the number of classes your model can identify.   \n\n![6](https://user-images.githubusercontent.com/24918359/56953177-8e968000-6b59-11e9-89c9-b7573f63d4ff.png)\n\nNow, when you run the script, it will use your model rather than the SSDLite_MobileNet model. \n\n### Training Datasets ###\n\n### 1. Set up TensorFlow Directory and Anaconda Virtual Environment ###\n\nThe TensorFlow Object Detection API requires using the specific directory structure provided in its GitHub repository. It also requires several additional Python packages, specific additions to the PATH and PYTHONPATH variables, and a few extra setup commands to get everything set up to run or train an object detection model.\n\n### 1a. Download TensorFlow Object Detection API repository from GitHub ###\n\nCreate a folder directly in C: and name it \u201ctensorflow1\u201d. This working directory will contain the full TensorFlow object detection framework, as well as your training images, training data, trained classifier, configuration files, and everything else needed for the object detection classifier.\n\n### 1b. Download the SSD_MobileNet-V2-COCO model from TensorFlow's model zoo ###\n\nTensorFlow provides several object detection models (pre-trained classifiers with specific neural network architectures) in its model zoo. Some models (such as the SSD-MobileNet model) have an architecture that allows for faster detection but with less accuracy, while some models (such as the Faster-RCNN model) give slower detection but with more accuracy. Use the SSD-MobileNet model for training.\n\n### 1c. Download repository ###\n\nDownload the full repository located on this page (scroll to the top and click Clone or Download) and extract all the contents directly into the C:\\tensorflow1\\models\\research\\object_detection directory. (You can overwrite the existing \"README.md\" file.) This establishes a specific directory structure that will be used.\n\n![7](https://user-images.githubusercontent.com/24918359/56953179-8e968000-6b59-11e9-9b27-2a64e62dcaf5.jpg)\n\nThis contains the images, annotation data, .csv files, and TFRecords needed to train a \"Pinochle Deck\" playing card detector. You can use these images and data to practice making your own Pinochle Card Detector. It also contains Python scripts that are used to generate the training data. It has scripts to test out the object detection classifier on images, videos, or a webcam feed. You can ignore the \\doc folder and its files; they are just there to hold the images used for this readme.\n\nIf you want to practice training your own \"Pinochle Deck\" card detector, you can leave all the files as they are. You can follow along with this tutorial to see how each of the files were generated, and then run the training. You will still need to generate the TFRecord files (train.record and test.record) as described in Step 4.\n\nIf you want to train your own object detector, delete the following files (do not delete the folders):\n\n\u2022\tAll files in \\object_detection\\training\n\u2022\tAll files in \\object_detection\\inference_graph\n\nNow, you are ready to start from scratch in training your own object detector. This tutorial will assume that all the files listed above were deleted, and will go on to explain how to generate the files for your own training dataset.\n\n### 1d. Set up new Anaconda virtual environment ###\n\nNext, we'll work on setting up a virtual environment in Anaconda for tensorflow-gpu. From the Start menu in Windows, search for the Anaconda Prompt utility, right click on it, and click \u201cRun as Administrator\u201d. If Windows asks you if you would like to allow it to make changes to your computer, click Yes.\n\nIn the command terminal that pops up, create a new virtual environment called \u201ctensorflow1\u201d by issuing the following command:\nC:\\> conda create -n tensorflow1 pip python=3.5\n\nThen, activate the environment by issuing:\n\nC:\\> activate tensorflow1\n\nInstall tensorflow-gpu in this environment by issuing:\n\n(tensorflow1) C:\\> pip install --ignore-installed --upgrade tensorflow-gpu\n\nInstall the other necessary packages by issuing the following commands:\n\n(tensorflow1) C:\\> conda install -c anaconda protobuf\n(tensorflow1) C:\\> pip install pillow\n(tensorflow1) C:\\> pip install lxml\n(tensorflow1) C:\\> pip install Cython\n(tensorflow1) C:\\> pip install jupyter\n(tensorflow1) C:\\> pip install matplotlib\n(tensorflow1) C:\\> pip install pandas\n(tensorflow1) C:\\> pip install opencv-python\n(Note: The \u2018pandas\u2019 and \u2018opencv-python\u2019 packages are not needed by TensorFlow, but they are used in the Python scripts to generate TFRecords and to work with images, videos, and webcam feeds.)\n\n### 1e. Configure PYTHONPATH environment variable ###\n\nA PYTHONPATH variable must be created that points to the \\models, \\models\\research, and \\models\\research\\slim directories. Do this by issuing the following commands (from any directory):\n(tensorflow1) C:\\> set PYTHONPATH=C:\\tensorflow1\\models;C:\\tensorflow1\\models\\research;C:\\tensorflow1\\models\\research\\slim\n\n(Note: Every time the \"tensorflow1\" virtual environment is exited, the PYTHONPATH variable is reset and needs to be set up again.)\n\n### 1f. Compile Protobufs and run setup.py ###\n\nNext, compile the Protobuf files, which are used by TensorFlow to configure model and training parameters. Unfortunately, the short protoc compilation command posted on TensorFlow\u2019s Object Detection API installation page does not work on Windows. Every .proto file in the \\object_detection\\protos directory must be called out individually by the command.\n\nIn the Anaconda Command Prompt, change directories to the \\models\\research directory and copy and paste the following command into the command line and press Enter:\n\nprotoc --python_out=. .\\object_detection\\protos\\anchor_generator.proto .\\object_detection\\protos\\argmax_matcher.proto .\\object_detection\\protos\\bipartite_matcher.proto .\\object_detection\\protos\\box_coder.proto .\\object_detection\\protos\\box_predictor.proto .\\object_detection\\protos\\eval.proto .\\object_detection\\protos\\faster_rcnn.proto .\\object_detection\\protos\\faster_rcnn_box_coder.proto .\\object_detection\\protos\\grid_anchor_generator.proto .\\object_detection\\protos\\hyperparams.proto .\\object_detection\\protos\\image_resizer.proto .\\object_detection\\protos\\input_reader.proto .\\object_detection\\protos\\losses.proto .\\object_detection\\protos\\matcher.proto .\\object_detection\\protos\\mean_stddev_box_coder.proto .\\object_detection\\protos\\model.proto .\\object_detection\\protos\\optimizer.proto .\\object_detection\\protos\\pipeline.proto .\\object_detection\\protos\\post_processing.proto .\\object_detection\\protos\\preprocessor.proto .\\object_detection\\protos\\region_similarity_calculator.proto .\\object_detection\\protos\\square_box_coder.proto .\\object_detection\\protos\\ssd.proto .\\object_detection\\protos\\ssd_anchor_generator.proto .\\object_detection\\protos\\string_int_label_map.proto .\\object_detection\\protos\\train.proto .\\object_detection\\protos\\keypoint_box_coder.proto .\\object_detection\\protos\\multiscale_anchor_generator.proto .\\object_detection\\protos\\graph_rewriter.proto\n\nThis creates a name_pb2.py file from every name.proto file in the \\object_detection\\protos folder.\n\nFinally, run the following commands from the C:\\tensorflow1\\models\\research directory:\n\n(tensorflow1) C:\\tensorflow1\\models\\research> python setup.py build\n(tensorflow1) C:\\tensorflow1\\models\\research> python setup.py install\n\n### 1g. Test TensorFlow setup to verify it works ###\n\nThe TensorFlow Object Detection API is now all set up to use pre-trained models for object detection, or to train a new one. You can test it out and verify your installation is working by launching the object_detection_tutorial.ipynb script with Jupyter. From the \\object_detection directory, issue this command:\n\n(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection> jupyter notebook object_detection_tutorial.ipynb\nThis opens the script in your default web browser and allows you to step through the code one section at a time. You can step through each section by clicking the \u201cRun\u201d button in the upper toolbar. The section is done running when the \u201cIn [ * ]\u201d text next to the section populates with a number (e.g. \u201cIn [1]\u201d).\nOnce you have stepped all the way through the script, you should see two labelled images at the bottom section the page. If you see this, then everything is working properly! If not, the bottom section will report any errors encountered.\n\n### 2. Gather and Label Pictures ###\n\n### 2a. Gather Pictures ###\n\nTensorFlow needs hundreds of images of an object to train a good detection classifier. To train a robust classifier, the training images should have random objects in the image along with the desired objects, and should have a variety of backgrounds and lighting conditions. There should be some images where the desired object is partially obscured, overlapped with something else, or only halfway in the picture.\n\nWe have two different objects I want to detect (red, green). Then, it took about another 169 pictures with multiple images in the picture. I know I want to be able to detect the cards when they\u2019re overlapping, so I made sure to have the cards be overlapped in many images.\n\n![8](https://user-images.githubusercontent.com/24918359/56953180-8f2f1680-6b59-11e9-8df3-c8369b8fac26.png)\n\nMake sure the images aren\u2019t too large. They should be less than 200KB each, and their resolution shouldn\u2019t be more than 720x1280. The larger the images are, the longer it will take to train the classifier. You can use the resizer.py script in this repository to reduce the size of the images.\n\n![9](https://user-images.githubusercontent.com/24918359/56953182-8f2f1680-6b59-11e9-8446-305b2d82e0d2.png)\n\nAfter you have all the pictures you need, move 20% of them to the \\object_detection\\images\\test directory, and 80% of them to the \\object_detection\\images\\train directory. Make sure there are a variety of pictures in both the \\test and \\train directories.\n\n### 2b. Label Pictures ###\n\nWith all the pictures gathered, it\u2019s time to label the desired objects in every picture. LabelImg is a great tool for labeling images, and its GitHub page has very clear instructions on how to install and use it.\n\nDownload and install LabelImg, point it to your \\images\\train directory, and then draw a box around each object in each image. Repeat the process for all the images in the \\images\\test directory. \n\n![10](https://user-images.githubusercontent.com/24918359/56953184-8f2f1680-6b59-11e9-8184-7ec582bca87e.png)\n\nLabelImg saves a .xml file containing the label data for each image. These .xml files will be used to generate TFRecords, which are one of the inputs to the TensorFlow trainer. Once you have labeled and saved each image, there will be one .xml file for each image in the \\test and \\train directories.\n\n![11](https://user-images.githubusercontent.com/24918359/56953163-8c342600-6b59-11e9-9ab1-8d7a5ac1bbc0.png)\n\nAlso, can check if the size of each bounding box is correct by running sizeChecker.py\n\n(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection> python sizeChecker.py --move\n\n![12](https://user-images.githubusercontent.com/24918359/56953164-8cccbc80-6b59-11e9-9080-308976164e9c.png)\n\n### 3. Generate Training Data ###\n\nWith the images labeled, it\u2019s time to generate the TFRecords that serve as input data to the TensorFlow training model. This tutorial uses the xml_to_csv.py and generate_tfrecord.py scripts from Dat Tran\u2019s Raccoon Detector dataset, with some slight modifications to work with our directory structure.\nFirst, the image .xml data will be used to create .csv files containing all the data for the train and test images. From the \\object_detection folder, issue the following command in the Anaconda command prompt:\n(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection> python xml_to_csv.py\nThis creates a train_labels.csv and test_labels.csv file in the \\object_detection\\images folder.\nNext, open the generate_tfrecord.py file in a text editor. Replace the label map starting at line 31 with your own label map, where each object is assigned an ID number. This same number assignment will be used when configuring the labelmap.pbtxt file.\n\ndef class_text_to_int(row_label):\n    if row_label == 'red':\n        return 1\n    elif row_label == 'green':\n        return 2\n    else:\n        return None\n\nFor example:\n\ndef class_text_to_int(row_label):\n    if row_label == 'basketball':\n        return 1\n    elif row_label == 'shirt':\n        return 2\n    elif row_label == 'shoe':\n        return 3\n    else:\n        return None\n\nThen, generate the TFRecord files by issuing these commands from the \\object_detection folder:\npython generate_tfrecord.py --csv_input=images\\train_labels.csv --image_dir=images\\train --output_path=train.record\npython generate_tfrecord.py --csv_input=images\\test_labels.csv --image_dir=images\\test --output_path=test.record\n\nThese generate a train.record and a test.record file in \\object_detection. These will be used to train the new object detection classifier.\n\n### 4. Create Label Map and Configure Training ###\n\nThe last thing to do before training is to create a label map and edit the training configuration file.\n\n### 4a. Label map ###\n\nThe label map tells the trainer what each object is by defining a mapping of class names to class ID numbers. Use a text editor to create a new file and save it as labelmap.pbtxt in the C:\\tensorflow1\\models\\research\\object_detection\\training folder. \nitem {\n  id: 1\n  name: 'red'\n}\n\nitem {\n  id: 2\n  name: 'green'\n}\n}\n\nThe label map ID numbers should be the same as what is defined in the generate_tfrecord.py file. For the basketball, shirt, and shoe detector example mentioned, the labelmap.pbtxt file will look like:\n\nitem {\n  id: 1\n  name: 'basketball'\n}\n\nitem {\n  id: 2\n  name: 'shirt'\n}\n\nitem {\n  id: 3\n  name: 'shoe'\n}\n\n### 4b. Configure training ###\n\nFinally, the object detection training pipeline must be configured. It defines which model and what parameters will be used for training. This is the last step before running training!\n \nNavigate to C:\\tensorflow1\\models\\research\\object_detection\\samples\\configs and copy the faster_rcnn_inception_v2_pets.config file into the \\object_detection\\training directory\n\n![13](https://user-images.githubusercontent.com/24918359/56953165-8cccbc80-6b59-11e9-82c8-056788bb04d2.png)\n\n![14](https://user-images.githubusercontent.com/24918359/56953166-8cccbc80-6b59-11e9-8d61-bc1a36e04202.jpg)\n\nSave the file after the changes have been made. That\u2019s it! The training job is all configured and ready to go!\n\n### 5. Run the Training ###\n\nHere we go! From the \\object_detection directory, issue the following command to begin training:\npython train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_signals.config\n\nEach step of training reports the loss. It will start high and get lower and lower as training progresses. For my training on the Faster-RCNN-Inception-V2 model, it started at about 3.0 and quickly dropped below 0.8.\n\n![15](https://user-images.githubusercontent.com/24918359/56953168-8d655300-6b59-11e9-8ee0-d3f22e9ddf33.png)\n\n![16](https://user-images.githubusercontent.com/24918359/56953169-8d655300-6b59-11e9-8c76-9e9da0d354dc.png)\n\n\nYou can view the progress of the training job by using TensorBoard. To do this, open a new instance of Anaconda Prompt, activate the tensorflow1 virtual environment, change to the C:\\tensorflow1\\models\\research\\object_detection directory, and issue the following command:\n(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection>tensorboard --logdir=training\n\n![17](https://user-images.githubusercontent.com/24918359/56953170-8d655300-6b59-11e9-97f0-b71c18cbf2db.png)\n\nThe training routine periodically saves checkpoints about every five minutes. You can terminate the training by pressing Ctrl+C while in the command prompt window. I typically wait until just after a checkpoint has been saved to terminate the training. You can terminate training and start it later, and it will restart from the last saved checkpoint. The checkpoint at the highest number of steps will be used to generate the frozen inference graph.\n\n### 6. Export Inference Graph ###\n\nNow that training is complete, the last step is to generate the frozen inference graph (.pb file). From the \\object_detection folder, issue the following command, where \u201cXXXX\u201d in \u201cmodel.ckpt-1962\u201d should be replaced with the highest-numbered .ckpt file in the training folder:\n\npython export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v2_signal.config --trained_checkpoint_prefix training/model.ckpt-1962 --output_directory inference_graph\n\n![18](https://user-images.githubusercontent.com/24918359/56953171-8d655300-6b59-11e9-95d1-5ae14ae023c6.png)\n\nThis creates a frozen_inference_graph.pb file in the \\object_detection\\inference_graph folder. The .pb file contains the object detection classifier.\n\n### 7. Use Newly Trained Object Detection Classifier ###\n\nBefore running the Python scripts, you need to modify the NUM_CLASSES variable in the script to equal the number of classes you want to detect. So NUM_CLASSES = 2. To test your object detector, move a picture of the object or objects into the \\object_detection folder, and change the IMAGE_NAME variable in the Object_detection_image.py to match the file name of the picture. Alternatively, you can use a video of the objects (using Object_detection_video.py), or just plug in a USB webcam and point it at the objects (using Object_detection_webcam.py).\n\n## Working with Machine ##\nProducing control systems capable of analyzing sensory data in order to provide accurate detection of other vehicles and the road ahead fusing data from multiple sensors. Data Replication between sensory data and real time camera [8]. Monitoring entire surrounding to identify the colour patterns of traffic signal and performing right/left turn by getting trained in predicting static/dynamic models. Maintain ratio of frames captured and running status of machine.\n\n## Sensor Testing ##\n\nIR sensor sends binary values by receiving and decoding signals. The led sends and receives live data of road patterns and communicate the output value with mo-tor IC. Absorption and Reflection of black and white signals to make the machine sense the road patterns for moving forward and turn operations (left and right).\n\n### Motor Testing ###\n\nL293D Motor Driver IC consists of 4 input pins, 4 output pins and 2 enable pins. When enable A is high, left motor (input 1 and input 2) works. When enable B is high, right motor (input 3 and input 4) works. \n\nInput 1\tInput 2\t Result\n   0      \t0\t     Stop\n   0\t      1\t     Anti-Clockwise\n   1\t      0\t     Clockwise\n   1\t      1\t     Stop\n   \n\nA GPS aerial is mounted at the rear for location information featuring gyroscopes, altimeters and a tachometer to avoid a minute miscalculation of the machine\u2019s positions.\n\nThe machine can safely drive itself under specific conditions such as expressway merging, high speed cruising, low speed traffic jam, closed-campus operations by classifying object patterns and traffic signals. Infrared Sensor performs identification of road patterns during self-parking scenarios by absorbing and reflecting Infrared signals [9]. Pi camera is fixed on top of the machine to detect signal patterns by clas-sifying colors from comparing with the trained data at frequent measure of time\n\n## Conclusion and Future Work ##\n\nThe proposed work uses different modules for providing automation to the robotic machine. There are certain systems providing security in existence but with less effi-ciency but our project deals with live data in the form of continuous images which gets automatically deleted after some point of time. This improves the efficiency of the system. Although the machine senses traffic signals and proceed moving opera-tions, installation of higher-grade cameras and motor power shall improve efficiency, performance and speed control of the machine respectively. More complicated im-ages are given as input to train the machine act according to emergency situations by predicting and analyzing. \n\n\n"
    },
    "Sentiment_Analysis": {
        "description": null,
        "readme": "No README found"
    }
}